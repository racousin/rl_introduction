{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "2-f7uQBMb133"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEAUjjhOb136"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/3_Temporal_Difference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVgWUZjpb137"
   },
   "outputs": [],
   "source": [
    "!apt-get install swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
    "!pip install pygame==2.1.0 > /dev/null 2>&1\n",
    "!pip install gym==0.23.1 > /dev/null 2>&1\n",
    "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install colabgymrender==1.0.2 > /dev/null 2>&1\n",
    "!pip install imageio==2.4.1 > /dev/null 2>&1\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eugR1jSXcH2m"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Oa03cAjLb138"
   },
   "outputs": [],
   "source": [
    "from rl_introduction.rl_introduction.tools import Agent, plot_values_lake, policy_evaluation, value_iteration, discount_cumsum, MyRandomAgent, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZwAAf2b139"
   },
   "source": [
    "# 3_Temporal_Difference"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQQPZhpb139"
   },
   "source": [
    "### Objective\n",
    "Here we present methods to solve the problem of environment and agents when the model is not known.\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TUO-omP3b13-"
   },
   "source": [
    "### Evaluate and train the agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wVyqgyPVb13-"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jcDX5myqb13-"
   },
   "source": [
    "run_experiment_episode_train will be the function used to interact and learn from environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YiOxEwbGb13_"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode_train(env, agent, nb_episode, is_train=True):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            if is_train:\n",
    "                agent.train(current_state, action, reward, state, done) # agent need a train method\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "154OqgF6b13_"
   },
   "outputs": [],
   "source": [
    "class Example_of_trainable_agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "        self.policy = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])\n",
    "        return action\n",
    "    def train(self, current_state, action, reward, state, done):\n",
    "        pass # do something smart to improve the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecT8NxQNb14A",
    "outputId": "625d2625-34ed-4e2f-ed9a-96c078647daa"
   },
   "outputs": [],
   "source": [
    "demo_agent = Example_of_trainable_agent(env)\n",
    "run_experiment_episode_train(env, demo_agent, nb_episode=10, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rl1xMx3lb14A"
   },
   "source": [
    "## Simplest exploration\n",
    "Without knowing the model, we improve the policy by interacting with the environment. We start with an arbitrary policy, a major problem is caused by a local maximum due to insufficient exploration. To avoid it, we force the agent to act sometimes in a random way (control by an epsilon).\n",
    "\n",
    "Until we are not confident of our evaluation of $Q$ we don't know if choising $\\pi(s) = \\max_a Q_\\pi(s,a)$ will bring to a better policy.\n",
    "\n",
    "It gives us Epsilon-Greedy policy $\\pi(s) = \\max_a Q_\\pi(s,a)$ with probability $1-\\epsilon$ any other action with probability $\\epsilon$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "thsYkjbWb14A"
   },
   "outputs": [],
   "source": [
    "#TODO: get epsilon greedy policy\n",
    "def get_epsilon_greedy_policy(Q_s, epsilon, nA):\n",
    "    return policy_s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1AkTJc1Wb14B"
   },
   "outputs": [],
   "source": [
    "# If we find a way to evalue the action-value function Q. We are ready to train our agent\n",
    "def train(self, current_state, action, reward, state, done):\n",
    "    self.q = evalution_of_Q()\n",
    "    for state in range(env.observation_space.n): # update policy\n",
    "        self.policy[state] = get_epsilon_greedy_policy(self.q[state], self.epsilon, env.action_space.n)\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cpDPZ7YEb14B"
   },
   "source": [
    "# Usefull tools\n",
    "Many times, it will be necessary to calculate the discount return $G_t = \\sum_{k=0}^T\\gamma^k R_{t+k+1}$. For that, we use optimized discount_cumsum function. Example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7xA0CGwob14B",
    "outputId": "408f7f03-48fc-42bd-8433-8980ff50d8e7"
   },
   "outputs": [],
   "source": [
    "episode_reward = [0,0,0,1,0,0,0,-.3,0,1,1,0,10]\n",
    "gamma = 0.99\n",
    "discount_cumsum(episode_reward, gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q37GHfHkb14C"
   },
   "source": [
    "In many cases, we will need to compute the disount rewards through multiple episodes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqCsXMC4b14C"
   },
   "source": [
    "### TODO 1): compute G along episodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "64y9pPLbb14C"
   },
   "outputs": [],
   "source": [
    "#TODO: compute G along episodes for a random agent (action = env.action_space.sample())\n",
    "def get_G(env, gamma=0.99, nb_episode=500):\n",
    "    return discount_returns\n",
    "# It should return the list of discounted return through multiple episodes.\n",
    "# Example:\n",
    "# gamma=1\n",
    "# nb_episode=2\n",
    "# episode_reward1 = [1,2,3]\n",
    "# episode_reward2 = [-1,0,1,4]\n",
    "# return = [6., 5., 3., 4., 5., 5., 4.]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "FuelEMuGb14C",
    "outputId": "e8f38761-fed4-4eff-c244-0061dcd80c0d"
   },
   "outputs": [],
   "source": [
    "res = get_G(env)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4i9MSbSHb14C"
   },
   "source": [
    "And to compute the trajectories $(S,A,R,G)_\\pi$:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rb3aB5Pvb14C"
   },
   "source": [
    "### TODO 2): compute trajectory along episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x4kcum3Tb14D"
   },
   "outputs": [],
   "source": [
    "#TODO: compute trajectory along episode for a random agent (action = env.action_space.sample())\n",
    "def get_trajectories(env, gamma=0.99, nb_episode=50):\n",
    "    return trajectories\n",
    "# It should return the list of S_t, A_t, R_{t+1}, G_t through multiple episodes.\n",
    "# Example:\n",
    "# gamma=1\n",
    "# nb_episode=2\n",
    "# episode_reward1 = [1,2,3]\n",
    "# episode_state1 = ['s1', 's2', 's1'] \n",
    "# episode_action1 = ['a1', 'a2', 'a1'] \n",
    "# episode_reward2 = [-1,0,1,4]\n",
    "# episode_state2 = ['s1', 's2', 's3', 's2'] \n",
    "# episode_action2 = ['a1', 'a2', 'a3', 'a3'] \n",
    "# return = [['s1', 'a1', 1, 6.],\n",
    "#           ['s2', 'a2', 2, 5.],\n",
    "#           ['s1', 'a1', 3, 3.],\n",
    "#           ['s1', 'a1', -1, 4.],\n",
    "#           ['s2',' a2', 0, 5.],\n",
    "#           ['s3', 'a3', 1, 5.],\n",
    "#           ['s2', 'a3', 4, 4.]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9Wm-_t4Fb14D",
    "outputId": "948700c3-9990-40ec-ca40-ac817e17db1b"
   },
   "outputs": [],
   "source": [
    "res = get_trajectories(env)\n",
    "res.shape\n",
    "#print('states' : res[:,0])\n",
    "#print('actions' : res[:,1])\n",
    "#print('rewards' : res[:,2])\n",
    "#print('cumulative discounted rewards' : res[:,3])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vr_PzpTMb14D",
    "outputId": "1784b2dd-f832-47df-e77f-9c68ac25dbc5"
   },
   "outputs": [],
   "source": [
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zWQz_M1Kb14D"
   },
   "source": [
    "# Monte-Carlo Methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9i0ULGbpb14D"
   },
   "source": [
    "Now, considering an environment without knowing its transition model, we want to build a smart agent, a free model based agent.\n",
    "The naive approach is to estimate the Q function using monte-carlo estimation.\n",
    "As we know:\n",
    "\\begin{aligned}\n",
    "V_{\\pi}(s) &= \\mathbb{E}[G_t \\vert S_t = s] = \\frac{1}{P(S_t=s)}E[G_t \\mathbb{1}_{S_t=s}] \\\\\n",
    "Q_{\\pi}(s, a) &= \\mathbb{E}_{\\pi}[G_t \\vert S_t = s, A_t = a]\n",
    "\\end{aligned}\n",
    "We compute the empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$, following policy $\\pi$.\n",
    "From law of large numbers the estimators:\n",
    "\\begin{aligned}\n",
    "V_{\\pi}(s) &\\simeq \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s]}\\\\\n",
    "Q(s, a) &\\simeq \\frac{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a] G_t}{\\sum_{t=1}^T \\mathbb{1}[S_t = s, A_t = a]}\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J59LuS54b14E"
   },
   "source": [
    "Remember Incremental mean:\n",
    "\\begin{aligned}\n",
    "\\mu_K &= \\frac{1}{K}\\sum_{j=1}^K X_j\\\\\n",
    "\\mu_K &= \\frac{1}{K}[X_K + \\sum_{j=1}^{K-1} X_j]\\\\\n",
    "\\mu_K &= \\frac{1}{K}[X_K + (K-1)\\mu_{K-1}]\\\\\n",
    "\\mu_K &= \\mu_{K-1} + \\frac{1}{K}(X_K -\\mu_{K-1})\\\\\n",
    "\\end{aligned}\n",
    "As well:\n",
    "\\begin{aligned}\n",
    "\\mu_K &= \\mu_{K-p} + \\frac{1}{K}(\\sum_{K-p}^K X_k - p\\mu_{K-p})\\\\\n",
    "\\end{aligned}\n",
    "We do the same to update incrementally at each episode the empirical $V$. For each state $S_t$ with return $G_t$:\n",
    "\\begin{aligned}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\frac{1}{N_{\\text{total}}(S_t)}(G_t - N_{\\text{trajectory}}(S_t)V(S_t))\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "68fq9PQyb14E"
   },
   "source": [
    "### TODO 3): complete policy MC evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xZ4jjlHXb14F"
   },
   "outputs": [],
   "source": [
    "#TODO: complete policy MC evaluation step\n",
    "# It should evaluate and return the value function for all state for an agent\n",
    "# initialize value function\n",
    "# 1) Compute an episode (cumulative reward, N_{total}, N_{trajectory})\n",
    "# 2) For each state update the value function using incremental mean\n",
    "# 3) go back to 1\n",
    "def policy_MC_evaluation(env, agent, gamma=1, nb_episode=5000):\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "N1Q8mHXEb14F",
    "outputId": "f67bf480-0798-4edf-94e2-44cbe7ae5371"
   },
   "outputs": [],
   "source": [
    "rand_agent = MyRandomAgent(env)\n",
    "V= policy_MC_evaluation(env, rand_agent)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lOHbp0PRb14F",
    "outputId": "c8fcb9f3-f6da-4ccd-d9c6-5964145e4f90"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Phnpg6sWb14G",
    "outputId": "35ad2a3c-3b56-4e38-d48e-bd49999a2499"
   },
   "outputs": [],
   "source": [
    "# In reality, here we know the model, we use it to control our results\n",
    "rand_agent = MyRandomAgent(env)\n",
    "V = policy_evaluation(env, rand_agent.policy) #see II) dynamic-programming\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lk4f08xZb14G",
    "outputId": "d4e18d7d-a629-47d8-8b64-5e9b79bd39fc"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DOl7t8xZb14G"
   },
   "source": [
    "In the same way, we estimate the Q function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Hj82D-Czb14G"
   },
   "source": [
    "For Q evaluation\n",
    "\\begin{aligned}\n",
    "Q(A_t, S_t) &\\leftarrow Q(A_t, S_t) + \\frac{1}{N_{\\text{total}}(A_t, S_t)}(G_t - N_{\\text{trajectory}}(A_t, S_t)Q(A_t, S_t))\\\\\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cY_mCfLnb14G"
   },
   "source": [
    "And we train an agent, improving its policy by acting greddy.\n",
    "$\\forall s$ $\\pi'(.|s) = \\arg\\max_a Q_\\pi(s,a)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSYO20zsb14G"
   },
   "source": [
    "### Train Monte-Carlo agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qZPs5xQIb14H"
   },
   "source": [
    "### TODO 4): complete policy MC algo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "frg0ESltb14H"
   },
   "outputs": [],
   "source": [
    "### TODO 4): complete policy MC algo\n",
    "class MyMCAgent(Agent):\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n])\n",
    "        # add the value you need (episode, N_{total}, N_{trajectory})\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        # collect trajectories\n",
    "        # update N_{total}, N_{trajectory}\n",
    "        if done is True: # we train the agent at every end of episode\n",
    "        # evaluate Q\n",
    "            for state in range(env.observation_space.n): # update policy\n",
    "                self.policy[state] = get_epsilon_greedy_policy(self.q[state], self.epsilon, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qUynT82hb14H"
   },
   "outputs": [],
   "source": [
    "### Done 4): complete policy MC algo\n",
    "class MyMCAgent(Agent):\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n])\n",
    "        self.count_state_actions = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.count_state_actions_by_update = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "        self.episode = []\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward])) # collect trajectories\n",
    "        self.count_state_actions[current_state, action] += 1\n",
    "        self.count_state_actions_by_update[current_state, action] += 1\n",
    "        if done is True: # we train the agent at every end of episode\n",
    "            episode = np.asarray(self.episode)\n",
    "            discount_empirical_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            for state in range(len(self.count_state_actions)): # evaluate Q\n",
    "                for action, count in enumerate(self.count_state_actions[state]):\n",
    "                    if count > 0 : # evaluate Q\n",
    "                        self.q[state,action] += (discount_empirical_return[(episode[:,0] == state) & (episode[:,1] == action)].sum() - self.count_state_actions_by_update[state, action] * self.q[state,action]) / count\n",
    "            self.count_state_actions_by_update = np.zeros((self.env.observation_space.n, self.env.action_space.n))\n",
    "            self.episode = []\n",
    "            for state in range(env.observation_space.n): # update policy\n",
    "                self.policy[state] = get_epsilon_greedy_policy(self.q[state], self.epsilon, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "Xrkm5bWvb14H",
    "outputId": "24ced83b-ca2e-4b00-93f3-a3e554e807f6",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "mc_agent = MyMCAgent(env)\n",
    "rewards = run_experiment_episode_train(env, mc_agent, 5000)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - mc_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NEl5eK2-b14H",
    "outputId": "eb716227-e727-4e81-8b64-cc1734ce4af5"
   },
   "outputs": [],
   "source": [
    "mc_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "Tq6kO06Lb14I",
    "outputId": "2402720d-047e-494a-8177-bb85c7dd8dff"
   },
   "outputs": [],
   "source": [
    "V = policy_evaluation(env, mc_agent.policy)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "V-fXsOCZb14I",
    "outputId": "9c28e480-3bce-42fd-9c76-60731e4d5d57"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "I-kcj14jb14I"
   },
   "source": [
    "# Temporal-Difference-Learning (Monte-Carlo bootstrap) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Xu0I34S2b14I"
   },
   "source": [
    "Using monte-carlo, we update $V(S_t)$ in that way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rwbwgWUJb14I"
   },
   "source": [
    "\\begin{aligned}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_t)) \\\\\n",
    "\\end{aligned}\n",
    "Using bellman equation $\\mathbb{E}[G_t|S_t=s] = \\mathbb{E}[R_{t+1} + \\gamma V(S_{t+1})|S_t=s]$, That pushes new estimators/update\n",
    "\n",
    "TD target $= R_{t+1} + \\gamma V(S_{t+1}$)\n",
    "\n",
    "TD error $=$ target $- V(S_t)$\n",
    "\n",
    "update:  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jonref4Db14I"
   },
   "source": [
    "\\begin{aligned}\n",
    "V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W7iHkXDQb14I"
   },
   "source": [
    "### TODO 5): complete policy td evaluation step"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "go-yls6Ub14J"
   },
   "outputs": [],
   "source": [
    "#TODO: complete policy td evaluation step\n",
    "def policy_td_evaluation(env, agent, gamma=1, nb_episode=5000, alpha = .01):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            target = None #complete here reward + gamma * V[state]\n",
    "            td_error = None #complete here target - V[current_state]\n",
    "            V[current_state] += None #complete here\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "qvh0ncUcb14J",
    "outputId": "8f025a66-49d4-4553-87dc-93d5719dda7a"
   },
   "outputs": [],
   "source": [
    "rand_agent = MyRandomAgent(env)\n",
    "V = policy_td_evaluation(env, rand_agent)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5DptnyFob14J"
   },
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2UE3fVIPb14J"
   },
   "source": [
    "Same principle for q function update using Temporal difference$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})−Q(S_t,A_t))$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jKPz-Ewyb14J"
   },
   "source": [
    "### TODO 5): complete SARSA agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0w-udBPhb14J"
   },
   "outputs": [],
   "source": [
    "### TODO 5): complete SARSA agent\n",
    "class MySarsaAgent(Agent):\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1, alpha = .01):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        self.alpha = alpha\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n       \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        # TRAIN SARSA\n",
    "        for state in range(env.observation_space.n):\n",
    "            self.policy[state] = get_epsilon_greedy_policy(self.q[state], self.epsilon, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "geTdrchsb14K",
    "outputId": "d40115a2-3290-4bcb-aef2-abe131390447",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sarsa_agent = MySarsaAgent(env)\n",
    "rewards = run_experiment_episode_train(env, sarsa_agent, 5000)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - sarsa_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M_MCoTlxb14K",
    "outputId": "81dc2bad-bfe6-462b-a823-c673a9a9d395"
   },
   "outputs": [],
   "source": [
    "sarsa_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NPY1rSORb14K",
    "outputId": "0ceaa6dc-d437-40a9-878f-40b8ce8b9f66"
   },
   "outputs": [],
   "source": [
    "V = policy_evaluation(env, sarsa_agent.policy)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FTNPJeIub14L",
    "outputId": "99256161-4b67-4972-b1c4-c54cb6016c6b"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZxTyGuyEb14L"
   },
   "source": [
    "### Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "x9XvJ3lXb14L"
   },
   "source": [
    "Q learning is an offpolicy sarsa. Instead of update the Q function with the current policy action, it uses a greedy estimation of the policy action\n",
    "\n",
    "SARAS $Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})−Q(S_t,A_t))$\n",
    "\n",
    "Q-learning $Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)−Q(S_t,A_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kel0WAlAb14N"
   },
   "source": [
    "### TODO 6): complete Q agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GPGj7Kv9b14N"
   },
   "outputs": [],
   "source": [
    "#TODO: write Q learning update\n",
    "class MyQAgent(Agent):\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1, alpha = .01):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        self.alpha = alpha\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        # TRAIN Q\n",
    "        for state in range(env.observation_space.n):\n",
    "            self.policy[state] = get_epsilon_greedy_policy(self.q[state], self.epsilon, env.action_space.n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHaKyfddb14O",
    "outputId": "ed7e37ae-7391-4ed7-c76a-77838117c7eb"
   },
   "outputs": [],
   "source": [
    "q_agent = MyQAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 5000)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - q_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "vuYbeIrWb14O",
    "outputId": "23bc9a6a-c135-4fd3-bc4d-525213a24717"
   },
   "outputs": [],
   "source": [
    "q_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "gr25mQ92b14O",
    "outputId": "dbe3eac3-46e0-4ac4-c935-e2065afb4026"
   },
   "outputs": [],
   "source": [
    "V = policy_evaluation(env, q_agent.policy)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "7u1QIYYsb14O",
    "outputId": "48fc4bd0-8e59-4979-8c22-0b0a1a74320a"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 390
    },
    "id": "ax2I85lVb14O",
    "outputId": "77e67a2a-930f-49b5-f544-45164c1a07be"
   },
   "outputs": [],
   "source": [
    "policy_best, V_best = value_iteration(env)\n",
    "plot_values_lake(V_best)\n",
    "V_best.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cVyBVb_sb14O"
   },
   "source": [
    "# Question: Why we don't have the optimal policy?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "fkeQodfbb14O",
    "outputId": "57f86246-2ba9-4095-a745-aece2ff8cc10"
   },
   "outputs": [],
   "source": [
    "q_agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "KmIm1oftjS6u"
   },
   "outputs": [],
   "source": [
    "#TODO: compute the best policy, using the current one\n",
    "q_policy_no_eps = np.zeros(q_agent.policy.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xy5J_dMVb14P",
    "outputId": "1fce82bf-b055-401d-d395-250d2942324a"
   },
   "outputs": [],
   "source": [
    "V = policy_evaluation(env, q_policy_no_eps)\n",
    "plot_values_lake(V)\n",
    "V_best.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YIOL72rxb14P",
    "outputId": "9aca1ac0-911a-4128-e25f-450281b7770b"
   },
   "outputs": [],
   "source": [
    "q_policy_no_eps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zpw_ghQEb14P"
   },
   "source": [
    "# Use what you built"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55mqMXHXb14P"
   },
   "source": [
    "# train/test your agent in other discrete action-space env https://gym.openai.com/envs/#toy_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyh9zZ2mb14P"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mjC_Vibb14P"
   },
   "outputs": [],
   "source": [
    "your_agent = \n",
    "nb_episode = 10\n",
    "run_experiment_episode_train(env, your_agent, nb_episode, is_train=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Jq_pLHeob14P"
   },
   "source": [
    "### keep in mind:\n",
    "\\begin{aligned}\n",
    "MDP \\rightarrow V(S_t) &\\leftarrow \\mathbb{E}[R_{t+1} + \\gamma V(S_{t+1})] \\\\\n",
    "MC \\rightarrow V(S_t) &\\leftarrow V(S_t) + \\alpha (G_t - V(S_{t}))\\\\\n",
    "TD \\rightarrow V(S_t) &\\leftarrow V(S_t) + \\alpha (R_{t+1} + \\gamma V(S_{t+1}) - V(S_t))\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbSNr7G0b14P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
