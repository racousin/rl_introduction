{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SoWByR4ODcD"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/2_Dynamic_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaM9R9w9ODcE"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!apt-get install xvfb\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install gymnasium[box2d,atari,accept-rom-license]==0.29.1\n",
    "!pip install pyvirtualdisplay==3.0\n",
    "!pip install opencv-python-headless\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kS-4aqgZODcH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import gymnasium as gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from rl_introduction.rl_introduction.tools import Agent, plot_values_lake\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLHzFR2DODcG"
   },
   "source": [
    "# 2_Dynamic_Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lzbjlcdODcG"
   },
   "source": [
    "### Objective\n",
    "Before diving deeper into Reinforcement Learning (RL), it's essential to understand how to compute the best agent's strategy when the model of the environment is perfectly known, referred to as the Markov Decision Process (MDP). In this exercise, we will use the FrozenLake environment as our example to solve an MDP.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 533
    },
    "id": "zmc80pb8rjlI",
    "outputId": "290b37d1-7d09-49f9-c0ca-465b1b103732"
   },
   "outputs": [],
   "source": [
    "from rl_introduction.rl_introduction.render_colab import exp_render\n",
    "exp_render({\"name\":'FrozenLake-v1', \"fps\":2, \"nb_step\":30})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS3uUIZNODcH"
   },
   "source": [
    "### Understanding the Environment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4vXtlYJlsD2u"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "MFYLiPBasDFe"
   },
   "source": [
    "#### 1) Environment Transition Model and Policy\n",
    "**Question 1:** Environment Description\n",
    "\n",
    "Describe the **observation space** and **action space** of the FrozenLake environment.\n",
    "\n",
    "**Question 2:** Transition Model\n",
    "\n",
    "Describe the **Transistion Model** `env.P[state][action]`. Is the transition model of the environment stochastic?\n",
    "\n",
    "**Exercise 1:** Implement a Random Policy\n",
    "\n",
    " Write a **random policy** to perform experiments in the FrozenLake environment. Then run the code bellow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8WUiDiimtVqq"
   },
   "outputs": [],
   "source": [
    "# Random Policy Implementation\n",
    "policy ="
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 452
    },
    "id": "k3u8znhStO1k",
    "outputId": "5738517d-3f41-4302-9833-a6ef58a6a43c"
   },
   "outputs": [],
   "source": [
    "class MyAgent(Agent):\n",
    "    def __init__(self, env, policy):\n",
    "        super().__init__(env)\n",
    "        self.policy = policy\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])\n",
    "        return action\n",
    "\n",
    "my_agent = MyAgent(env, policy)\n",
    "\n",
    "# Experiment Running Function\n",
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        rews = []\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, _, _ = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards\n",
    "\n",
    "# Running the experiment\n",
    "rewards = run_experiment_episode(env, my_agent, 50)\n",
    "plt.plot(rewards, 'o')\n",
    "plt.title('Cumulative Reward per Episode for Random Agent')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9y_8brHODcO"
   },
   "source": [
    "## 2a) Policy Evaluation - Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVgXIIAODcP"
   },
   "source": [
    "To evaluate the value function of a policy, we use the iterative approach\n",
    "\n",
    "\n",
    "$V_{k+1}(s) = \\mathbb{E}_\\pi [r + \\gamma V_k(s_{t+1}) | S_t = s] $.\n",
    "\n",
    "$(V_k)_{k\\in \\mathbb{N}}$ converges to $V_\\pi$.\n",
    "\n",
    "**Exercise 1:** Policy Evaluation Implementation\n",
    "\n",
    "Complete the Python function below to evaluate the given policy using the iterative approach described above.\n",
    "\n",
    "\n",
    "- Iterate over all states in the environment within the outer loop.\n",
    "- For each state, compute the expected return by considering all possible actions and their probabilities under the current policy.\n",
    "- Use the transition model (env.P[s][a]) to access the probabilities of next states and rewards for each action.\n",
    "- Update the value function until the maximum change across all states is less than Î¸, indicating convergence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LxWowp4ODcP"
   },
   "outputs": [],
   "source": [
    "#TODO: write the value evaluation from Policy, reward and transition model\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n) # initialization\n",
    "    #complete here\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 521
    },
    "id": "4O0zdN54ODcQ",
    "outputId": "3980f0b3-c879-4c41-b3c4-c265b6bf6c74"
   },
   "outputs": [],
   "source": [
    "# evaluate the policy\n",
    "policy =  np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n\n",
    "V = policy_evaluation(env, policy)\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ffVH6Tvu1PXJ"
   },
   "source": [
    "## 2b) Policy Evaluation - Action Value Function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W85hyLdpODcR"
   },
   "source": [
    "The action-value function Q(s,a) is computed as the expected return of taking an action a in state s, which includes the immediate reward plus the discounted future value as per the state transition probabilities. Mathematically, it is expressed as:\n",
    "\n",
    "\\begin{aligned}\n",
    "Q(s, a)\n",
    "&= \\sum_{s'} [r_{t+1} + \\gamma V(s')] P(S_{t+1}=s'|S_t=s,A_t=a)\n",
    "\\end{aligned}\n",
    "\n",
    "**Exercise 2:** Action Value Function\n",
    "\n",
    "compute the action value function from the value function\n",
    "- For each state, compute the q(s) action value using reward and transition model\n",
    "- Iterate over all states to get the action value function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItTE1INhODcR"
   },
   "outputs": [],
   "source": [
    "#TODO: write the q evaluation from the value function, reward and transition model\n",
    "def q_from_v(env, V, s, gamma=1):\n",
    "    #complete here\n",
    "    return q"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K90-Z5BrODcS"
   },
   "source": [
    "## 3) Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNeZXB61ODcT"
   },
   "source": [
    "The policy improvement step uses the action-value function Q to make the policy greedy, thereby ensuring the policy selects the action with the highest value in each state. Mathematically, this is represented as: $\\pi'(.|s) = \\arg\\max_a Q_\\pi(a,s)$.\n",
    "\n",
    "**Exercise 1:** Choosing the Best Action\n",
    "\n",
    "Complete the `best_action_from_Q` function bellow to determine the best action for a state s from the action-value function Q.\n",
    "\n",
    "**Exercise 2:**\n",
    "\n",
    "Complete the `policy_improvement` function bellow  to generate a new, improved policy $\\pi'$ based on the value function $V$ and the best_action_from_Q.\n",
    "\n",
    "**Exercise 3:**\n",
    "E Evaluate the value function of the new, improved policy $\\pi'$and compare it to the original policy. Better?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 176
    },
    "id": "OUlr9Js_ZYi8",
    "outputId": "19177096-9558-4722-cc1a-630787782708"
   },
   "outputs": [],
   "source": [
    "#TODO: choose the best action in a state s from Q, What the best direction/action on state 1?\n",
    "def best_action_from_Q(env, Q, s):\n",
    "  # Complete\n",
    "  return best_a\n",
    "print(f\"best direction/action on state 1: {best_action_from_Q(env, Q, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNu3cphtODcT"
   },
   "outputs": [],
   "source": [
    "#TODO: write the policy improvment update step\n",
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    #complete here\n",
    "    return policy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o1PcaW-ODcV"
   },
   "source": [
    "## 4) Policy iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gjn3OlMODcV"
   },
   "source": [
    "Policy iteration alternates between evaluating a policy and improving it until convergence.\n",
    "\n",
    "$\\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_* \\xrightarrow[]{\\text{evaluation}} V_*$\n",
    "\n",
    "- Policy Evaluation: Compute the value function $V_{\\pi}$ for the current policy.\n",
    "- Policy Improvement: Generate a new policy $\\pi'$ that is greedy with respect to $V_{\\pi}$.\n",
    "\n",
    "**Exercise 1:**\n",
    "Complete the policy iteration function below, which iteratively evaluates and improves a policy until it converges to the optimal policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lH06UMqvODcV"
   },
   "outputs": [],
   "source": [
    "#TODO: write the policy iteration\n",
    "def policy_iteration(env):\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n # init a random policy\n",
    "    # complete here\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwwfReIfODcY"
   },
   "source": [
    "## 5) Run experiments (optimal policy vs random)\n",
    "\n",
    "**Last Exercise:**Evaluate the effectiveness of the optimal policy obtained from policy iteration compared to a random policy. compare the cumulative rewards obtained by an agent using a random policy versus an agent using the optimal policy over 50 episodes. Use the run_experiment_episode function to collect the cumulative rewards for each policy and visualize the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAkBE-UYcm0N"
   },
   "outputs": [],
   "source": [
    "#TODO: eval best Policy with run_experiment_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVCEhiXyODcW"
   },
   "source": [
    "## 4) Value iteration"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjSijEDwODcX"
   },
   "source": [
    "Value iteration consists in directly compute the best policy evaluation.\n",
    "We initialize $V_0$ arbitrarly. And we update it using:\n",
    "\n",
    "$V_{k+1}(s) = \\mathbb{E}_\\pi [r + \\gamma \\max_a Q_k(s_{t+1},a) | S_t = s] $ (2).\n",
    "$\\forall s$, $V_{\\pi^*}(s)$ is a fix point for (2), so if $(V_k)_{k\\in \\mathbb{N}}$ converges, it converges to $V_{\\pi^*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KuZ3ceUODcX"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            V[s] = max(q_from_v(env, V, s, gamma))\n",
    "            delta = max(delta,abs(V[s]-v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "g66w4z-QODcX",
    "outputId": "f930a0d2-7a1a-472f-c379-c4cfc862b93b"
   },
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values_lake(V_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3v2e7fmODcY",
    "outputId": "e222f1ce-2f45-4e70-9fa6-309d3350358d"
   },
   "outputs": [],
   "source": [
    "V_vi.sum()"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
