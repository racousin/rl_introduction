{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XOW-UKFUODcB"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SoWByR4ODcD"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/2_Dynamic_Programming.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "TaM9R9w9ODcE"
   },
   "outputs": [],
   "source": [
    "!apt-get install swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
    "!pip install pygame==2.1.0 > /dev/null 2>&1\n",
    "!pip install gym==0.23.1 > /dev/null 2>&1\n",
    "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install colabgymrender==1.0.2 > /dev/null 2>&1\n",
    "!pip install imageio==2.4.1 > /dev/null 2>&1\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "M1Vl9DwJbjuz"
   },
   "outputs": [],
   "source": [
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aLHzFR2DODcG"
   },
   "source": [
    "# 2_Dynamic_Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-lzbjlcdODcG"
   },
   "source": [
    "### Objective\n",
    "Before we go any further into RL. We will compute the best agent when the model is perfectly known (MDP). As example, we will solve the FrozenLake problem.\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kS-4aqgZODcH"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from time import sleep\n",
    "from rl_introduction.rl_introduction.tools import Agent, plot_values_lake\n",
    "from rl_introduction.rl_introduction.render_colab import gym_render\n",
    "env = gym.make('FrozenLake-v1')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sS3uUIZNODcH"
   },
   "source": [
    "### FrozenLake- environment\n",
    "The agent controls the movement of a character in a grid world. Some tiles of the grid are walkable, and others lead to the agent falling into the water. Additionally, the movement direction of the agent is uncertain and only partially depends on the chosen direction. The agent is rewarded for finding a walkable path to a goal tile."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Cdz5BJh4ODcH"
   },
   "source": [
    "The surface is described using a grid like the following:\n",
    "    \n",
    "SFFF       (S: starting point, safe)\n",
    "\n",
    "FHFH       (F: frozen surface, safe)\n",
    "\n",
    "FFFH       (H: hole, fall to your doom)\n",
    "\n",
    "HFFG       (G: goal, where the frisbee is located)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wy0ert7HRySx"
   },
   "outputs": [],
   "source": [
    "#TODO: Get the state, action size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GlUTaaZsUfkg"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "EOMmNoU2ODcK",
    "outputId": "345b7a11-f013-4683-f8c3-ca89cd35963f",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='FrozenLake-v1', directory='./video', agent = 'random', slow_coeff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jy2eLiUlODcK"
   },
   "source": [
    "## 1) env transition model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ov-F5CvSODcK"
   },
   "source": [
    "We are access of the transition model, $P(S_{t+1}=s'|S_t = s, A_t=a)$ and the associated reward using <b>env.P[s][a]</b>. For example, the probabilities of each possible reward and next state, if the agent is in state 1 of the gridworld and decides to go left."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "teqGBzSHODcL"
   },
   "outputs": [],
   "source": [
    "state = 1\n",
    "action = 0 #left"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Q0_fiyS1ODcL",
    "outputId": "bcd2f966-dda3-40cb-ee4e-3a11c3cb6935"
   },
   "outputs": [],
   "source": [
    "# P(s'|state,action), s', reward, done\n",
    "env.P[state][action]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZroahN5wODcL"
   },
   "source": [
    "We see here that there is a $1/3$ probability falling in the hole in state 5 (and finish the episode)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8RwnG7mNODcM"
   },
   "source": [
    "To resume, we have \n",
    "\n",
    "**States:** $S = \\{0,...,15\\}$\n",
    "\n",
    "**Actions:** $A = \\{0,1,2,3\\}$\n",
    "\n",
    "**Transition model:** $P_{ss'}^a = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]$ -> env.P[state][action]\n",
    "\n",
    "**Reward function:**\n",
    "$R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a]$\n",
    "\n",
    "$\\forall a \\in {1,2,3} : R(14,a) = 1/3$\n",
    "    \n",
    "$R(14,0) = 0$\n",
    "\n",
    "$\\forall a \\in A \\forall s \\in \\{0,...,13\\} : R(s,a) = 0$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "i7QNaybVODcM"
   },
   "source": [
    "## 2) agent policy\n",
    "We add to our agent its <b>policy</b> $\\pi(a|s)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "FQudtciEODcM"
   },
   "outputs": [],
   "source": [
    "#TODO: write Random Policy (ex uniformly random policy)\n",
    "class MyRandomAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.policy = None#complete here\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "xFINBom8ODcN",
    "outputId": "e3ba3887-5a0e-4843-8dd3-e9cf3cb7551c"
   },
   "outputs": [],
   "source": [
    "agent = MyRandomAgent(env)\n",
    "agent.policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z3kbyqQDVLXF"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5fuZB3IlWGAT"
   },
   "outputs": [],
   "source": [
    "#TODO: eval Random Policy with run_experiment_episode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W9y_8brHODcO"
   },
   "source": [
    "## 2) Policy Evaluation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DhchO4xlODcO"
   },
   "source": [
    "The Value Function $V_\\pi(s)$ is the expected return in state $s$, according to $\\pi$.\n",
    "$V_\\pi(s) \n",
    "= \\mathbb{E}_\\pi [r + \\gamma V_\\pi(s_{t+1}) | S_t = s]\n",
    "= \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s, a) (r + \\gamma V_\\pi(s'))$\n",
    "\n",
    "We have all the information to resolve linear system for $V_\\pi$:\n",
    "\\begin{equation}\n",
    "V(s_0) =  \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s_0, a) (r + \\gamma V(s'))\\\\\n",
    "V(s_1) =  \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s_1, a) (r + \\gamma V(s'))\\\\\n",
    "...\\\\\n",
    "V(s_{16}) = \\sum_a \\pi(a \\vert s) \\sum_{s', r} P(s', r \\vert s_{16}, a) (r + \\gamma V(s'))\n",
    "\\end{equation}\n",
    "Even for 16 states it could be complicated. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mZVgXIIAODcP"
   },
   "source": [
    "So we use an iterative approach.\n",
    "\n",
    "\n",
    "We initialize $V_0$ arbitrarly. And we update it using:\n",
    "\n",
    "\n",
    "$V_{k+1}(s) = \\mathbb{E}_\\pi [r + \\gamma V_k(s_{t+1}) | S_t = s] $ (1).\n",
    "\n",
    "$\\forall s$, $V_\\pi(s)$ is a fix point for (1), so if $(V_k)_{k\\in \\mathbb{N}}$ converges, it converges to $V_\\pi$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-LxWowp4ODcP"
   },
   "outputs": [],
   "source": [
    "#TODO: write the value evaluation from Policy, reward and transition model\n",
    "def policy_evaluation(env, policy, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n) # initialization\n",
    "    #complete here\n",
    "    return V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WiGUNAfqODcQ"
   },
   "outputs": [],
   "source": [
    "my_rand_agent = MyRandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "4O0zdN54ODcQ",
    "outputId": "b9bbd259-6d6c-4cfe-da3b-342b08df9aa9"
   },
   "outputs": [],
   "source": [
    "# evaluate the policy \n",
    "V = policy_evaluation(env, my_rand_agent.policy)\n",
    "\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "gbeiCmafODcQ",
    "outputId": "b1e37e9a-846c-4543-ff96-b7896bc26551"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "o3P9aNi-ODcR"
   },
   "source": [
    "Knowing the transition $\\mathbb{P}(S_{t+1},R_{t+1}|S_t,A_t)$, it is natural to compute the q function from value function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "W85hyLdpODcR"
   },
   "source": [
    "\\begin{aligned}\n",
    "Q(s, a) \n",
    "&= \\mathbb{E} [R_{t+1} + \\gamma V(S_{t+1}) \\mid S_t = s, A_t = a] \\\\\n",
    "&= \\sum_{s'} [r_{t+1} + \\gamma V(s')] P(S_{t+1}=s'|S_t=s,A_t=a)\n",
    "\\end{aligned}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ItTE1INhODcR"
   },
   "outputs": [],
   "source": [
    "#TODO: write the q evaluation from the value function, reward and transition model\n",
    "def q_from_v(env, V, s, gamma=1):\n",
    "    #complete here \n",
    "    return q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "VCF3INfRODcS",
    "outputId": "5916921a-4113-484a-8fb3-3c9e4d787560"
   },
   "outputs": [],
   "source": [
    "Q = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "for s in range(env.observation_space.n):\n",
    "    Q[s] = q_from_v(env, V, s)\n",
    "print(\"Action-Value Function:\")\n",
    "print(Q)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "K90-Z5BrODcS"
   },
   "source": [
    "## 3) Policy Improvement"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wNeZXB61ODcT"
   },
   "source": [
    "With the q function, we update our policy from $\\pi$ to $\\pi'$ by acting greedy. That means $\\pi'(.|s) = \\arg\\max_a Q_\\pi(a,s)$.\n",
    "\n",
    "This improvment is given by: $\\forall s, V_\\pi(s) = \\sum_a\\pi(a|s)Q_\\pi(s,a) \\leq \\max_a Q_\\pi(s,a) = V_{\\pi'}(s)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 304
    },
    "id": "OUlr9Js_ZYi8",
    "outputId": "fecefd0d-21d4-48ba-a174-2bf01026b74d"
   },
   "outputs": [],
   "source": [
    "#TODO: choose the best action in a state s from Q, What the best direction/action on state 1?\n",
    "def best_action_from_Q(env, Q, s):\n",
    "  # Complete\n",
    "  return best_a\n",
    "print(f\"best direction/action on state 1: {best_action_from_Q(env, Q, 1)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oNu3cphtODcT"
   },
   "outputs": [],
   "source": [
    "#TODO: write the policy improvment update step\n",
    "def policy_improvement(env, V, gamma=1):\n",
    "    policy = np.zeros([env.observation_space.n, env.action_space.n])\n",
    "    #complete here    \n",
    "    return policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 373
    },
    "id": "M61ow3VUODcT",
    "outputId": "f9347ebb-5fb3-4d54-c460-c74f8b54f831"
   },
   "outputs": [],
   "source": [
    "# evaluate the policy \n",
    "V = policy_evaluation(env, my_rand_agent.policy)\n",
    "\n",
    "plot_values_lake(V)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F-Qynp7CODcU",
    "outputId": "97efc3b5-246b-463f-ab2b-6e054ec4b711"
   },
   "outputs": [],
   "source": [
    "V.sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HnhktSMLZnun"
   },
   "outputs": [],
   "source": [
    "#TODO: Improve and evaluate the policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "_c9vl3cZODcU",
    "outputId": "774f5ef3-4c3e-430d-b3d6-51382a947a1b"
   },
   "outputs": [],
   "source": [
    "new_V.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2o1PcaW-ODcV"
   },
   "source": [
    "## 4) Policy iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3Gjn3OlMODcV"
   },
   "source": [
    "$\\pi_0 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_0} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_1 \\xrightarrow[]{\\text{evaluation}} V_{\\pi_1} \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_2 \\xrightarrow[]{\\text{evaluation}} \\dots \\xrightarrow[]{\\text{improve}}\n",
    "\\pi_* \\xrightarrow[]{\\text{evaluation}} V_*$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lH06UMqvODcV"
   },
   "outputs": [],
   "source": [
    "#TODO: write the policy iteration\n",
    "def policy_iteration(env):\n",
    "    policy = np.ones([env.observation_space.n, env.action_space.n]) / env.action_space.n # init a random policy\n",
    "    # complete here\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "sDwyCR9bODcW",
    "outputId": "f7c18a55-43f4-41c6-c44c-b4ecef535e7a"
   },
   "outputs": [],
   "source": [
    "\n",
    "# obtain the optimal policy and optimal state-value function\n",
    "policy_pi, V_pi = policy_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_pi,\"\\n\")\n",
    "\n",
    "plot_values_lake(V_pi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "F9OsgMe9ODcW",
    "outputId": "df7936d5-16cf-4597-ac56-af65f9a2d1ee"
   },
   "outputs": [],
   "source": [
    "policy_pi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Ynve8ExUODcW",
    "outputId": "b5cf6b06-7b1d-4e2d-8750-c26d8b9f532a"
   },
   "outputs": [],
   "source": [
    "V_pi.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nVCEhiXyODcW"
   },
   "source": [
    "## 4) Value iteration "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xjSijEDwODcX"
   },
   "source": [
    "Value iteration consists in directly compute the best policy evaluation.\n",
    "We initialize $V_0$ arbitrarly. And we update it using:\n",
    "\n",
    "$V_{k+1}(s) = \\mathbb{E}_\\pi [r + \\gamma \\max_a Q_k(s_{t+1},a) | S_t = s] $ (2).\n",
    "$\\forall s$, $V_{\\pi^*}(s)$ is a fix point for (2), so if $(V_k)_{k\\in \\mathbb{N}}$ converges, it converges to $V_{\\pi^*}$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "9KuZ3ceUODcX"
   },
   "outputs": [],
   "source": [
    "def value_iteration(env, gamma=1, theta=1e-8):\n",
    "    V = np.zeros(env.observation_space.n)\n",
    "    while True:\n",
    "        delta = 0\n",
    "        for s in range(env.observation_space.n):\n",
    "            v = V[s]\n",
    "            V[s] = max(q_from_v(env, V, s, gamma))\n",
    "            delta = max(delta,abs(V[s]-v))\n",
    "        if delta < theta:\n",
    "            break\n",
    "    policy = policy_improvement(env, V, gamma)\n",
    "    return policy, V"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 703
    },
    "id": "g66w4z-QODcX",
    "outputId": "f930a0d2-7a1a-472f-c379-c4cfc862b93b"
   },
   "outputs": [],
   "source": [
    "policy_vi, V_vi = value_iteration(env)\n",
    "\n",
    "# print the optimal policy\n",
    "print(\"\\nOptimal Policy (LEFT = 0, DOWN = 1, RIGHT = 2, UP = 3):\")\n",
    "print(policy_vi,\"\\n\")\n",
    "\n",
    "# plot the optimal state-value function\n",
    "plot_values_lake(V_vi)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "n3v2e7fmODcY",
    "outputId": "e222f1ce-2f45-4e70-9fa6-309d3350358d"
   },
   "outputs": [],
   "source": [
    "V_vi.sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NwwfReIfODcY"
   },
   "source": [
    "## Train agent and Run experiments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZfP0Bx--ODcY"
   },
   "outputs": [],
   "source": [
    "class MyMDPAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "        self.policy = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n\n",
    "    def act(self, state):\n",
    "        action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])\n",
    "        return action\n",
    "    def train(self):\n",
    "        self.policy, _ = value_iteration(self.env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "OiBc20i4ODcY"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QAkBE-UYcm0N"
   },
   "outputs": [],
   "source": [
    "#TODO: eval best Policy with run_experiment_episode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 165
    },
    "id": "H4wvwYciauyo",
    "outputId": "3c7d193f-7b9b-4fe6-a1d2-0ceca8c9873d"
   },
   "outputs": [],
   "source": [
    "gym_render(env=env, directory='./video', agent = mdp_agent, slow_coeff=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NlJXFYIsbuDU"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
