{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZEAUjjhOb136"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/3_Temporal_Difference.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uVgWUZjpb137"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!apt-get install xvfb\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install gymnasium[box2d,atari,accept-rom-license]==0.29.1\n",
    "!pip install pyvirtualdisplay==3.0\n",
    "!pip install opencv-python-headless\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "Oa03cAjLb138"
   },
   "outputs": [],
   "source": [
    "# from rl_introduction.rl_introduction.tools import Agent, plot_values_lake, policy_evaluation, value_iteration, discount_cumsum, MyRandomAgent, run_experiment_episode_train\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import numpy as np\n",
    "import gymnasium as gym\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JJZwAAf2b139"
   },
   "source": [
    "# 3 Q_learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "WYQQPZhpb139"
   },
   "source": [
    "### Objective\n",
    "Implement and train a Q-Learning agent to interact with and learn from the 'FrozenLake-v1' environment without a known model.\n",
    "\n",
    "### Experiment Setup: Evaluate and Train Your Agent\n",
    "\n",
    "`run_experiment_episode_train` is the core function you will use for agent-environment interaction and learning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "id": "YiOxEwbGb13_"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "\n",
    "def run_experiment_episode_train(env, agent, nb_episode, is_train=True):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()[0]\n",
    "        done = False\n",
    "        rews = []\n",
    "        while not done:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info, _ = env.step(action)\n",
    "            if is_train:\n",
    "                agent.train(current_state, action, reward, state, done)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print(f'Episode: {i} - Cumulative Reward: {rewards[i]}')\n",
    "    return rewards\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "id": "154OqgF6b13_"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, state):\n",
    "        action = env.action_space.sample()\n",
    "        return action\n",
    "    def train(self, current_state, action, reward, state, done):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "ecT8NxQNb14A",
    "outputId": "625d2625-34ed-4e2f-ed9a-96c078647daa"
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'FrozenLakeEnv' object has no attribute 'observation_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[11], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m demo_agent \u001b[38;5;241m=\u001b[39m Agent(env)\n\u001b[0;32m----> 2\u001b[0m \u001b[43mrun_experiment_episode_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdemo_agent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnb_episode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mis_train\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[9], line 10\u001b[0m, in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode, is_train)\u001b[0m\n\u001b[1;32m      8\u001b[0m rews \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m      9\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 10\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43magent\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mact\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     11\u001b[0m     current_state \u001b[38;5;241m=\u001b[39m state\n\u001b[1;32m     12\u001b[0m     state, reward, done, info, _ \u001b[38;5;241m=\u001b[39m env\u001b[38;5;241m.\u001b[39mstep(action)\n",
      "Cell \u001b[0;32mIn[10], line 5\u001b[0m, in \u001b[0;36mAgent.act\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mact\u001b[39m(\u001b[38;5;28mself\u001b[39m, state):\n\u001b[0;32m----> 5\u001b[0m     action \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mobservation_state\u001b[49m\u001b[38;5;241m.\u001b[39mrandom()\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m action\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rl-introduction-Wr1d_zjF-py3.10/lib/python3.10/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` that will search the reminding wrappers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rl-introduction-Wr1d_zjF-py3.10/lib/python3.10/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` that will search the reminding wrappers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.cache/pypoetry/virtualenvs/rl-introduction-Wr1d_zjF-py3.10/lib/python3.10/site-packages/gymnasium/core.py:315\u001b[0m, in \u001b[0;36mWrapper.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maccessing private attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m is prohibited\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m    311\u001b[0m logger\u001b[38;5;241m.\u001b[39mwarn(\n\u001b[1;32m    312\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124menv.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m to get variables from other wrappers is deprecated and will be removed in v1.0, \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    313\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mto get this variable you can do `env.unwrapped.\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m` for environment variables or `env.get_wrapper_attr(\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m)` that will search the reminding wrappers.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    314\u001b[0m )\n\u001b[0;32m--> 315\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mgetattr\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43menv\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'FrozenLakeEnv' object has no attribute 'observation_state'"
     ]
    }
   ],
   "source": [
    "demo_agent = Agent(env)\n",
    "run_experiment_episode_train(env, demo_agent, nb_episode=10, is_train=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1:** Initialize Q-Learning Agent\n",
    "\n",
    "**Task 1a:** Initialize the `Agent` class with a Q-table filled with random values. The Q-table should have dimensions corresponding to the environment's state and action space sizes.\n",
    "\n",
    "**Task 1b:** Create a function, `get_epsilon_greedy_action_from_Q_s`, that chooses an action based on an epsilon-greedy strategy or the argmax of Q for the current state.\n",
    "\n",
    "**Task 1c:** Update the Agent class's act function to utilize `get_epsilon_greedy_action_from_Q_s` for action selection.\n",
    "\n",
    "**Task 1d:** Implement the Q-learning update formula in the Agent class's train method.\n",
    "\n",
    "$Q(S_t,A_t) \\leftarrow Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma \\max_a Q(S_{t+1},a)−Q(S_t,A_t))$\n",
    "\n",
    "**Exercise 2:** Train and Evaluate the Agent\n",
    "\n",
    "**Task 2a:** Run 100 training episodes with the Q-learning agent and collect the rewards.\n",
    "\n",
    "**Task 2b:** Plot the cumulative reward for each training episode.\n",
    "\n",
    "**Question 1:**\n",
    "\n",
    "How can we improve the learning convergence of our Q-learning agent?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done: get epsilon greedy policy\n",
    "def get_epislon_greedy_action_from_q(Q_s,epsilon):\n",
    "    if np.random.rand() > epsilon:\n",
    "        return np.argmax(Q_s)\n",
    "    else:\n",
    "        return np.random.randint(len(Q_s))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Xs67dandi6-Z"
   },
   "outputs": [],
   "source": [
    "#Done: write Q learning update\n",
    "class Agent():\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1, alpha = .01):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n\n",
    "    def act(self, state):\n",
    "        action = get_epislon_greedy_action_from_q(self.q[state], self.epsilon)\n",
    "        return action\n",
    "    def qsa_update(self, state, action, reward, next_state, done): \n",
    "        target = reward + (0 if done else self.gamma * np.max(self.q[next_state]))\n",
    "        td_error = target - self.q[state, action]\n",
    "        self.q[state, action]  += self.alpha * td_error\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.qsa_update(current_state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "eHaKyfddb14O",
    "outputId": "ed7e37ae-7391-4ed7-c76a-77838117c7eb"
   },
   "outputs": [],
   "source": [
    "q_agent = Agent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 5000)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - q_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Done: write Q learning update\n",
    "class DecayAgent():\n",
    "    def __init__(self, env, gamma = .99, epsilon = .1, alpha = .01, epsilon_decay_rate=.05):\n",
    "        self.env = env\n",
    "        self.gamma = gamma\n",
    "        self.epsilon = epsilon\n",
    "        self.alpha = alpha\n",
    "        self.initial_epsilon = epsilon\n",
    "        self.episode = 0\n",
    "        self.min_epsilon = 0\n",
    "        self.epsilon_decay_rate = epsilon_decay_rate\n",
    "        self.q = np.ones([self.env.observation_space.n, self.env.action_space.n]) / self.env.action_space.n\n",
    "    def epsilon_decay_exponential(self):\n",
    "        self.epsilon = self.initial_epsilon * (self.decay_rate ** self.episode)\n",
    "        return max(self.min_epsilon, self.epsilon)\n",
    "    def act(self, state):\n",
    "        action = get_epislon_greedy_action_from_q(self.q[state], self.epsilon)\n",
    "        self.episode += 1\n",
    "        return action\n",
    "    def qsa_update(self, state, action, reward, next_state, done): \n",
    "        target = reward + (0 if done else self.gamma * np.max(self.q[next_state]))\n",
    "        td_error = target - self.q[state, action]\n",
    "        self.q[state, action]  += self.alpha * td_error\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.qsa_update(current_state, action, reward, next_state, done)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Assume 'env' is the environment, 'policy_random' is a random policy, and 'policy_optimal' is the optimal policy obtained from policy iteration\n",
    "\n",
    "# Initialize agents with random and optimal policies\n",
    "decay_agent = DecayAgent(env)\n",
    "agent = Agent(env)\n",
    "\n",
    "# Run experiments for each agent\n",
    "rewards_decay_agent = run_experiment_episode_train(env, decay_agent, 5000)\n",
    "rewards_agent = run_experiment_episode_train(env, agent, 5000)\n",
    "\n",
    "# Visualization\n",
    "plt.figure(figsize=(14, 6))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(rewards_agent, 'o')\n",
    "plt.title('Cumulative Reward per Episode - decay_agent')\n",
    "plt.xlabel('Episode')\n",
    "plt.ylabel('Cumulative Reward')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(rewards_agent, 'o')\n",
    "plt.title('Cumulative Reward per Episode - agent')\n",
    "plt.xlabel('Episode')\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "exp_render({\"name\":'FrozenLake-v1', \"fps\":2, \"nb_step\":30, \"agent\": decay_agent})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "55mqMXHXb14P"
   },
   "source": [
    "# train/test your agent in other discrete action-space env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oyh9zZ2mb14P"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Blackjack-v1')\n",
    "env = gym.make('Taxi-v3')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4mjC_Vibb14P"
   },
   "outputs": [],
   "source": [
    "your_agent = \n",
    "nb_episode = 10\n",
    "run_experiment_episode_train(env, your_agent, nb_episode, is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IbSNr7G0b14P"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
