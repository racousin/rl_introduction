{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/4_deep_Q-learning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment 2 lines\n",
    "#!git clone https://github.com/racousin/rl_introduction.git\n",
    "#from rl_introduction.rl_introduction.tools import Agent, DeepAgent, plot_values_lake, policy_improvement, discount_cumsum, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "from rl_introduction.tools import Agent, DeepAgent, plot_values_lake, policy_improvement, discount_cumsum, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "In order to tackle difficult problems (of large size and complexity), we introduce an deep Q learning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:38:07.943125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/raphael/rl_introduction/venv/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-03-06 12:38:07.943149: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Input, Dense, concatenate, Dropout\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "import random\n",
    "from tensorflow.keras.models import Model, load_model\n",
    "import gym\n",
    "from time import time,sleep\n",
    "import numpy as np\n",
    "import gym\n",
    "import matplotlib.pyplot as plt\n",
    "from collections import deque\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 100\n",
    "state = env.reset()\n",
    "for _ in range(time_steps):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    env.render()\n",
    "    sleep(0.04)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# deep Q-learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When the state space is continous. It is necessary to have a function for the Q value. A common way to represent and update this function is to use parametric function (as neural network).\n",
    "In other words, we are looking for $\\theta \\in \\mathbb{R}^d$ such as \n",
    "$\\forall s Q_\\theta(s,a) = \\mathbb{E}_\\pi[G_t | S_t = s, A_t = a]$. We follow the same idea as q-learning:\n",
    "we learn and update $Q_\\theta(S_t,A_t)$ using the target $R_{t+1}+\\gamma \\max_a Q_\\theta(S_{t+1},a)$. A natural loss is the mean square error:\n",
    "\n",
    "$L(\\theta) = \\mathbb{E}_{s,a\\sim Q} [(y - Q(s,a,\\theta))^2]$\n",
    "\n",
    "\n",
    "\n",
    "$y = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$\n",
    "\n",
    "We have 2 ways to write our function:\n",
    "1. $Q_\\theta : S\\times A \\rightarrow \\mathbb{R}$\n",
    "\n",
    "in this case greedy policy looks like $\\pi(.|s) = \\arg\\max([Q_\\theta(s,a_0), Q_\\theta(s,a_1),... Q_\\theta(s,a_{dim(A)}]) $\n",
    "\n",
    "The target is $y = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$\n",
    "2. $Q_\\theta : S \\rightarrow \\mathbb{R}^{dim(A)}$\n",
    "\n",
    "in this case greedy policy looks like $\\pi(.|s) = \\arg\\max(Q_\\theta(s))$\n",
    "\n",
    "The target is $y_i = R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)$ for i corresponding to the played action, $Q_\\theta(s_t)_i$ otherwise.\n",
    "\n",
    "In other words, if we played $a$ (second action) in $s$, and we obseved $r$ and $s'$, our target will be (assuming we have 3 actions):\n",
    "\n",
    "$\\begin{aligned}\n",
    "y_0 =& Q(s,a,\\theta)_0\\\\\n",
    "y_1 =&R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta)\\\\\n",
    "y_2 =&Q(s,a,\\theta)_2\n",
    "\\end{aligned}$\n",
    "\n",
    "And our loss:\n",
    "\n",
    "$L(\\theta) = (R_{t+1} + \\gamma \\max_a Q(S_{t+1},a,\\theta) - Q(s,a,\\theta)_1)^2$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_sim, action_dim):\n",
    "    input_state = Input(name='input_state', shape=(state_dim,), dtype='float32')\n",
    "    x = Dense(32, activation='relu')(input_state)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(action_dim, activation='linear')(x)\n",
    "    model = Model(inputs=input_state, outputs=x)\n",
    "    model.compile(loss='mse',optimizer=Adam(learning_rate=1e-2))\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write deep Q learning update\n",
    "class DeepQAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .1):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            predicted_Qs = self.model.predict(state.reshape(1, -1))[0]\n",
    "            return np.argmax(predicted_Qs) \n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        pass #complete here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done: write deep Q learning update\n",
    "class DeepQAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .1):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            return np.random.randint(self.env.action_space.n)\n",
    "        else:\n",
    "            predicted_Qs = self.model.predict(state.reshape(1, -1))[0]\n",
    "            return np.argmax(predicted_Qs) \n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        predicted_Q_nexts = self.model.predict(next_state.reshape(1, -1))[0]\n",
    "        target = self.model.predict(current_state.reshape(1, -1))[0]\n",
    "        if done is True:\n",
    "            target[action] = reward\n",
    "        else:\n",
    "            target[action] = reward + self.gamma * np.max(predicted_Q_nexts)\n",
    "        self.model.train_on_batch(current_state.reshape(1, -1), target.reshape(1, -1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_state (InputLayer)    [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                160       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:38:23.708573: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/raphael/rl_introduction/venv/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-03-06 12:38:23.708605: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-06 12:38:23.708632: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (raphael-XPS-13-9370): /proc/driver/nvidia/version does not exist\n",
      "2022-03-06 12:38:23.708891: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 - cum reward 8.0\n",
      "episode: 1 - cum reward 10.0\n",
      "episode: 2 - cum reward 10.0\n",
      "episode: 3 - cum reward 9.0\n",
      "episode: 4 - cum reward 9.0\n",
      "episode: 5 - cum reward 10.0\n",
      "episode: 6 - cum reward 9.0\n",
      "episode: 7 - cum reward 9.0\n",
      "episode: 8 - cum reward 10.0\n",
      "episode: 9 - cum reward 12.0\n",
      "episode: 10 - cum reward 9.0\n",
      "episode: 11 - cum reward 10.0\n",
      "episode: 12 - cum reward 9.0\n",
      "episode: 13 - cum reward 10.0\n",
      "episode: 14 - cum reward 12.0\n",
      "episode: 15 - cum reward 10.0\n",
      "episode: 16 - cum reward 14.0\n",
      "episode: 17 - cum reward 10.0\n",
      "episode: 18 - cum reward 10.0\n",
      "episode: 19 - cum reward 9.0\n",
      "episode: 20 - cum reward 10.0\n",
      "episode: 21 - cum reward 11.0\n",
      "episode: 22 - cum reward 15.0\n",
      "episode: 23 - cum reward 9.0\n",
      "episode: 24 - cum reward 15.0\n",
      "episode: 25 - cum reward 12.0\n",
      "episode: 26 - cum reward 8.0\n",
      "episode: 27 - cum reward 9.0\n",
      "episode: 28 - cum reward 13.0\n",
      "episode: 29 - cum reward 13.0\n",
      "episode: 30 - cum reward 10.0\n",
      "episode: 31 - cum reward 15.0\n",
      "episode: 32 - cum reward 33.0\n",
      "episode: 33 - cum reward 10.0\n",
      "episode: 34 - cum reward 10.0\n",
      "episode: 35 - cum reward 8.0\n",
      "episode: 36 - cum reward 10.0\n",
      "episode: 37 - cum reward 10.0\n",
      "episode: 38 - cum reward 76.0\n",
      "episode: 39 - cum reward 14.0\n",
      "episode: 40 - cum reward 15.0\n",
      "episode: 41 - cum reward 20.0\n",
      "episode: 42 - cum reward 46.0\n",
      "episode: 43 - cum reward 11.0\n",
      "episode: 44 - cum reward 15.0\n",
      "episode: 45 - cum reward 52.0\n",
      "episode: 46 - cum reward 9.0\n",
      "episode: 47 - cum reward 11.0\n",
      "episode: 48 - cum reward 9.0\n",
      "episode: 49 - cum reward 12.0\n",
      "episode: 50 - cum reward 11.0\n",
      "episode: 51 - cum reward 12.0\n",
      "episode: 52 - cum reward 18.0\n",
      "episode: 53 - cum reward 34.0\n",
      "episode: 54 - cum reward 12.0\n",
      "episode: 55 - cum reward 10.0\n",
      "episode: 56 - cum reward 12.0\n",
      "episode: 57 - cum reward 10.0\n",
      "episode: 58 - cum reward 24.0\n",
      "episode: 59 - cum reward 70.0\n",
      "episode: 60 - cum reward 37.0\n",
      "episode: 61 - cum reward 11.0\n",
      "episode: 62 - cum reward 14.0\n",
      "episode: 63 - cum reward 71.0\n",
      "episode: 64 - cum reward 77.0\n",
      "episode: 65 - cum reward 11.0\n",
      "episode: 66 - cum reward 12.0\n",
      "episode: 67 - cum reward 22.0\n",
      "episode: 68 - cum reward 77.0\n",
      "episode: 69 - cum reward 38.0\n",
      "episode: 70 - cum reward 56.0\n",
      "episode: 71 - cum reward 122.0\n",
      "episode: 72 - cum reward 10.0\n",
      "episode: 73 - cum reward 9.0\n",
      "episode: 74 - cum reward 10.0\n",
      "episode: 75 - cum reward 9.0\n",
      "episode: 76 - cum reward 8.0\n",
      "episode: 77 - cum reward 9.0\n",
      "episode: 78 - cum reward 9.0\n",
      "episode: 79 - cum reward 11.0\n",
      "episode: 80 - cum reward 19.0\n",
      "episode: 81 - cum reward 13.0\n",
      "episode: 82 - cum reward 10.0\n",
      "episode: 83 - cum reward 12.0\n",
      "episode: 84 - cum reward 19.0\n",
      "episode: 85 - cum reward 43.0\n",
      "episode: 86 - cum reward 9.0\n",
      "episode: 87 - cum reward 26.0\n",
      "episode: 88 - cum reward 14.0\n",
      "episode: 89 - cum reward 17.0\n",
      "episode: 90 - cum reward 18.0\n",
      "episode: 91 - cum reward 17.0\n",
      "episode: 92 - cum reward 14.0\n",
      "episode: 93 - cum reward 11.0\n",
      "episode: 94 - cum reward 18.0\n",
      "episode: 95 - cum reward 16.0\n",
      "episode: 96 - cum reward 15.0\n",
      "episode: 97 - cum reward 22.0\n",
      "episode: 98 - cum reward 17.0\n",
      "episode: 99 - cum reward 19.0\n",
      "episode: 100 - cum reward 20.0\n",
      "episode: 101 - cum reward 23.0\n",
      "episode: 102 - cum reward 19.0\n",
      "episode: 103 - cum reward 19.0\n",
      "episode: 104 - cum reward 15.0\n",
      "episode: 105 - cum reward 15.0\n",
      "episode: 106 - cum reward 21.0\n",
      "episode: 107 - cum reward 16.0\n",
      "episode: 108 - cum reward 13.0\n",
      "episode: 109 - cum reward 11.0\n",
      "episode: 110 - cum reward 10.0\n",
      "episode: 111 - cum reward 10.0\n",
      "episode: 112 - cum reward 10.0\n",
      "episode: 113 - cum reward 11.0\n",
      "episode: 114 - cum reward 9.0\n",
      "episode: 115 - cum reward 11.0\n",
      "episode: 116 - cum reward 12.0\n",
      "episode: 117 - cum reward 13.0\n",
      "episode: 118 - cum reward 15.0\n",
      "episode: 119 - cum reward 14.0\n",
      "episode: 120 - cum reward 13.0\n",
      "episode: 121 - cum reward 11.0\n",
      "episode: 122 - cum reward 15.0\n",
      "episode: 123 - cum reward 17.0\n",
      "episode: 124 - cum reward 29.0\n",
      "episode: 125 - cum reward 22.0\n",
      "episode: 126 - cum reward 33.0\n",
      "episode: 127 - cum reward 28.0\n",
      "episode: 128 - cum reward 18.0\n",
      "episode: 129 - cum reward 38.0\n",
      "episode: 130 - cum reward 50.0\n",
      "episode: 131 - cum reward 38.0\n",
      "episode: 132 - cum reward 43.0\n",
      "episode: 133 - cum reward 52.0\n",
      "episode: 134 - cum reward 149.0\n",
      "episode: 135 - cum reward 9.0\n",
      "episode: 136 - cum reward 9.0\n",
      "episode: 137 - cum reward 9.0\n",
      "episode: 138 - cum reward 47.0\n",
      "episode: 139 - cum reward 75.0\n",
      "episode: 140 - cum reward 149.0\n",
      "episode: 141 - cum reward 70.0\n",
      "episode: 142 - cum reward 71.0\n",
      "episode: 143 - cum reward 34.0\n",
      "episode: 144 - cum reward 53.0\n",
      "episode: 145 - cum reward 101.0\n",
      "episode: 146 - cum reward 195.0\n",
      "episode: 147 - cum reward 9.0\n",
      "episode: 148 - cum reward 11.0\n",
      "episode: 149 - cum reward 11.0\n",
      "episode: 150 - cum reward 12.0\n",
      "episode: 151 - cum reward 15.0\n",
      "episode: 152 - cum reward 15.0\n",
      "episode: 153 - cum reward 18.0\n",
      "episode: 154 - cum reward 89.0\n",
      "episode: 155 - cum reward 114.0\n",
      "episode: 156 - cum reward 106.0\n",
      "episode: 157 - cum reward 104.0\n",
      "episode: 158 - cum reward 123.0\n",
      "episode: 159 - cum reward 18.0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:45:23.657698: W tensorflow/core/data/root_dataset.cc:200] Optimization loop failed: CANCELLED: Operation was cancelled\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 160 - cum reward 95.0\n",
      "episode: 161 - cum reward 91.0\n",
      "episode: 162 - cum reward 30.0\n",
      "episode: 163 - cum reward 40.0\n",
      "episode: 164 - cum reward 39.0\n",
      "episode: 165 - cum reward 82.0\n",
      "episode: 166 - cum reward 29.0\n",
      "episode: 167 - cum reward 39.0\n",
      "episode: 168 - cum reward 33.0\n",
      "episode: 169 - cum reward 39.0\n",
      "episode: 170 - cum reward 30.0\n",
      "episode: 171 - cum reward 116.0\n",
      "episode: 172 - cum reward 95.0\n",
      "episode: 173 - cum reward 105.0\n",
      "episode: 174 - cum reward 11.0\n",
      "episode: 175 - cum reward 12.0\n",
      "episode: 176 - cum reward 102.0\n",
      "episode: 177 - cum reward 22.0\n",
      "episode: 178 - cum reward 111.0\n",
      "episode: 179 - cum reward 33.0\n",
      "episode: 180 - cum reward 52.0\n",
      "episode: 181 - cum reward 84.0\n",
      "episode: 182 - cum reward 75.0\n",
      "episode: 183 - cum reward 105.0\n",
      "episode: 184 - cum reward 57.0\n",
      "episode: 185 - cum reward 44.0\n",
      "episode: 186 - cum reward 131.0\n",
      "episode: 187 - cum reward 16.0\n",
      "episode: 188 - cum reward 102.0\n",
      "episode: 189 - cum reward 15.0\n",
      "episode: 190 - cum reward 18.0\n",
      "episode: 191 - cum reward 25.0\n",
      "episode: 192 - cum reward 99.0\n",
      "episode: 193 - cum reward 200.0\n",
      "episode: 194 - cum reward 85.0\n"
     ]
    }
   ],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = DeepQAgent(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - deep_q_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experience replay"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$L_i(\\theta_i) = \\mathbb{E}_{(s, a, r, s') \\sim U(D)} \\left[ \\left(r + \\gamma \\max_{a'} Q(s', a'; \\theta_i^-) - Q(s, a; \\theta_i)\\right)^2 \\right]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write The function replay that return bacth from memory\n",
    "class DeepQAgent_experience_replay(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .1, memory_size = 2000, batch_size = 100):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "        \n",
    "    def replay(self, batch_size):\n",
    "        pass #complete here\n",
    "    \n",
    "        \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.env.action_space.n)\n",
    "            return action\n",
    "        predicted_Qs = self.model.predict(state.reshape(1, -1))[0]\n",
    "        action = np.argmax(predicted_Qs) \n",
    "        return action\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.memory.append([current_state, action, reward, next_state, done])\n",
    "        x_batch, y_batch = self.replay(self.batch_size)\n",
    "        loss = self.model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done: write The function replay that return bacth from memory\n",
    "class DeepQAgent_experience_replay(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .1, memory_size = 2000, batch_size = 100):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.memory_size = memory_size\n",
    "        self.batch_size = batch_size\n",
    "        self.memory = deque(maxlen=memory_size)\n",
    "    \n",
    "    # easy loop way\n",
    "    #def replay(self, batch_size):\n",
    "    #    x_batch =  np.zeros((batch_size, self.state_size))\n",
    "    #    y_batch =  np.zeros((batch_size, self.action_size))\n",
    "    #    minibatch = random.sample(self.memory, min(len(self.memory), batch_size))\n",
    "    #    for i, (state, action, reward, next_state, done) in enumerate(minibatch):\n",
    "    #        target = self.model.predict(state.reshape(1,-1))[0]\n",
    "    #        if done:\n",
    "    #            target[action] = reward\n",
    "    #        else:\n",
    "    #            target[action] = reward + self.gamma * np.max(self.model.predict(next_state.reshape(1,-1)))\n",
    "    #        x_batch[i] = state\n",
    "    #        y_batch[i] = target\n",
    "    #    return x_batch, y_batch\n",
    "    \n",
    "    #optimize way\n",
    "    def replay(self, batch_size):\n",
    "        minibatch = np.array(random.sample(self.memory, min(len(self.memory), batch_size)))\n",
    "        states = np.vstack(np.array(minibatch)[:,0])\n",
    "        actions = np.array(minibatch)[:,1].astype(int)\n",
    "        rewards = np.array(minibatch)[:,2]\n",
    "        next_states = np.vstack(np.array(minibatch)[:,3])\n",
    "        dones = np.array(minibatch)[:,4]\n",
    "        targets = self.model.predict(states)\n",
    "        targets_next = self.model.predict(next_states)\n",
    "        targets[np.arange(actions.size),actions] = rewards + (1 - dones) * self.gamma * np.max(targets_next, axis=1)\n",
    "        return states, targets\n",
    "    \n",
    "    \n",
    "    def act(self, state):\n",
    "        if np.random.rand() < self.epsilon:\n",
    "            action = np.random.randint(self.env.action_space.n)\n",
    "            return action\n",
    "        predicted_Qs = self.model.predict(state.reshape(1, -1))[0]\n",
    "        action = np.argmax(predicted_Qs) \n",
    "        return action\n",
    "    \n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.memory.append([current_state, action, reward, next_state, done])\n",
    "        x_batch, y_batch = self.replay(self.batch_size)\n",
    "        loss = self.model.train_on_batch(x_batch, y_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "\n",
    "q_agent = DeepQAgent_experience_replay(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 200)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - memory_deep_q_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Other improvments"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### epsilon decay\n",
    "Decay how random you take an action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clipping reward\n",
    "$Q(s,a):=Q(s,a)+\\alpha(clip(r+\\gamma \\arg\\max(Q(s',a'))-Q(s,a), -1, 1))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clipping loss\n",
    "tf.keras.losses.Huber(delta=1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Double Q learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "DQN: A reinforcement learning algorithm that combines Q-Learning with deep neural networks to let RL work for complex, high-dimensional environments, like video games, or robotics.\n",
    "Double Q Learning: Corrects the stock DQN algorithm’s tendency to sometimes overestimate the values tied to specific actions.\n",
    "Prioritized Replay: Extends DQN’s experience replay function by learning to replay memories where the real reward significantly diverges from the expected reward, letting the agent adjust itself in response to developing incorrect assumptions.\n",
    "Dueling DQN: Splits the neural network into two — one learns to provide an estimate of the value at every timestep, and the other calculates potential advantages of each action, and the two are combined for a single action-advantage Q function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
