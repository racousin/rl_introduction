{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)G_t] \\text{REINFORCE}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)V_w(s)] \\text{V actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a)] \\text{Q actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)A_w(s,a)] \\text{Advantage actor-critic}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q actor critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 4):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.turn = 0\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Y_values = np.vstack([ep[3] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                #early_stopping = np.inf\n",
    "                #while early_stopping > 25:\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs,Y_values)\n",
    "                print(early_stopping)\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                print('predict:',baselines.mean())\n",
    "                print('real:',discounted_returns.mean())\n",
    "                print('real_values:',Y_values.mean())\n",
    "                loss = policy_gradient_loss(discounted_returns)#baselines)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, Y_value])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode_train(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            agent.train(current_state, action, reward, state, done)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_22\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_76 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_77 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_78 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_23\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_79 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_80 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_81 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 14.0\n",
      "episode: 1 - cum reward 26.0\n",
      "episode: 2 - cum reward 25.0\n",
      "episode: 3 - cum reward 24.0\n",
      "1.0019172\n",
      "predict: 0.0916135\n",
      "real: 11.265795\n",
      "real_values: 0.9788997307252348\n",
      "episode: 4 - cum reward 19.0\n",
      "episode: 5 - cum reward 30.0\n",
      "episode: 6 - cum reward 25.0\n",
      "episode: 7 - cum reward 36.0\n",
      "episode: 8 - cum reward 18.0\n",
      "0.99647254\n",
      "predict: 0.19128881\n",
      "real: 13.5747\n",
      "real_values: 1.0842665586996516\n",
      "episode: 9 - cum reward 16.0\n",
      "episode: 10 - cum reward 30.0\n",
      "episode: 11 - cum reward 37.0\n",
      "episode: 12 - cum reward 19.0\n",
      "episode: 13 - cum reward 20.0\n",
      "0.99414504\n",
      "predict: 0.31914282\n",
      "real: 13.442715\n",
      "real_values: 1.197951909506096\n",
      "episode: 14 - cum reward 17.0\n",
      "episode: 15 - cum reward 43.0\n",
      "episode: 16 - cum reward 20.0\n",
      "episode: 17 - cum reward 18.0\n",
      "episode: 18 - cum reward 17.0\n",
      "0.99601597\n",
      "predict: 0.44000933\n",
      "real: 13.573658\n",
      "real_values: 1.2990052639221659\n",
      "episode: 19 - cum reward 56.0\n",
      "episode: 20 - cum reward 94.0\n",
      "episode: 21 - cum reward 15.0\n",
      "episode: 22 - cum reward 42.0\n",
      "episode: 23 - cum reward 23.0\n",
      "0.99632937\n",
      "predict: 0.5574425\n",
      "real: 25.927277\n",
      "real_values: 1.4011642391654267\n",
      "episode: 24 - cum reward 47.0\n",
      "episode: 25 - cum reward 38.0\n",
      "episode: 26 - cum reward 54.0\n",
      "episode: 27 - cum reward 25.0\n",
      "episode: 28 - cum reward 48.0\n",
      "0.9984622\n",
      "predict: 0.7291997\n",
      "real: 19.529951\n",
      "real_values: 1.5441259637023463\n",
      "episode: 29 - cum reward 24.0\n",
      "episode: 30 - cum reward 55.0\n",
      "episode: 31 - cum reward 30.0\n",
      "episode: 32 - cum reward 21.0\n",
      "episode: 33 - cum reward 76.0\n",
      "1.0145272\n",
      "predict: 1.0755297\n",
      "real: 23.338305\n",
      "real_values: 1.8317431931967263\n",
      "episode: 34 - cum reward 32.0\n",
      "episode: 35 - cum reward 53.0\n",
      "episode: 36 - cum reward 40.0\n",
      "episode: 37 - cum reward 54.0\n",
      "episode: 38 - cum reward 61.0\n",
      "1.017232\n",
      "predict: 1.0907613\n",
      "real: 22.83997\n",
      "real_values: 1.851209180859419\n",
      "episode: 39 - cum reward 54.0\n",
      "episode: 40 - cum reward 52.0\n",
      "episode: 41 - cum reward 49.0\n",
      "episode: 42 - cum reward 62.0\n",
      "episode: 43 - cum reward 52.0\n",
      "1.0390471\n",
      "predict: 1.4693558\n",
      "real: 23.255123\n",
      "real_values: 2.164618492126465\n",
      "episode: 44 - cum reward 19.0\n",
      "episode: 45 - cum reward 20.0\n",
      "episode: 46 - cum reward 43.0\n",
      "episode: 47 - cum reward 75.0\n",
      "episode: 48 - cum reward 38.0\n",
      "1.0974942\n",
      "predict: 1.9309512\n",
      "real: 22.383482\n",
      "real_values: 2.544983284717256\n",
      "episode: 49 - cum reward 38.0\n",
      "episode: 50 - cum reward 28.0\n",
      "episode: 51 - cum reward 40.0\n",
      "episode: 52 - cum reward 52.0\n",
      "episode: 53 - cum reward 28.0\n",
      "1.1436619\n",
      "predict: 2.2313828\n",
      "real: 17.81507\n",
      "real_values: 2.7997511754164823\n",
      "episode: 54 - cum reward 108.0\n",
      "episode: 55 - cum reward 19.0\n",
      "episode: 56 - cum reward 54.0\n",
      "episode: 57 - cum reward 118.0\n",
      "episode: 58 - cum reward 48.0\n",
      "1.2012028\n",
      "predict: 2.676917\n",
      "real: 30.827757\n",
      "real_values: 3.187910120856313\n",
      "episode: 59 - cum reward 39.0\n",
      "episode: 60 - cum reward 25.0\n",
      "episode: 61 - cum reward 43.0\n",
      "episode: 62 - cum reward 68.0\n",
      "episode: 63 - cum reward 56.0\n",
      "1.3298914\n",
      "predict: 3.0463696\n",
      "real: 22.72901\n",
      "real_values: 3.4894711623589196\n",
      "episode: 64 - cum reward 26.0\n",
      "episode: 65 - cum reward 38.0\n",
      "episode: 66 - cum reward 45.0\n",
      "episode: 67 - cum reward 60.0\n",
      "episode: 68 - cum reward 35.0\n",
      "1.3302263\n",
      "predict: 3.362399\n",
      "real: 20.42983\n",
      "real_values: 3.7506809649842507\n",
      "episode: 69 - cum reward 66.0\n",
      "episode: 70 - cum reward 101.0\n",
      "episode: 71 - cum reward 52.0\n",
      "episode: 72 - cum reward 89.0\n",
      "episode: 73 - cum reward 55.0\n",
      "1.4420106\n",
      "predict: 4.2311378\n",
      "real: 31.317114\n",
      "real_values: 4.5091337362925215\n",
      "episode: 74 - cum reward 80.0\n",
      "episode: 75 - cum reward 49.0\n",
      "episode: 76 - cum reward 41.0\n",
      "episode: 77 - cum reward 84.0\n",
      "episode: 78 - cum reward 71.0\n",
      "1.6111867\n",
      "predict: 4.454489\n",
      "real: 26.989664\n",
      "real_values: 4.7244730637998\n",
      "episode: 79 - cum reward 85.0\n",
      "episode: 80 - cum reward 31.0\n",
      "episode: 81 - cum reward 62.0\n",
      "episode: 82 - cum reward 39.0\n",
      "episode: 83 - cum reward 46.0\n",
      "2.369755\n",
      "predict: 6.1931105\n",
      "real: 20.695677\n",
      "real_values: 6.172648590602232\n",
      "episode: 84 - cum reward 69.0\n",
      "episode: 85 - cum reward 45.0\n",
      "episode: 86 - cum reward 49.0\n",
      "episode: 87 - cum reward 79.0\n",
      "episode: 88 - cum reward 57.0\n",
      "2.7073982\n",
      "predict: 7.010467\n",
      "real: 25.244307\n",
      "real_values: 6.905891200770503\n",
      "episode: 89 - cum reward 43.0\n",
      "episode: 90 - cum reward 65.0\n",
      "episode: 91 - cum reward 42.0\n",
      "episode: 92 - cum reward 28.0\n",
      "episode: 93 - cum reward 138.0\n",
      "2.8101819\n",
      "predict: 7.3183446\n",
      "real: 34.020164\n",
      "real_values: 7.215599068791875\n",
      "episode: 94 - cum reward 109.0\n",
      "episode: 95 - cum reward 35.0\n",
      "episode: 96 - cum reward 75.0\n",
      "episode: 97 - cum reward 30.0\n",
      "episode: 98 - cum reward 36.0\n",
      "5.410486\n",
      "predict: 9.224595\n",
      "real: 21.820822\n",
      "real_values: 8.852613684805958\n",
      "episode: 99 - cum reward 30.0\n",
      "episode: 100 - cum reward 26.0\n",
      "episode: 101 - cum reward 30.0\n",
      "episode: 102 - cum reward 43.0\n",
      "episode: 103 - cum reward 43.0\n",
      "7.1163254\n",
      "predict: 10.733716\n",
      "real: 16.892202\n",
      "real_values: 10.180240396042946\n",
      "episode: 104 - cum reward 47.0\n",
      "episode: 105 - cum reward 55.0\n",
      "episode: 106 - cum reward 53.0\n",
      "episode: 107 - cum reward 27.0\n",
      "episode: 108 - cum reward 23.0\n",
      "7.411931\n",
      "predict: 12.059927\n",
      "real: 19.69399\n",
      "real_values: 11.377976290787323\n",
      "episode: 109 - cum reward 57.0\n",
      "episode: 110 - cum reward 34.0\n",
      "episode: 111 - cum reward 56.0\n",
      "episode: 112 - cum reward 49.0\n",
      "episode: 113 - cum reward 25.0\n",
      "7.3149543\n",
      "predict: 13.3295\n",
      "real: 19.658665\n",
      "real_values: 12.510421735484426\n",
      "episode: 114 - cum reward 47.0\n",
      "episode: 115 - cum reward 35.0\n",
      "episode: 116 - cum reward 42.0\n",
      "episode: 117 - cum reward 46.0\n",
      "episode: 118 - cum reward 40.0\n",
      "8.91801\n",
      "predict: 15.266746\n",
      "real: 18.486092\n",
      "real_values: 14.244800163924328\n",
      "episode: 119 - cum reward 48.0\n",
      "episode: 120 - cum reward 27.0\n",
      "episode: 121 - cum reward 48.0\n",
      "episode: 122 - cum reward 37.0\n",
      "episode: 123 - cum reward 38.0\n",
      "13.557216\n",
      "predict: 16.57557\n",
      "real: 17.608011\n",
      "real_values: 15.500664469401041\n",
      "episode: 124 - cum reward 22.0\n",
      "episode: 125 - cum reward 24.0\n",
      "episode: 126 - cum reward 56.0\n",
      "episode: 127 - cum reward 22.0\n",
      "episode: 128 - cum reward 35.0\n",
      "14.697567\n",
      "predict: 19.029612\n",
      "real: 17.64628\n",
      "real_values: 17.707493747237827\n",
      "episode: 129 - cum reward 28.0\n",
      "episode: 130 - cum reward 82.0\n",
      "episode: 131 - cum reward 73.0\n",
      "episode: 132 - cum reward 80.0\n",
      "episode: 133 - cum reward 72.0\n",
      "12.63937\n",
      "predict: 23.554352\n",
      "real: 30.708818\n",
      "real_values: 22.051631843616597\n",
      "episode: 134 - cum reward 100.0\n",
      "episode: 135 - cum reward 118.0\n",
      "episode: 136 - cum reward 124.0\n",
      "episode: 137 - cum reward 170.0\n",
      "episode: 138 - cum reward 143.0\n",
      "9.762592\n",
      "predict: 26.193388\n",
      "real: 46.696297\n",
      "real_values: 24.537991821014128\n",
      "episode: 139 - cum reward 139.0\n",
      "episode: 140 - cum reward 169.0\n",
      "episode: 141 - cum reward 179.0\n",
      "episode: 142 - cum reward 179.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 143 - cum reward 200.0\n",
      "10.971241\n",
      "predict: 27.728659\n",
      "real: 54.351894\n",
      "real_values: 25.777835717391444\n",
      "episode: 144 - cum reward 200.0\n",
      "episode: 145 - cum reward 171.0\n",
      "episode: 146 - cum reward 143.0\n",
      "episode: 147 - cum reward 128.0\n",
      "episode: 148 - cum reward 105.0\n",
      "14.753931\n",
      "predict: 32.00918\n",
      "real: 46.450466\n",
      "real_values: 29.59035116052715\n",
      "episode: 149 - cum reward 182.0\n",
      "episode: 150 - cum reward 151.0\n",
      "episode: 151 - cum reward 200.0\n",
      "episode: 152 - cum reward 132.0\n",
      "episode: 153 - cum reward 164.0\n",
      "11.157339\n",
      "predict: 32.82723\n",
      "real: 51.203384\n",
      "real_values: 30.211230123245734\n",
      "episode: 154 - cum reward 113.0\n",
      "episode: 155 - cum reward 39.0\n",
      "episode: 156 - cum reward 169.0\n",
      "episode: 157 - cum reward 187.0\n",
      "episode: 158 - cum reward 163.0\n",
      "13.743277\n",
      "predict: 37.457508\n",
      "real: 50.4236\n",
      "real_values: 34.37318617981395\n",
      "episode: 159 - cum reward 153.0\n",
      "episode: 160 - cum reward 116.0\n",
      "episode: 161 - cum reward 111.0\n",
      "episode: 162 - cum reward 99.0\n",
      "episode: 163 - cum reward 130.0\n",
      "30.997728\n",
      "predict: 48.28471\n",
      "real: 40.944496\n",
      "real_values: 45.17763966426515\n",
      "episode: 164 - cum reward 26.0\n",
      "episode: 165 - cum reward 148.0\n",
      "episode: 166 - cum reward 127.0\n",
      "episode: 167 - cum reward 128.0\n",
      "episode: 168 - cum reward 144.0\n",
      "26.323315\n",
      "predict: 47.408703\n",
      "real: 46.001717\n",
      "real_values: 44.77087976291803\n",
      "episode: 169 - cum reward 159.0\n",
      "episode: 170 - cum reward 44.0\n",
      "episode: 171 - cum reward 124.0\n",
      "episode: 172 - cum reward 132.0\n",
      "episode: 173 - cum reward 114.0\n",
      "38.883\n",
      "predict: 53.37037\n",
      "real: 40.541428\n",
      "real_values: 50.9001265908209\n",
      "episode: 174 - cum reward 123.0\n",
      "episode: 175 - cum reward 161.0\n",
      "episode: 176 - cum reward 165.0\n",
      "episode: 177 - cum reward 178.0\n",
      "episode: 178 - cum reward 181.0\n",
      "22.909714\n",
      "predict: 49.32937\n",
      "real: 52.567284\n",
      "real_values: 47.27921411556049\n",
      "episode: 179 - cum reward 175.0\n",
      "episode: 180 - cum reward 124.0\n",
      "episode: 181 - cum reward 36.0\n",
      "episode: 182 - cum reward 148.0\n",
      "episode: 183 - cum reward 146.0\n",
      "36.049488\n",
      "predict: 53.54277\n",
      "real: 44.186695\n",
      "real_values: 51.36609299277419\n",
      "episode: 184 - cum reward 117.0\n",
      "episode: 185 - cum reward 121.0\n",
      "episode: 186 - cum reward 200.0\n",
      "episode: 187 - cum reward 172.0\n",
      "episode: 188 - cum reward 183.0\n",
      "20.581707\n",
      "predict: 51.387203\n",
      "real: 52.650345\n",
      "real_values: 49.42943193221233\n",
      "episode: 189 - cum reward 177.0\n",
      "episode: 190 - cum reward 125.0\n",
      "episode: 191 - cum reward 134.0\n",
      "episode: 192 - cum reward 147.0\n",
      "episode: 193 - cum reward 16.0\n",
      "50.802734\n",
      "predict: 59.32023\n",
      "real: 44.27101\n",
      "real_values: 57.87277245182562\n",
      "episode: 194 - cum reward 135.0\n",
      "episode: 195 - cum reward 200.0\n",
      "episode: 196 - cum reward 158.0\n",
      "episode: 197 - cum reward 176.0\n",
      "episode: 198 - cum reward 154.0\n",
      "31.171137\n",
      "predict: 57.003372\n",
      "real: 52.82501\n",
      "real_values: 55.565749550974644\n",
      "episode: 199 - cum reward 101.0\n",
      "episode: 200 - cum reward 200.0\n",
      "episode: 201 - cum reward 200.0\n",
      "episode: 202 - cum reward 200.0\n",
      "episode: 203 - cum reward 200.0\n",
      "23.263035\n",
      "predict: 54.779472\n",
      "real: 57.131992\n",
      "real_values: 52.92454363822937\n",
      "episode: 204 - cum reward 200.0\n",
      "episode: 205 - cum reward 200.0\n",
      "episode: 206 - cum reward 200.0\n",
      "episode: 207 - cum reward 200.0\n",
      "episode: 208 - cum reward 200.0\n",
      "27.425512\n",
      "predict: 56.788418\n",
      "real: 57.131992\n",
      "real_values: 54.38052180767059\n",
      "episode: 209 - cum reward 121.0\n",
      "episode: 210 - cum reward 148.0\n",
      "episode: 211 - cum reward 200.0\n",
      "episode: 212 - cum reward 200.0\n",
      "episode: 213 - cum reward 200.0\n",
      "26.95342\n",
      "predict: 59.538696\n",
      "real: 55.3691\n",
      "real_values: 57.24591599938704\n",
      "episode: 214 - cum reward 200.0\n",
      "episode: 215 - cum reward 200.0\n",
      "episode: 216 - cum reward 187.0\n",
      "episode: 217 - cum reward 200.0\n",
      "episode: 218 - cum reward 200.0\n",
      "16.013773\n",
      "predict: 58.235275\n",
      "real: 56.65912\n",
      "real_values: 56.28029173059718\n",
      "episode: 219 - cum reward 200.0\n",
      "episode: 220 - cum reward 200.0\n",
      "episode: 221 - cum reward 200.0\n",
      "episode: 222 - cum reward 199.0\n",
      "episode: 223 - cum reward 200.0\n",
      "20.851145\n",
      "predict: 63.130505\n",
      "real: 57.095108\n",
      "real_values: 60.748506844416724\n",
      "episode: 224 - cum reward 200.0\n",
      "episode: 225 - cum reward 200.0\n",
      "episode: 226 - cum reward 200.0\n",
      "episode: 227 - cum reward 194.0\n",
      "episode: 228 - cum reward 200.0\n",
      "17.531986\n",
      "predict: 63.941574\n",
      "real: 56.91189\n",
      "real_values: 61.74601367018385\n",
      "episode: 229 - cum reward 200.0\n",
      "episode: 230 - cum reward 200.0\n",
      "episode: 231 - cum reward 160.0\n",
      "episode: 232 - cum reward 113.0\n",
      "episode: 233 - cum reward 114.0\n",
      "34.807762\n",
      "predict: 68.4639\n",
      "real: 48.955917\n",
      "real_values: 66.86117472478031\n",
      "episode: 234 - cum reward 179.0\n",
      "episode: 235 - cum reward 171.0\n",
      "episode: 236 - cum reward 176.0\n",
      "episode: 237 - cum reward 183.0\n",
      "episode: 238 - cum reward 192.0\n",
      "19.434551\n",
      "predict: 67.02837\n",
      "real: 54.11971\n",
      "real_values: 66.02150220686049\n",
      "episode: 239 - cum reward 168.0\n",
      "episode: 240 - cum reward 127.0\n",
      "episode: 241 - cum reward 151.0\n",
      "episode: 242 - cum reward 161.0\n",
      "episode: 243 - cum reward 147.0\n",
      "25.814962\n",
      "predict: 70.83162\n",
      "real: 48.046696\n",
      "real_values: 70.85867853132125\n",
      "episode: 244 - cum reward 137.0\n",
      "episode: 245 - cum reward 164.0\n",
      "episode: 246 - cum reward 174.0\n",
      "episode: 247 - cum reward 175.0\n",
      "episode: 248 - cum reward 190.0\n",
      "16.669966\n",
      "predict: 66.13963\n",
      "real: 53.341427\n",
      "real_values: 66.58065896827839\n",
      "episode: 249 - cum reward 200.0\n",
      "episode: 250 - cum reward 200.0\n",
      "episode: 251 - cum reward 200.0\n",
      "episode: 252 - cum reward 200.0\n",
      "episode: 253 - cum reward 200.0\n",
      "13.342086\n",
      "predict: 69.01072\n",
      "real: 57.131992\n",
      "real_values: 68.70350601673127\n",
      "episode: 254 - cum reward 200.0\n",
      "episode: 255 - cum reward 55.0\n",
      "episode: 256 - cum reward 200.0\n",
      "episode: 257 - cum reward 200.0\n",
      "episode: 258 - cum reward 200.0\n",
      "18.960201\n",
      "predict: 63.64233\n",
      "real: 54.313293\n",
      "real_values: 63.93373014144315\n",
      "episode: 259 - cum reward 200.0\n",
      "episode: 260 - cum reward 200.0\n",
      "episode: 261 - cum reward 200.0\n",
      "episode: 262 - cum reward 200.0\n",
      "episode: 263 - cum reward 200.0\n",
      "8.673422\n",
      "predict: 60.271152\n",
      "real: 57.131992\n",
      "real_values: 60.92969325065613\n",
      "episode: 264 - cum reward 200.0\n",
      "episode: 265 - cum reward 51.0\n",
      "episode: 266 - cum reward 139.0\n",
      "episode: 267 - cum reward 151.0\n",
      "episode: 268 - cum reward 167.0\n",
      "19.995583\n",
      "predict: 65.58062\n",
      "real: 46.450417\n",
      "real_values: 66.73794036024199\n",
      "episode: 269 - cum reward 166.0\n",
      "episode: 270 - cum reward 140.0\n",
      "episode: 271 - cum reward 20.0\n",
      "episode: 272 - cum reward 20.0\n",
      "episode: 273 - cum reward 22.0\n",
      "139.41846\n",
      "predict: 68.52606\n",
      "real: 35.420128\n",
      "real_values: 70.68468422464805\n",
      "episode: 274 - cum reward 15.0\n",
      "episode: 275 - cum reward 99.0\n",
      "episode: 276 - cum reward 106.0\n",
      "episode: 277 - cum reward 171.0\n",
      "episode: 278 - cum reward 200.0\n",
      "22.221153\n",
      "predict: 58.836052\n",
      "real: 48.912518\n",
      "real_values: 61.81933665606711\n",
      "episode: 279 - cum reward 155.0\n",
      "episode: 280 - cum reward 200.0\n",
      "episode: 281 - cum reward 170.0\n",
      "episode: 282 - cum reward 200.0\n",
      "episode: 283 - cum reward 200.0\n",
      "11.495436\n",
      "predict: 56.354977\n",
      "real: 56.06799\n",
      "real_values: 58.879581183891794\n",
      "episode: 284 - cum reward 200.0\n",
      "episode: 285 - cum reward 200.0\n",
      "episode: 286 - cum reward 200.0\n",
      "episode: 287 - cum reward 200.0\n",
      "episode: 288 - cum reward 200.0\n",
      "6.5730352\n",
      "predict: 48.230637\n",
      "real: 57.131992\n",
      "real_values: 50.50243863582611\n",
      "episode: 289 - cum reward 200.0\n",
      "episode: 290 - cum reward 143.0\n",
      "episode: 291 - cum reward 200.0\n",
      "episode: 292 - cum reward 200.0\n",
      "episode: 293 - cum reward 200.0\n",
      "10.42777\n",
      "predict: 55.74697\n",
      "real: 55.223904\n",
      "real_values: 57.42594335152997\n",
      "episode: 294 - cum reward 200.0\n",
      "episode: 295 - cum reward 200.0\n",
      "episode: 296 - cum reward 200.0\n",
      "episode: 297 - cum reward 200.0\n",
      "episode: 298 - cum reward 130.0\n",
      "13.929771\n",
      "predict: 61.746754\n",
      "real: 54.87625\n",
      "real_values: 63.07654764358311\n",
      "episode: 299 - cum reward 200.0\n",
      "episode: 300 - cum reward 200.0\n",
      "episode: 301 - cum reward 200.0\n",
      "episode: 302 - cum reward 200.0\n",
      "episode: 303 - cum reward 200.0\n",
      "19.557913\n",
      "predict: 63.996105\n",
      "real: 57.131992\n",
      "real_values: 65.06889885425568\n",
      "episode: 304 - cum reward 200.0\n",
      "episode: 305 - cum reward 200.0\n",
      "episode: 306 - cum reward 200.0\n",
      "episode: 307 - cum reward 91.0\n",
      "episode: 308 - cum reward 200.0\n",
      "27.16943\n",
      "predict: 66.97354\n",
      "real: 54.190994\n",
      "real_values: 67.88080008985682\n",
      "episode: 309 - cum reward 49.0\n",
      "episode: 310 - cum reward 200.0\n",
      "episode: 311 - cum reward 200.0\n",
      "episode: 312 - cum reward 200.0\n",
      "episode: 313 - cum reward 200.0\n",
      "22.417568\n",
      "predict: 61.524532\n",
      "real: 57.131992\n",
      "real_values: 62.466410112380984\n",
      "episode: 314 - cum reward 200.0\n",
      "episode: 315 - cum reward 200.0\n",
      "episode: 316 - cum reward 200.0\n",
      "episode: 317 - cum reward 200.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 318 - cum reward 200.0\n",
      "12.685977\n",
      "predict: 50.336742\n",
      "real: 57.131992\n",
      "real_values: 51.18637390613556\n",
      "episode: 319 - cum reward 200.0\n",
      "episode: 320 - cum reward 200.0\n",
      "episode: 321 - cum reward 200.0\n",
      "episode: 322 - cum reward 200.0\n",
      "episode: 323 - cum reward 200.0\n",
      "12.423218\n",
      "predict: 48.309746\n",
      "real: 57.131992\n",
      "real_values: 48.85374749183655\n",
      "episode: 324 - cum reward 200.0\n",
      "episode: 325 - cum reward 200.0\n",
      "episode: 326 - cum reward 200.0\n",
      "episode: 327 - cum reward 54.0\n",
      "episode: 328 - cum reward 200.0\n",
      "8.919004\n",
      "predict: 41.40026\n",
      "real: 54.33141\n",
      "real_values: 41.7907241914615\n",
      "episode: 329 - cum reward 129.0\n",
      "episode: 330 - cum reward 177.0\n",
      "episode: 331 - cum reward 195.0\n",
      "episode: 332 - cum reward 200.0\n",
      "episode: 333 - cum reward 200.0\n",
      "4.9212613\n",
      "predict: 35.16872\n",
      "real: 56.112553\n",
      "real_values: 35.37801905864261\n",
      "episode: 334 - cum reward 200.0\n",
      "episode: 335 - cum reward 200.0\n",
      "episode: 336 - cum reward 185.0\n",
      "episode: 337 - cum reward 200.0\n",
      "episode: 338 - cum reward 200.0\n",
      "6.6269507\n",
      "predict: 42.093517\n",
      "real: 56.587788\n",
      "real_values: 41.881628428902594\n",
      "episode: 339 - cum reward 130.0\n",
      "episode: 340 - cum reward 162.0\n",
      "episode: 341 - cum reward 171.0\n",
      "episode: 342 - cum reward 179.0\n",
      "episode: 343 - cum reward 200.0\n",
      "3.7962887\n",
      "predict: 37.01669\n",
      "real: 53.76824\n",
      "real_values: 36.59816085890438\n",
      "episode: 344 - cum reward 200.0\n",
      "episode: 345 - cum reward 125.0\n",
      "episode: 346 - cum reward 136.0\n",
      "episode: 347 - cum reward 114.0\n",
      "episode: 348 - cum reward 149.0\n",
      "3.0069022\n",
      "predict: 40.999126\n",
      "real: 44.85653\n",
      "real_values: 40.48814512755125\n",
      "episode: 349 - cum reward 22.0\n",
      "episode: 350 - cum reward 23.0\n",
      "episode: 351 - cum reward 146.0\n",
      "episode: 352 - cum reward 36.0\n",
      "episode: 353 - cum reward 24.0\n",
      "66.01192\n",
      "predict: 51.62629\n",
      "real: 35.42267\n",
      "real_values: 51.59311252910497\n",
      "episode: 354 - cum reward 132.0\n",
      "episode: 355 - cum reward 200.0\n",
      "episode: 356 - cum reward 194.0\n",
      "episode: 357 - cum reward 200.0\n",
      "episode: 358 - cum reward 200.0\n",
      "2.9817173\n",
      "predict: 36.21376\n",
      "real: 56.91189\n",
      "real_values: 36.21899285424566\n",
      "episode: 359 - cum reward 154.0\n",
      "episode: 360 - cum reward 200.0\n",
      "episode: 361 - cum reward 200.0\n",
      "episode: 362 - cum reward 200.0\n",
      "episode: 363 - cum reward 200.0\n",
      "4.703043\n",
      "predict: 43.635902\n",
      "real: 57.131992\n",
      "real_values: 43.41955172538757\n",
      "episode: 364 - cum reward 143.0\n",
      "episode: 365 - cum reward 200.0\n",
      "episode: 366 - cum reward 200.0\n",
      "episode: 367 - cum reward 200.0\n",
      "episode: 368 - cum reward 200.0\n",
      "4.0108037\n",
      "predict: 42.945587\n",
      "real: 57.131992\n",
      "real_values: 42.42036026716232\n",
      "episode: 369 - cum reward 200.0\n",
      "episode: 370 - cum reward 200.0\n",
      "episode: 371 - cum reward 185.0\n",
      "episode: 372 - cum reward 200.0\n",
      "episode: 373 - cum reward 200.0\n",
      "6.407856\n",
      "predict: 48.68779\n",
      "real: 56.587788\n",
      "real_values: 47.99643586031191\n",
      "episode: 374 - cum reward 200.0\n",
      "episode: 375 - cum reward 152.0\n",
      "episode: 376 - cum reward 135.0\n",
      "episode: 377 - cum reward 200.0\n",
      "episode: 378 - cum reward 200.0\n",
      "8.233836\n",
      "predict: 46.10576\n",
      "real: 53.0577\n",
      "real_values: 45.18199013900202\n",
      "episode: 379 - cum reward 200.0\n",
      "episode: 380 - cum reward 200.0\n",
      "episode: 381 - cum reward 200.0\n",
      "episode: 382 - cum reward 200.0\n",
      "episode: 383 - cum reward 200.0\n",
      "4.2897\n",
      "predict: 44.080425\n",
      "real: 57.131992\n",
      "real_values: 42.91446897745132\n",
      "episode: 384 - cum reward 200.0\n",
      "episode: 385 - cum reward 200.0\n",
      "episode: 386 - cum reward 200.0\n",
      "episode: 387 - cum reward 200.0\n",
      "episode: 388 - cum reward 200.0\n",
      "8.251891\n",
      "predict: 51.682728\n",
      "real: 57.131992\n",
      "real_values: 50.48115026950836\n",
      "episode: 389 - cum reward 120.0\n",
      "episode: 390 - cum reward 200.0\n",
      "episode: 391 - cum reward 200.0\n",
      "episode: 392 - cum reward 200.0\n",
      "episode: 393 - cum reward 200.0\n",
      "11.120863\n",
      "predict: 56.999542\n",
      "real: 57.131992\n",
      "real_values: 55.78418093204498\n",
      "episode: 394 - cum reward 200.0\n",
      "episode: 395 - cum reward 200.0\n",
      "episode: 396 - cum reward 200.0\n",
      "episode: 397 - cum reward 200.0\n",
      "episode: 398 - cum reward 200.0\n",
      "17.69199\n",
      "predict: 63.20027\n",
      "real: 57.131992\n",
      "real_values: 62.051059045791625\n",
      "episode: 399 - cum reward 200.0\n",
      "episode: 400 - cum reward 200.0\n",
      "episode: 401 - cum reward 200.0\n",
      "episode: 402 - cum reward 200.0\n",
      "episode: 403 - cum reward 200.0\n",
      "18.355461\n",
      "predict: 62.88056\n",
      "real: 57.131992\n",
      "real_values: 61.73172478675842\n",
      "episode: 404 - cum reward 200.0\n",
      "episode: 405 - cum reward 200.0\n",
      "episode: 406 - cum reward 200.0\n",
      "episode: 407 - cum reward 200.0\n",
      "episode: 408 - cum reward 200.0\n",
      "16.510212\n",
      "predict: 63.780518\n",
      "real: 57.131992\n",
      "real_values: 62.70321820735931\n",
      "episode: 409 - cum reward 200.0\n",
      "episode: 410 - cum reward 200.0\n",
      "episode: 411 - cum reward 200.0\n",
      "episode: 412 - cum reward 200.0\n",
      "episode: 413 - cum reward 200.0\n",
      "13.397609\n",
      "predict: 62.196182\n",
      "real: 57.131992\n",
      "real_values: 61.16575475215912\n",
      "episode: 414 - cum reward 200.0\n",
      "episode: 415 - cum reward 200.0\n",
      "episode: 416 - cum reward 200.0\n",
      "episode: 417 - cum reward 200.0\n",
      "episode: 418 - cum reward 200.0\n",
      "17.535988\n",
      "predict: 65.279816\n",
      "real: 57.131992\n",
      "real_values: 64.30326416492463\n",
      "episode: 419 - cum reward 200.0\n",
      "episode: 420 - cum reward 200.0\n",
      "episode: 421 - cum reward 200.0\n",
      "episode: 422 - cum reward 200.0\n",
      "episode: 423 - cum reward 200.0\n",
      "15.211867\n",
      "predict: 63.426342\n",
      "real: 57.131992\n",
      "real_values: 62.53271751880646\n",
      "episode: 424 - cum reward 200.0\n",
      "episode: 425 - cum reward 200.0\n",
      "episode: 426 - cum reward 200.0\n",
      "episode: 427 - cum reward 200.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-54-e986afab2bb4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#env = gym.make('LunarLander-v2')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m600\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-53-e16deee0f198>\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mrews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-51-820d81eedf16>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, current_state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     57\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     60\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 390\u001b[0;31m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1591\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3935\u001b[0m         \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_use_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3936\u001b[0m         \u001b[0mpreserve_cardinality\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_preserve_cardinality\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3937\u001b[0;31m         **self._flat_structure)\n\u001b[0m\u001b[1;32m   3938\u001b[0m     \u001b[0msuper\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mParallelMapDataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__init__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mvariant_tensor\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/ops/gen_dataset_ops.py\u001b[0m in \u001b[0;36mparallel_map_dataset\u001b[0;34m(input_dataset, other_arguments, num_parallel_calls, f, output_types, output_shapes, use_inter_op_parallelism, sloppy, preserve_cardinality, name)\u001b[0m\n\u001b[1;32m   4325\u001b[0m         \u001b[0;34m\"f\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_types\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_types\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"output_shapes\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0moutput_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4326\u001b[0m         \u001b[0;34m\"use_inter_op_parallelism\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0muse_inter_op_parallelism\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"sloppy\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 4327\u001b[0;31m         sloppy, \"preserve_cardinality\", preserve_cardinality)\n\u001b[0m\u001b[1;32m   4328\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0m_result\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   4329\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0m_core\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_FallbackException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 600)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 4):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.turn = 0\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Y_values = np.vstack([ep[3] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                #early_stopping = np.inf\n",
    "                #while early_stopping > 25:\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs,discounted_returns)\n",
    "                print(early_stopping)\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                print('predict:',baselines.mean())\n",
    "                print('real:',discounted_returns.mean())\n",
    "                print('real_values:',Y_values.mean())\n",
    "                loss = policy_gradient_loss(discounted_returns)#baselines)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Y_values)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, Y_value])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 3):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                episodes = np.asarray(self.episodes)\n",
    "                self.episodes = []\n",
    "                Xs = np.vstack(episodes[:,0])\n",
    "                Ys = np.vstack(episodes[:,1])\n",
    "                Y_values = np.vstack(episodes[:,3])\n",
    "                discounted_returns = np.hstack(episodes[:,2])\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                loss = policy_gradient_loss(baselines)\n",
    "                print(baselines.max(), baselines.min())\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.model_critic.train_on_batch(Xs, Y_values)#discounted_returns.astype(np.dtype('float32')))\n",
    "            else:\n",
    "                self.episodes.append((X,Y,discounted_return, Y_value))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 400)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import spinup.algos.vpg.core as core\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_tf import MpiAdamOptimizer, sync_all_params\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "\n",
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a VPG agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
    "                self.ret_buf, self.logp_buf]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Vanilla Policy Gradient\n",
    "(with GAE-Lambda for advantage estimation)\n",
    "\"\"\"\n",
    "def vpg(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        logger_kwargs=dict(), save_freq=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       (batch, act_dim)  | Samples actions from policy given \n",
    "                                           | states.\n",
    "            ``logp``     (batch,)          | Gives log probability, according to\n",
    "                                           | the policy, of taking actions ``a_ph``\n",
    "                                           | in states ``x_ph``.\n",
    "            ``logp_pi``  (batch,)          | Gives log probability, according to\n",
    "                                           | the policy, of the action sampled by\n",
    "                                           | ``pi``.\n",
    "            ``v``        (batch,)          | Gives the value estimate for states\n",
    "                                           | in ``x_ph``. (Critical: make sure \n",
    "                                           | to flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to VPG.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs of interaction (equivalent to\n",
    "            number of policy updates) to perform.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        pi_lr (float): Learning rate for policy optimizer.\n",
    "        vf_lr (float): Learning rate for value function optimizer.\n",
    "        train_v_iters (int): Number of gradient descent steps to take on \n",
    "            value function per epoch.\n",
    "        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
    "            close to 1.)\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    seed += 10000 * proc_id()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    \n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph = core.placeholders_from_spaces(env.observation_space, env.action_space)\n",
    "    adv_ph, ret_ph, logp_old_ph = core.placeholders(None, None, None)\n",
    "\n",
    "    # Main outputs from computation graph\n",
    "    pi, logp, logp_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "    # Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "    all_phs = [x_ph, a_ph, adv_ph, ret_ph, logp_old_ph]\n",
    "\n",
    "    # Every step, get: action, value, and logprob\n",
    "    get_action_ops = [pi, v, logp_pi]\n",
    "\n",
    "    # Experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['pi', 'v'])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # VPG objectives\n",
    "    pi_loss = -tf.reduce_mean(logp * adv_ph)\n",
    "    v_loss = tf.reduce_mean((ret_ph - v)**2)\n",
    "\n",
    "    # Info (useful to watch during learning)\n",
    "    approx_kl = tf.reduce_mean(logp_old_ph - logp)      # a sample estimate for KL-divergence, easy to compute\n",
    "    approx_ent = tf.reduce_mean(-logp)                  # a sample estimate for entropy, also easy to compute\n",
    "\n",
    "    # Optimizers\n",
    "    train_pi = MpiAdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "    train_v = MpiAdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Sync params across processes\n",
    "    sess.run(sync_all_params())\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph}, outputs={'pi': pi, 'v': v})\n",
    "\n",
    "    def update():\n",
    "        inputs = {k:v for k,v in zip(all_phs, buf.get())}\n",
    "        pi_l_old, v_l_old, ent = sess.run([pi_loss, v_loss, approx_ent], feed_dict=inputs)\n",
    "\n",
    "        # Policy gradient step\n",
    "        sess.run(train_pi, feed_dict=inputs)\n",
    "\n",
    "        # Value function learning\n",
    "        for _ in range(train_v_iters):\n",
    "            sess.run(train_v, feed_dict=inputs)\n",
    "\n",
    "        # Log changes from update\n",
    "        pi_l_new, v_l_new, kl = sess.run([pi_loss, v_loss, approx_kl], feed_dict=inputs)\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old, \n",
    "                     KL=kl, Entropy=ent, \n",
    "                     DeltaLossPi=(pi_l_new - pi_l_old),\n",
    "                     DeltaLossV=(v_l_new - v_l_old))\n",
    "\n",
    "    start_time = time.time()\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a, v_t, logp_t = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v_t, logp_t)\n",
    "            logger.store(VVals=v_t)\n",
    "\n",
    "            o, r, d, _ = env.step(a[0])\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            terminal = d or (ep_len == max_ep_len)\n",
    "            if terminal or (t==local_steps_per_epoch-1):\n",
    "                if not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                last_val = r if d else sess.run(v, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "                buf.finish_path(last_val)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "    parser.add_argument('--hid', type=int, default=64)\n",
    "    parser.add_argument('--l', type=int, default=2)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "    parser.add_argument('--cpu', type=int, default=4)\n",
    "    parser.add_argument('--steps', type=int, default=4000)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--exp_name', type=str, default='vpg')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "\n",
    "    from spinup.utils.run_utils import setup_logger_kwargs\n",
    "    logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "\n",
    "    vpg(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,\n",
    "        ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,\n",
    "        logger_kwargs=logger_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
