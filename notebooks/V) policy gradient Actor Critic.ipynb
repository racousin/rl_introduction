{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)G_t] \\text{REINFORCE}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)V_w(s)] \\text{V actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a)] \\text{Q actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)A_w(s,a)] \\text{Advantage actor-critic}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q actor critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 4):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.turn = 0\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, next_state, done]))#, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, next_state, done]))#, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            rewards = episode[:,2]\n",
    "            dones = episode[:,4]\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Y_values = np.vstack([ep[3] for ep in self.episodes])\n",
    "                rewardss = np.hstack([ep[4] for ep in self.episodes])\n",
    "                doness = np.hstack([ep[5] for ep in self.episodes])\n",
    "                discounted_returns = rewardss + (1 - doness) * self.model_critic.predict(Y_values).ravel()\n",
    "                #discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs, discounted_returns.astype('float32'))\n",
    "                baselines = rewardss + (1 - doness) * self.model_critic.predict(Y_values).ravel() - self.model_critic.predict(Xs).ravel() \n",
    "                baselines -= baselines.mean()\n",
    "                baselines /= baselines.std()\n",
    "                loss = policy_gradient_loss(baselines.astype('float32'))\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                loss = self.model.train_on_batch(Xs,Ys)\n",
    "                print('loss ', loss)\n",
    "                #Y_values -= Y_values.mean()\n",
    "                #Y_values /= Y_values.std()\n",
    "                #discounted_returns -= discounted_returns.mean()\n",
    "                #discounted_returns /= discounted_returns.std()\n",
    "                #early_stopping = self.model_critic.train_on_batch(Xs,Y_values)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, Y_value, rewards, dones])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_124\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_386 (Dense)            (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_387 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_388 (Dense)            (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_125\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_389 (Dense)            (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_390 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_391 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 16.0\n",
      "episode: 1 - cum reward 12.0\n",
      "episode: 2 - cum reward 20.0\n",
      "episode: 3 - cum reward 19.0\n",
      "loss  0.013034543\n",
      "episode: 4 - cum reward 19.0\n",
      "episode: 5 - cum reward 11.0\n",
      "episode: 6 - cum reward 11.0\n",
      "episode: 7 - cum reward 15.0\n",
      "episode: 8 - cum reward 31.0\n",
      "loss  0.0019511352\n",
      "episode: 9 - cum reward 21.0\n",
      "episode: 10 - cum reward 15.0\n",
      "episode: 11 - cum reward 21.0\n",
      "episode: 12 - cum reward 20.0\n",
      "episode: 13 - cum reward 17.0\n",
      "loss  -0.045599442\n",
      "episode: 14 - cum reward 17.0\n",
      "episode: 15 - cum reward 12.0\n",
      "episode: 16 - cum reward 13.0\n",
      "episode: 17 - cum reward 9.0\n",
      "episode: 18 - cum reward 16.0\n",
      "loss  -0.22920756\n",
      "episode: 19 - cum reward 14.0\n",
      "episode: 20 - cum reward 9.0\n",
      "episode: 21 - cum reward 13.0\n",
      "episode: 22 - cum reward 13.0\n",
      "episode: 23 - cum reward 17.0\n",
      "loss  -0.17855062\n",
      "episode: 24 - cum reward 11.0\n",
      "episode: 25 - cum reward 14.0\n",
      "episode: 26 - cum reward 15.0\n",
      "episode: 27 - cum reward 8.0\n",
      "episode: 28 - cum reward 12.0\n",
      "loss  0.020123329\n",
      "episode: 29 - cum reward 16.0\n",
      "episode: 30 - cum reward 10.0\n",
      "episode: 31 - cum reward 10.0\n",
      "episode: 32 - cum reward 26.0\n",
      "episode: 33 - cum reward 19.0\n",
      "loss  -0.012381234\n",
      "episode: 34 - cum reward 11.0\n",
      "episode: 35 - cum reward 8.0\n",
      "episode: 36 - cum reward 14.0\n",
      "episode: 37 - cum reward 17.0\n",
      "episode: 38 - cum reward 10.0\n",
      "loss  -0.050055593\n",
      "episode: 39 - cum reward 8.0\n",
      "episode: 40 - cum reward 9.0\n",
      "episode: 41 - cum reward 11.0\n",
      "episode: 42 - cum reward 8.0\n",
      "episode: 43 - cum reward 10.0\n",
      "loss  0.040080167\n",
      "episode: 44 - cum reward 9.0\n",
      "episode: 45 - cum reward 10.0\n",
      "episode: 46 - cum reward 12.0\n",
      "episode: 47 - cum reward 10.0\n",
      "episode: 48 - cum reward 11.0\n",
      "loss  0.03779714\n",
      "episode: 49 - cum reward 14.0\n",
      "episode: 50 - cum reward 11.0\n",
      "episode: 51 - cum reward 10.0\n",
      "episode: 52 - cum reward 8.0\n",
      "episode: 53 - cum reward 8.0\n",
      "loss  0.039217602\n",
      "episode: 54 - cum reward 10.0\n",
      "episode: 55 - cum reward 8.0\n",
      "episode: 56 - cum reward 11.0\n",
      "episode: 57 - cum reward 9.0\n",
      "episode: 58 - cum reward 10.0\n",
      "loss  -0.0017288319\n",
      "episode: 59 - cum reward 25.0\n",
      "episode: 60 - cum reward 8.0\n",
      "episode: 61 - cum reward 9.0\n",
      "episode: 62 - cum reward 9.0\n",
      "episode: 63 - cum reward 10.0\n",
      "loss  0.016860453\n",
      "episode: 64 - cum reward 10.0\n",
      "episode: 65 - cum reward 10.0\n",
      "episode: 66 - cum reward 10.0\n",
      "episode: 67 - cum reward 9.0\n",
      "episode: 68 - cum reward 10.0\n",
      "loss  0.009355436\n",
      "episode: 69 - cum reward 9.0\n",
      "episode: 70 - cum reward 9.0\n",
      "episode: 71 - cum reward 12.0\n",
      "episode: 72 - cum reward 11.0\n",
      "episode: 73 - cum reward 10.0\n",
      "loss  0.016651561\n",
      "episode: 74 - cum reward 9.0\n",
      "episode: 75 - cum reward 13.0\n",
      "episode: 76 - cum reward 10.0\n",
      "episode: 77 - cum reward 10.0\n",
      "episode: 78 - cum reward 9.0\n",
      "loss  nan\n",
      "episode: 79 - cum reward 10.0\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "probabilities contain NaN",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-133-15c1dfc573f9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m#env = gym.make('LunarLander-v2')\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/notebooks/tools.py\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-132-e46c85ae2801>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     51\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     52\u001b[0m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 53\u001b[0;31m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     54\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     55\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32mmtrand.pyx\u001b[0m in \u001b[0;36mnumpy.random.mtrand.RandomState.choice\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: probabilities contain NaN"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 2,532\n",
      "Trainable params: 2,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,433\n",
      "Trainable params: 2,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward -184.25776796935327\n",
      "episode: 1 - cum reward -189.7188397849216\n",
      "episode: 2 - cum reward -89.32067428250814\n",
      "episode: 3 - cum reward -201.13122889744562\n",
      "episode: 4 - cum reward -82.80972569626624\n",
      "episode: 5 - cum reward -223.33968360429787\n",
      "episode: 6 - cum reward -195.65721351916847\n",
      "episode: 7 - cum reward -124.6918889163855\n",
      "episode: 8 - cum reward -201.836140750835\n",
      "episode: 9 - cum reward -88.86257267247626\n",
      "-0.8462877135701886\n",
      "-95.6284728050232\n",
      "22.46339073404557\n",
      "7.2011014814001655\n",
      "loss  -0.0155251585\n",
      "episode: 10 - cum reward -205.76924930520224\n",
      "episode: 11 - cum reward -187.76524757600816\n",
      "episode: 12 - cum reward -149.83629706184533\n",
      "episode: 13 - cum reward -38.028492212305\n",
      "episode: 14 - cum reward -151.68694134437175\n",
      "episode: 15 - cum reward -141.49944064085912\n",
      "episode: 16 - cum reward -191.69290636673935\n",
      "episode: 17 - cum reward -30.449633268400817\n",
      "episode: 18 - cum reward -131.4836130309174\n",
      "episode: 19 - cum reward -112.20058430733198\n",
      "episode: 20 - cum reward -144.67998416187024\n",
      "-0.42879662099192223\n",
      "-85.49467945098877\n",
      "15.487158750589515\n",
      "4.889347188997014\n",
      "loss  -0.15041415\n",
      "episode: 21 - cum reward -178.70493696847183\n",
      "episode: 22 - cum reward -371.5884428467354\n",
      "episode: 23 - cum reward -136.71749914278507\n",
      "episode: 24 - cum reward -129.21931694127437\n",
      "episode: 25 - cum reward -291.52803181453567\n",
      "episode: 26 - cum reward -223.38714697032896\n",
      "episode: 27 - cum reward -282.42865184076595\n",
      "episode: 28 - cum reward -206.8275501179483\n",
      "episode: 29 - cum reward -251.9481461666055\n",
      "episode: 30 - cum reward -136.8496677995348\n",
      "episode: 31 - cum reward -301.1730480807282\n",
      "-0.27112397706557956\n",
      "-53.78745651245117\n",
      "20.065527146560992\n",
      "2.0978155130848095\n",
      "loss  -0.3256114\n",
      "episode: 32 - cum reward -142.86343571379925\n",
      "episode: 33 - cum reward -458.2955192350013\n",
      "episode: 34 - cum reward -297.29269098469916\n",
      "episode: 35 - cum reward -545.6750639176689\n",
      "episode: 36 - cum reward -792.5820765673528\n",
      "episode: 37 - cum reward -601.5944855848437\n",
      "episode: 38 - cum reward -912.3721605506588\n",
      "episode: 39 - cum reward -555.2918619742906\n",
      "episode: 40 - cum reward -619.4811334799715\n",
      "episode: 41 - cum reward -418.49613282376146\n",
      "episode: 42 - cum reward -510.8334771393393\n",
      "-0.8134059649294032\n",
      "-7.521640788624893\n",
      "447.4102028877959\n",
      "10.166423623708294\n",
      "loss  0.13952035\n",
      "episode: 43 - cum reward -640.9253647520052\n",
      "episode: 44 - cum reward -207.3919751884524\n",
      "episode: 45 - cum reward -159.84176705409084\n",
      "episode: 46 - cum reward -138.13934205082143\n",
      "episode: 47 - cum reward -138.00873360641225\n",
      "episode: 48 - cum reward -252.59293589544978\n",
      "episode: 49 - cum reward -165.4838206924821\n",
      "episode: 50 - cum reward -178.3722360470003\n",
      "episode: 51 - cum reward -140.01430163135038\n",
      "episode: 52 - cum reward -202.45476124344452\n",
      "episode: 53 - cum reward -113.98869476356427\n",
      "0.1102024538021292\n",
      "-60.35348120089111\n",
      "255.69387976019976\n",
      "7.410646056668727\n",
      "loss  -0.07719101\n",
      "episode: 54 - cum reward -204.57565570254926\n",
      "episode: 55 - cum reward -168.85406480979833\n",
      "episode: 56 - cum reward -187.36188365260057\n",
      "episode: 57 - cum reward -186.20209222907386\n",
      "episode: 58 - cum reward -159.46849718502457\n",
      "episode: 59 - cum reward -203.20531200595917\n",
      "episode: 60 - cum reward -190.21515629901356\n",
      "episode: 61 - cum reward -201.24144165437593\n",
      "episode: 62 - cum reward -162.5133207636229\n",
      "episode: 63 - cum reward -188.72146177678707\n",
      "episode: 64 - cum reward -180.0917855435179\n",
      "0.3105122404948001\n",
      "-7.27621847482726\n",
      "182.52706909179688\n",
      "7.190547575292405\n",
      "loss  0.0133434525\n",
      "episode: 65 - cum reward -169.51255298098158\n",
      "episode: 66 - cum reward 17.652505730875156\n",
      "episode: 67 - cum reward -136.12870561499687\n",
      "episode: 68 - cum reward 179.26747929260824\n",
      "episode: 69 - cum reward -172.28927473948414\n",
      "episode: 70 - cum reward -79.63149710898699\n",
      "episode: 71 - cum reward -61.80333490782785\n",
      "episode: 72 - cum reward 29.799314062958175\n",
      "episode: 73 - cum reward -98.98238839601267\n",
      "episode: 74 - cum reward 163.79055818542895\n",
      "episode: 75 - cum reward -35.76269107533085\n",
      "0.42597188768022093\n",
      "-41.301421655102416\n",
      "267.1455841064453\n",
      "9.110255618226926\n",
      "loss  -0.13035227\n",
      "episode: 76 - cum reward 190.3495937813521\n",
      "episode: 77 - cum reward -147.4898211094934\n",
      "episode: 78 - cum reward -167.39483677280526\n",
      "episode: 79 - cum reward -167.39883076954112\n",
      "episode: 80 - cum reward -128.477568192603\n",
      "episode: 81 - cum reward -126.49274938435642\n",
      "episode: 82 - cum reward -185.88721980482154\n",
      "episode: 83 - cum reward -106.86604923861634\n",
      "episode: 84 - cum reward -207.19706929944664\n",
      "episode: 85 - cum reward -203.65239822081287\n",
      "episode: 86 - cum reward -102.37714300774493\n",
      "0.1129354364027203\n",
      "-29.347335815429688\n",
      "138.25390313948262\n",
      "2.934264981547829\n",
      "loss  0.07055293\n",
      "episode: 87 - cum reward -218.37837398312007\n",
      "episode: 88 - cum reward 75.50523355121337\n",
      "episode: 89 - cum reward -145.63086854223093\n",
      "episode: 90 - cum reward -101.76389575288802\n",
      "episode: 91 - cum reward 1.4832099804435757\n",
      "episode: 92 - cum reward -6.771106465155567\n",
      "episode: 93 - cum reward 123.96447816212824\n",
      "episode: 94 - cum reward -97.70219278420777\n",
      "episode: 95 - cum reward 21.153211516046866\n",
      "episode: 96 - cum reward 60.44780267546307\n",
      "episode: 97 - cum reward 0.3908055112821338\n",
      "0.22672466752610726\n",
      "-45.50437545776367\n",
      "144.41716766357422\n",
      "4.63199717967435\n",
      "loss  -0.15179555\n",
      "episode: 98 - cum reward -135.1080049232205\n",
      "episode: 99 - cum reward -236.44870935486605\n",
      "episode: 100 - cum reward -150.83057442478002\n",
      "episode: 101 - cum reward -183.0830815551149\n",
      "episode: 102 - cum reward -171.85892260428477\n",
      "episode: 103 - cum reward -162.3340100527394\n",
      "episode: 104 - cum reward -110.16240974207625\n",
      "episode: 105 - cum reward -124.548124012323\n",
      "episode: 106 - cum reward -138.05347662397392\n",
      "episode: 107 - cum reward -136.79071002109947\n",
      "episode: 108 - cum reward -87.56224970219125\n",
      "-0.029259029024922164\n",
      "-7.913162869809199\n",
      "174.50435638646587\n",
      "4.731851934532375\n",
      "loss  0.040146966\n",
      "episode: 109 - cum reward -106.90607538196\n",
      "episode: 110 - cum reward 89.06003623970949\n",
      "episode: 111 - cum reward 50.80917306319995\n",
      "episode: 112 - cum reward -138.18460549898614\n",
      "episode: 113 - cum reward -114.87255095554245\n",
      "episode: 114 - cum reward -113.0694866585397\n",
      "episode: 115 - cum reward -165.21743859332452\n",
      "episode: 116 - cum reward -53.2163636853099\n",
      "episode: 117 - cum reward -89.85210779734197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 118 - cum reward -30.939273351842203\n",
      "episode: 119 - cum reward 83.75108754642108\n",
      "0.0910284429801067\n",
      "-66.29582977294922\n",
      "33.60968203840678\n",
      "4.782722005987471\n",
      "loss  -0.15136275\n",
      "episode: 120 - cum reward -127.7296877413506\n",
      "episode: 121 - cum reward -47.37488191119409\n",
      "episode: 122 - cum reward -201.5246072866475\n",
      "episode: 123 - cum reward -69.22632130061918\n",
      "episode: 124 - cum reward -131.8582223879456\n",
      "episode: 125 - cum reward -132.27978076841302\n",
      "episode: 126 - cum reward -91.78998025289884\n",
      "episode: 127 - cum reward -133.66091062907685\n",
      "episode: 128 - cum reward -108.57776135791242\n",
      "episode: 129 - cum reward -34.24535062213563\n",
      "episode: 130 - cum reward -94.63917081117404\n",
      "-0.0346758702405022\n",
      "-20.359222412109375\n",
      "63.897746147145824\n",
      "2.5229538186706804\n",
      "loss  -0.08508712\n",
      "episode: 131 - cum reward -102.4606704593317\n",
      "episode: 132 - cum reward 12.83033047133247\n",
      "episode: 133 - cum reward -105.4859218398813\n",
      "episode: 134 - cum reward -185.28300519797534\n",
      "episode: 135 - cum reward 49.447321615503185\n",
      "episode: 136 - cum reward -80.06241797305438\n",
      "episode: 137 - cum reward -71.8672110003753\n",
      "episode: 138 - cum reward -72.57271686489378\n",
      "episode: 139 - cum reward -126.19750364759624\n",
      "episode: 140 - cum reward 40.92670722456728\n",
      "episode: 141 - cum reward -69.84274683273568\n",
      "0.0036138199830857776\n",
      "-86.92252349853516\n",
      "32.233455630339435\n",
      "4.724952431808968\n",
      "loss  -0.12469114\n",
      "episode: 142 - cum reward 91.6851272640511\n",
      "episode: 143 - cum reward -110.3268770884952\n",
      "episode: 144 - cum reward -87.28262333270183\n",
      "episode: 145 - cum reward -122.94655258154357\n",
      "episode: 146 - cum reward -113.43740285614324\n",
      "episode: 147 - cum reward -53.933144878667036\n",
      "episode: 148 - cum reward -127.40766380088239\n",
      "episode: 149 - cum reward -87.99503307954268\n",
      "episode: 150 - cum reward -94.61601515134785\n",
      "episode: 151 - cum reward -87.02261905376184\n",
      "episode: 152 - cum reward -251.33591757305365\n",
      "-0.07084554954988784\n",
      "-44.732791900634766\n",
      "45.15066178381085\n",
      "2.146447837560253\n",
      "loss  -0.17877002\n",
      "episode: 153 - cum reward -111.03197884116524\n",
      "episode: 154 - cum reward 56.82854946801981\n",
      "episode: 155 - cum reward 53.45078348147874\n",
      "episode: 156 - cum reward -3.1184323884562417\n",
      "episode: 157 - cum reward 49.30300410756516\n",
      "episode: 158 - cum reward 62.023111742975544\n",
      "episode: 159 - cum reward -5.565313042424492\n",
      "episode: 160 - cum reward 77.40893388315575\n",
      "episode: 161 - cum reward 72.59678368726583\n",
      "episode: 162 - cum reward -21.09297112928985\n",
      "episode: 163 - cum reward 48.07194946661882\n",
      "0.07467539969345913\n",
      "-93.81529378890991\n",
      "28.044469166797946\n",
      "4.227756654909067\n",
      "loss  -0.2247485\n",
      "episode: 164 - cum reward -43.04481298049792\n",
      "episode: 165 - cum reward -121.0021464780871\n",
      "episode: 166 - cum reward -138.94040292506398\n",
      "episode: 167 - cum reward -177.56598541852858\n",
      "episode: 168 - cum reward -106.11719000679558\n",
      "episode: 169 - cum reward -103.92266931865245\n",
      "episode: 170 - cum reward -133.575794734882\n",
      "episode: 171 - cum reward -123.78445446398234\n",
      "episode: 172 - cum reward -131.9475445405991\n",
      "episode: 173 - cum reward -149.9890713786126\n",
      "episode: 174 - cum reward -141.86835285545706\n",
      "-0.1080932644408254\n",
      "-6.336991508758462\n",
      "23.592517117875815\n",
      "1.849002341241483\n",
      "loss  -0.17118298\n",
      "episode: 175 - cum reward -87.719693183022\n",
      "episode: 176 - cum reward -186.57008465463957\n",
      "episode: 177 - cum reward -211.50074263636284\n",
      "episode: 178 - cum reward -128.8558305644369\n",
      "episode: 179 - cum reward -196.54835005315402\n",
      "episode: 180 - cum reward -183.89711895120877\n",
      "episode: 181 - cum reward -178.36440405745222\n",
      "episode: 182 - cum reward -181.1107934940884\n",
      "episode: 183 - cum reward -184.32842546598044\n",
      "episode: 184 - cum reward -216.92243812906088\n",
      "episode: 185 - cum reward -154.672198360586\n",
      "-0.2391852135315642\n",
      "-86.82855033874512\n",
      "10.815953670452018\n",
      "3.240852351437435\n",
      "loss  -0.14843668\n",
      "episode: 186 - cum reward -210.96785370632077\n",
      "episode: 187 - cum reward -149.76369649831705\n",
      "episode: 188 - cum reward -193.75216871283374\n",
      "episode: 189 - cum reward -164.8737867263636\n",
      "episode: 190 - cum reward -131.78440456747634\n",
      "episode: 191 - cum reward -135.10864751904646\n",
      "episode: 192 - cum reward -159.0915393679732\n",
      "episode: 193 - cum reward -150.5008698944955\n",
      "episode: 194 - cum reward -129.27758771854667\n",
      "episode: 195 - cum reward -183.02649199426446\n",
      "episode: 196 - cum reward -115.81969084265691\n",
      "-0.138187917768805\n",
      "-6.0525514760615415\n",
      "16.10293387982759\n",
      "1.9991869205616506\n",
      "loss  -0.06750456\n",
      "episode: 197 - cum reward -134.16216789194533\n",
      "episode: 198 - cum reward -89.29659816635603\n",
      "episode: 199 - cum reward -197.02679973550096\n",
      "episode: 200 - cum reward -110.44316176203301\n",
      "episode: 201 - cum reward -54.54315758936224\n",
      "episode: 202 - cum reward -89.41560656900239\n",
      "episode: 203 - cum reward -79.66908656942896\n",
      "episode: 204 - cum reward -41.26895235976191\n",
      "episode: 205 - cum reward -126.76012880683378\n",
      "episode: 206 - cum reward -165.0943078080285\n",
      "episode: 207 - cum reward -170.93090566260156\n",
      "-0.11374441344126683\n",
      "-94.66185760498047\n",
      "17.281744246286205\n",
      "3.066683491807916\n",
      "loss  -0.29399672\n",
      "episode: 208 - cum reward -9.041334318307001\n",
      "episode: 209 - cum reward -154.2487934187462\n",
      "episode: 210 - cum reward -163.0688458276065\n",
      "episode: 211 - cum reward -155.38911419728606\n",
      "episode: 212 - cum reward -96.42933779545804\n",
      "episode: 213 - cum reward -145.39221908894748\n",
      "episode: 214 - cum reward -139.91793817006777\n",
      "episode: 215 - cum reward -126.42056503378082\n",
      "episode: 216 - cum reward -130.12502119140956\n",
      "episode: 217 - cum reward -168.41062387448255\n",
      "episode: 218 - cum reward -114.90883019839576\n",
      "-0.1283934503040981\n",
      "-5.799625551247875\n",
      "13.649565969209364\n",
      "1.9312917713124584\n",
      "loss  -0.14712872\n",
      "episode: 219 - cum reward -177.39588830538014\n",
      "episode: 220 - cum reward -151.93902306300416\n",
      "episode: 221 - cum reward -275.3204896389268\n",
      "episode: 222 - cum reward -96.38481204579335\n",
      "episode: 223 - cum reward -202.04228653926032\n",
      "episode: 224 - cum reward 45.21256541591964\n",
      "episode: 225 - cum reward -181.14105899557325\n",
      "episode: 226 - cum reward -168.56316590984164\n",
      "episode: 227 - cum reward -105.87560229730741\n",
      "episode: 228 - cum reward -127.79942438394511\n",
      "episode: 229 - cum reward -227.94070680002136\n",
      "-0.16412372024428606\n",
      "-94.77457237243652\n",
      "14.108195324207346\n",
      "2.9042900372031593\n",
      "loss  -0.25920555\n",
      "episode: 230 - cum reward -76.88369393735525\n",
      "episode: 231 - cum reward -141.87036343492287\n",
      "episode: 232 - cum reward -153.47604921193724\n",
      "episode: 233 - cum reward -115.00913924941354\n",
      "episode: 234 - cum reward -124.49156919607725\n",
      "episode: 235 - cum reward -153.75708124613197\n",
      "episode: 236 - cum reward -131.93397679210705\n",
      "episode: 237 - cum reward -127.76703769754572\n",
      "episode: 238 - cum reward -139.49565548139358\n",
      "episode: 239 - cum reward -123.7502350103729\n",
      "episode: 240 - cum reward -181.04167502483833\n",
      "-0.12662590950392366\n",
      "-5.799605508126167\n",
      "12.991205348902792\n",
      "1.876505074159686\n",
      "loss  -0.18761316\n",
      "episode: 241 - cum reward -136.08644820074062\n",
      "episode: 242 - cum reward -147.2093148499187\n",
      "episode: 243 - cum reward 49.19632556638791\n",
      "episode: 244 - cum reward 148.80862839592203\n",
      "episode: 245 - cum reward -115.58159176400906\n",
      "episode: 246 - cum reward -188.44218628838652\n",
      "episode: 247 - cum reward -34.85909615355122\n",
      "episode: 248 - cum reward -124.69629576958738\n",
      "episode: 249 - cum reward -110.59082939317068\n",
      "episode: 250 - cum reward -221.2297465451378\n",
      "episode: 251 - cum reward 11.825519708377003\n",
      "-0.07462984772875986\n",
      "-93.76863384246826\n",
      "105.43467903137207\n",
      "3.259184451397438\n",
      "loss  -0.2798618\n",
      "episode: 252 - cum reward -84.94099195258126\n",
      "episode: 253 - cum reward -166.23525170178792\n",
      "episode: 254 - cum reward -180.90243812546558\n",
      "episode: 255 - cum reward -161.19556020879293\n",
      "episode: 256 - cum reward -116.00471863395012\n",
      "episode: 257 - cum reward -136.93740784902948\n",
      "episode: 258 - cum reward -117.43154949101994\n",
      "episode: 259 - cum reward -130.66276843253854\n",
      "episode: 260 - cum reward -190.05329401865103\n",
      "episode: 261 - cum reward -153.67308066501752\n",
      "episode: 262 - cum reward -151.227849802813\n",
      "-0.14282204509877292\n",
      "-5.761240586847453\n",
      "12.969063605103914\n",
      "1.9796738376860727\n",
      "loss  -0.09393469\n",
      "episode: 263 - cum reward -183.87846761993433\n",
      "episode: 264 - cum reward 28.19665569999146\n",
      "episode: 265 - cum reward -116.07392154256993\n",
      "episode: 266 - cum reward -39.657885769973895\n",
      "episode: 267 - cum reward -4.977978990913954\n",
      "episode: 268 - cum reward -87.18977787033229\n",
      "episode: 269 - cum reward -90.42414659499664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 270 - cum reward -58.2075791028067\n",
      "episode: 271 - cum reward -114.36150103688873\n",
      "episode: 272 - cum reward -31.150895654663714\n",
      "episode: 273 - cum reward -23.243892552802464\n",
      "-0.04583874374263967\n",
      "-94.40737533569336\n",
      "17.210923539207332\n",
      "2.4301084118632463\n",
      "loss  -0.4204816\n",
      "episode: 274 - cum reward -72.89728689894827\n",
      "episode: 275 - cum reward -184.7719007078856\n",
      "episode: 276 - cum reward -145.8524113738241\n",
      "episode: 277 - cum reward -129.73092485475036\n",
      "episode: 278 - cum reward -179.92326352818966\n",
      "episode: 279 - cum reward -129.1591215854279\n",
      "episode: 280 - cum reward -108.038797460422\n",
      "episode: 281 - cum reward -130.1862740015889\n",
      "episode: 282 - cum reward -114.42713079612862\n",
      "episode: 283 - cum reward -153.33079076065547\n",
      "episode: 284 - cum reward -130.78570338568795\n",
      "-0.13302253232762848\n",
      "-6.186378481815968\n",
      "13.542862364550446\n",
      "1.993552718304925\n",
      "loss  -0.06656126\n",
      "episode: 285 - cum reward -153.15787054447796\n",
      "episode: 286 - cum reward -37.42036811570414\n",
      "episode: 287 - cum reward 14.936145523316977\n",
      "episode: 288 - cum reward -70.27956488241124\n",
      "episode: 289 - cum reward -86.28290220806437\n",
      "episode: 290 - cum reward 130.99254490826905\n",
      "episode: 291 - cum reward 9.810798770580604\n",
      "episode: 292 - cum reward -43.95700904627327\n",
      "episode: 293 - cum reward -25.876636704955747\n",
      "episode: 294 - cum reward -122.25958872776796\n",
      "episode: 295 - cum reward -46.36919329131228\n",
      "-0.021556381338846985\n",
      "-94.30782890319824\n",
      "22.145678696534112\n",
      "2.70688458114526\n",
      "loss  -0.3338249\n",
      "episode: 296 - cum reward 49.42881999142314\n",
      "episode: 297 - cum reward -130.4184069603458\n",
      "episode: 298 - cum reward -152.9218147425486\n",
      "episode: 299 - cum reward -139.05817097242752\n",
      "episode: 300 - cum reward -98.0414962962946\n",
      "episode: 301 - cum reward -159.49619988563782\n",
      "episode: 302 - cum reward -155.44564567544987\n",
      "episode: 303 - cum reward -157.93516446466901\n",
      "episode: 304 - cum reward -127.58448051966411\n",
      "episode: 305 - cum reward -100.08854011602091\n",
      "episode: 306 - cum reward -116.47718265262955\n",
      "-0.1252947778459801\n",
      "-5.8304709782592745\n",
      "10.416146045825828\n",
      "1.8842512784796914\n",
      "loss  -0.19804996\n",
      "episode: 307 - cum reward -122.81503946100341\n",
      "episode: 308 - cum reward -76.35247266650022\n",
      "episode: 309 - cum reward -11.965292497879405\n",
      "episode: 310 - cum reward 3.648260839692842\n",
      "episode: 311 - cum reward -15.587065419432987\n",
      "episode: 312 - cum reward 30.509537259788374\n",
      "episode: 313 - cum reward -58.49963393032662\n",
      "episode: 314 - cum reward -45.00560020323432\n",
      "episode: 315 - cum reward -65.47937580705153\n",
      "episode: 316 - cum reward 30.65435301803339\n",
      "episode: 317 - cum reward -31.280195663243724\n",
      "-0.017429251337328838\n",
      "-95.28112840652466\n",
      "26.255721985184323\n",
      "2.5330670592622826\n",
      "loss  -0.37397993\n",
      "episode: 318 - cum reward 101.63240624179632\n",
      "episode: 319 - cum reward -115.51541366195123\n",
      "episode: 320 - cum reward -149.3627974773625\n",
      "episode: 321 - cum reward -135.40998864347847\n",
      "episode: 322 - cum reward -123.07018579873879\n",
      "episode: 323 - cum reward -170.63381450663323\n",
      "episode: 324 - cum reward -107.38125617222354\n",
      "episode: 325 - cum reward 23.626413171279243\n",
      "episode: 326 - cum reward -154.82326013412373\n",
      "episode: 327 - cum reward -127.88381827651251\n",
      "episode: 328 - cum reward -99.17514871557026\n",
      "-0.10950301877453335\n",
      "-21.442153945844264\n",
      "19.742122904703237\n",
      "2.192732142119998\n",
      "loss  -0.09180434\n",
      "episode: 329 - cum reward -200.24193304149574\n",
      "episode: 330 - cum reward -72.69464799418853\n",
      "episode: 331 - cum reward -45.408522823519554\n",
      "episode: 332 - cum reward 16.222697819141956\n",
      "episode: 333 - cum reward -42.074225486252004\n",
      "episode: 334 - cum reward -33.45157029981638\n",
      "episode: 335 - cum reward -25.390664815510405\n",
      "episode: 336 - cum reward -47.59441209904276\n",
      "episode: 337 - cum reward 78.18031436573501\n",
      "episode: 338 - cum reward 75.21750937418769\n",
      "episode: 339 - cum reward -70.33783842764203\n",
      "-0.010921596012197785\n",
      "-94.50461912155151\n",
      "106.87475538253784\n",
      "2.915757861328792\n",
      "loss  -0.3326524\n",
      "episode: 340 - cum reward 49.74388846446675\n",
      "episode: 341 - cum reward -80.62450986301374\n",
      "episode: 342 - cum reward -198.86899983533382\n",
      "episode: 343 - cum reward -65.18647574535544\n",
      "episode: 344 - cum reward -79.55725330109107\n",
      "episode: 345 - cum reward -150.55045286359834\n",
      "episode: 346 - cum reward -91.10998531528125\n",
      "episode: 347 - cum reward -162.54241882139667\n",
      "episode: 348 - cum reward -161.54413885955353\n",
      "episode: 349 - cum reward -179.22034888229007\n",
      "episode: 350 - cum reward -181.0194188034117\n",
      "-0.1370644952502098\n",
      "-86.12279891967773\n",
      "11.1947968430911\n",
      "2.178209425479503\n",
      "loss  -0.11733987\n",
      "episode: 351 - cum reward 4.710232528564161\n",
      "episode: 352 - cum reward 35.85456481300196\n",
      "episode: 353 - cum reward -11.90065736718792\n",
      "episode: 354 - cum reward -53.64453667521555\n",
      "episode: 355 - cum reward -37.49180708257745\n",
      "episode: 356 - cum reward 24.686449090757517\n",
      "episode: 357 - cum reward -76.5029984709998\n",
      "episode: 358 - cum reward -33.23292147051455\n",
      "episode: 359 - cum reward -74.54607373757875\n",
      "episode: 360 - cum reward 17.49631286927717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d1ea5bdcc751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/notebooks/tools.py\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-66ba180a87e4>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env, compiled_model=q_agent.model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 800)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss_continuous(returns):\n",
    "    def modified_mse(action_true, action_pred):\n",
    "        loss = -K.mean(returns * K.mean((action_true - action_pred)**2, axis=1))\n",
    "        return loss\n",
    "    return modified_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgentContinuous:                                                                                                                                                                                                \n",
    "    def __init__(self, env, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        pass\n",
    "    def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgentContinuous):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, gamma = .99, epsilon = .01, alpha = .01, memory_size = 6):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.normal(prob)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.vstack(episode[:,1])\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Y_values = np.vstack([ep[3] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs,Y_values)\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                loss = policy_gradient_loss(discounted_returns)#baselines)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, Y_value])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,410\n",
      "Trainable params: 1,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward -558.0030592811014\n",
      "episode: 1 - cum reward -627.1235896057083\n",
      "episode: 2 - cum reward -478.171866270921\n",
      "episode: 3 - cum reward -26.19963633216399\n",
      "episode: 4 - cum reward -523.9392390330768\n",
      "episode: 5 - cum reward -200.39749380120867\n",
      "episode: 6 - cum reward -117.65392450433579\n",
      "episode: 7 - cum reward -530.3594154558305\n",
      "episode: 8 - cum reward -35.99259969526929\n",
      "episode: 9 - cum reward -454.2532442722762\n",
      "episode: 10 - cum reward -545.1567592785166\n",
      "episode: 11 - cum reward -790.4951432465905\n",
      "episode: 12 - cum reward -522.2525795182219\n",
      "episode: 13 - cum reward -562.0789230777125\n",
      "episode: 14 - cum reward -564.4644572156662\n",
      "episode: 15 - cum reward -387.68334994859384\n",
      "episode: 16 - cum reward -481.19986938235155\n",
      "episode: 17 - cum reward -674.4386511587968\n",
      "episode: 18 - cum reward -299.7152027123494\n",
      "episode: 19 - cum reward -634.5650814052633\n",
      "episode: 20 - cum reward -527.9133981916659\n",
      "episode: 21 - cum reward -640.4941111431478\n",
      "episode: 22 - cum reward -671.9517231445145\n",
      "episode: 23 - cum reward -283.39378504568833\n",
      "episode: 24 - cum reward -471.73165159305955\n",
      "episode: 25 - cum reward -748.8423517335302\n",
      "episode: 26 - cum reward -645.2584174897795\n",
      "episode: 27 - cum reward -861.3856585995883\n",
      "episode: 28 - cum reward -704.0829419241172\n",
      "episode: 29 - cum reward -1089.6799384224828\n",
      "episode: 30 - cum reward -723.2796756434669\n",
      "episode: 31 - cum reward -951.7728537452163\n",
      "episode: 32 - cum reward -1473.525821037492\n",
      "episode: 33 - cum reward -1319.2265038638561\n",
      "episode: 34 - cum reward -1947.6515368950836\n",
      "episode: 35 - cum reward -668.5290146651461\n",
      "episode: 36 - cum reward -585.4416104156077\n",
      "episode: 37 - cum reward -331.0386419259671\n",
      "episode: 38 - cum reward -254.9114672111579\n",
      "episode: 39 - cum reward -600.0098562920092\n",
      "episode: 40 - cum reward -677.3307131336645\n",
      "episode: 41 - cum reward -251.99593548539096\n",
      "episode: 42 - cum reward -806.5171826439905\n",
      "episode: 43 - cum reward -706.8040866501027\n",
      "episode: 44 - cum reward -501.29982607553643\n",
      "episode: 45 - cum reward -858.89978535774\n",
      "episode: 46 - cum reward -665.7348837671206\n",
      "episode: 47 - cum reward -634.3524300233089\n",
      "episode: 48 - cum reward -753.2247990202144\n",
      "episode: 49 - cum reward -880.4386708590309\n",
      "episode: 50 - cum reward -503.1230798161159\n",
      "episode: 51 - cum reward -852.1642326446994\n",
      "episode: 52 - cum reward -807.031224211955\n",
      "episode: 53 - cum reward -568.3311553058657\n",
      "episode: 54 - cum reward -868.016145399487\n",
      "episode: 55 - cum reward -596.0537499255609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9dcc911b1a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLanderContinuous-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReinforceAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/notebooks/tools.py\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mrews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-52453a78595b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, current_state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "q_agent = ReinforceAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
