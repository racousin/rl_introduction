{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)G_t] \\text{REINFORCE}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)V_w(s)] \\text{V actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a)] \\text{Q actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)A_w(s,a)] \\text{Advantage actor-critic}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q actor critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 3):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.turn = 0\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(4, activation='linear')(input_state)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward]))\n",
    "        else:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                episodes = np.asarray(self.episodes)\n",
    "                self.episodes = []\n",
    "                Xs = np.vstack(episodes[:,0])\n",
    "                Ys = np.vstack(episodes[:,1])\n",
    "                discounted_returns = np.hstack(episodes[:,2])\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                print(baselines.max())\n",
    "                loss = policy_gradient_loss(baselines)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.model_critic.train_on_batch(Xs,discounted_returns.astype(np.dtype('float32')))\n",
    "            else:\n",
    "                self.episodes.append((X,Y,discounted_return))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode_train(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            agent.train(current_state, action, reward, state, done)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_32\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_93 (Dense)             (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_94 (Dense)             (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_95 (Dense)             (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_33\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_96 (Dense)             (None, 4)                 20        \n",
      "_________________________________________________________________\n",
      "dense_97 (Dense)             (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 25\n",
      "Trainable params: 25\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 35.0\n",
      "episode: 1 - cum reward 10.0\n",
      "episode: 2 - cum reward 13.0\n",
      "2.687089\n",
      "episode: 3 - cum reward 12.0\n",
      "episode: 4 - cum reward 9.0\n",
      "episode: 5 - cum reward 16.0\n",
      "episode: 6 - cum reward 30.0\n",
      "2.5916283\n",
      "episode: 7 - cum reward 15.0\n",
      "episode: 8 - cum reward 9.0\n",
      "episode: 9 - cum reward 19.0\n",
      "episode: 10 - cum reward 14.0\n",
      "1.8390346\n",
      "episode: 11 - cum reward 13.0\n",
      "episode: 12 - cum reward 16.0\n",
      "episode: 13 - cum reward 19.0\n",
      "episode: 14 - cum reward 19.0\n",
      "1.5019122\n",
      "episode: 15 - cum reward 14.0\n",
      "episode: 16 - cum reward 34.0\n",
      "episode: 17 - cum reward 36.0\n",
      "episode: 18 - cum reward 22.0\n",
      "1.3748238\n",
      "episode: 19 - cum reward 15.0\n",
      "episode: 20 - cum reward 19.0\n",
      "episode: 21 - cum reward 26.0\n",
      "episode: 22 - cum reward 18.0\n",
      "2.171091\n",
      "episode: 23 - cum reward 21.0\n",
      "episode: 24 - cum reward 25.0\n",
      "episode: 25 - cum reward 26.0\n",
      "episode: 26 - cum reward 9.0\n",
      "1.9118848\n",
      "episode: 27 - cum reward 22.0\n",
      "episode: 28 - cum reward 55.0\n",
      "episode: 29 - cum reward 34.0\n",
      "episode: 30 - cum reward 21.0\n",
      "2.8197813\n",
      "episode: 31 - cum reward 12.0\n",
      "episode: 32 - cum reward 12.0\n",
      "episode: 33 - cum reward 30.0\n",
      "episode: 34 - cum reward 11.0\n",
      "2.9140737\n",
      "episode: 35 - cum reward 54.0\n",
      "episode: 36 - cum reward 50.0\n",
      "episode: 37 - cum reward 26.0\n",
      "episode: 38 - cum reward 11.0\n",
      "2.00187\n",
      "episode: 39 - cum reward 17.0\n",
      "episode: 40 - cum reward 39.0\n",
      "episode: 41 - cum reward 13.0\n",
      "episode: 42 - cum reward 27.0\n",
      "2.7364254\n",
      "episode: 43 - cum reward 20.0\n",
      "episode: 44 - cum reward 12.0\n",
      "episode: 45 - cum reward 15.0\n",
      "episode: 46 - cum reward 14.0\n",
      "0.79769325\n",
      "episode: 47 - cum reward 12.0\n",
      "episode: 48 - cum reward 19.0\n",
      "episode: 49 - cum reward 13.0\n",
      "episode: 50 - cum reward 14.0\n",
      "2.627018\n",
      "episode: 51 - cum reward 10.0\n",
      "episode: 52 - cum reward 16.0\n",
      "episode: 53 - cum reward 13.0\n",
      "episode: 54 - cum reward 21.0\n",
      "2.2763894\n",
      "episode: 55 - cum reward 29.0\n",
      "episode: 56 - cum reward 13.0\n",
      "episode: 57 - cum reward 24.0\n",
      "episode: 58 - cum reward 59.0\n",
      "2.2917697\n",
      "episode: 59 - cum reward 15.0\n",
      "episode: 60 - cum reward 15.0\n",
      "episode: 61 - cum reward 55.0\n",
      "episode: 62 - cum reward 18.0\n",
      "3.2908373\n",
      "episode: 63 - cum reward 28.0\n",
      "episode: 64 - cum reward 13.0\n",
      "episode: 65 - cum reward 15.0\n",
      "episode: 66 - cum reward 13.0\n",
      "2.5184116\n",
      "episode: 67 - cum reward 13.0\n",
      "episode: 68 - cum reward 36.0\n",
      "episode: 69 - cum reward 19.0\n",
      "episode: 70 - cum reward 13.0\n",
      "3.321287\n",
      "episode: 71 - cum reward 42.0\n",
      "episode: 72 - cum reward 22.0\n",
      "episode: 73 - cum reward 11.0\n",
      "episode: 74 - cum reward 23.0\n",
      "2.6373217\n",
      "episode: 75 - cum reward 21.0\n",
      "episode: 76 - cum reward 19.0\n",
      "episode: 77 - cum reward 26.0\n",
      "episode: 78 - cum reward 41.0\n",
      "2.9925778\n",
      "episode: 79 - cum reward 22.0\n",
      "episode: 80 - cum reward 12.0\n",
      "episode: 81 - cum reward 36.0\n",
      "episode: 82 - cum reward 13.0\n",
      "3.8229573\n",
      "episode: 83 - cum reward 13.0\n",
      "episode: 84 - cum reward 13.0\n",
      "episode: 85 - cum reward 47.0\n",
      "episode: 86 - cum reward 24.0\n",
      "2.0160687\n",
      "episode: 87 - cum reward 24.0\n",
      "episode: 88 - cum reward 15.0\n",
      "episode: 89 - cum reward 16.0\n",
      "episode: 90 - cum reward 41.0\n",
      "3.095934\n",
      "episode: 91 - cum reward 15.0\n",
      "episode: 92 - cum reward 25.0\n",
      "episode: 93 - cum reward 10.0\n",
      "episode: 94 - cum reward 59.0\n",
      "3.9151666\n",
      "episode: 95 - cum reward 34.0\n",
      "episode: 96 - cum reward 26.0\n",
      "episode: 97 - cum reward 15.0\n",
      "episode: 98 - cum reward 32.0\n",
      "2.968985\n",
      "episode: 99 - cum reward 21.0\n",
      "episode: 100 - cum reward 28.0\n",
      "episode: 101 - cum reward 15.0\n",
      "episode: 102 - cum reward 27.0\n",
      "3.312011\n",
      "episode: 103 - cum reward 16.0\n",
      "episode: 104 - cum reward 21.0\n",
      "episode: 105 - cum reward 12.0\n",
      "episode: 106 - cum reward 14.0\n",
      "2.7001398\n",
      "episode: 107 - cum reward 21.0\n",
      "episode: 108 - cum reward 11.0\n",
      "episode: 109 - cum reward 17.0\n",
      "episode: 110 - cum reward 9.0\n",
      "3.4938788\n",
      "episode: 111 - cum reward 21.0\n",
      "episode: 112 - cum reward 28.0\n",
      "episode: 113 - cum reward 28.0\n",
      "episode: 114 - cum reward 19.0\n",
      "3.6666915\n",
      "episode: 115 - cum reward 25.0\n",
      "episode: 116 - cum reward 13.0\n",
      "episode: 117 - cum reward 14.0\n",
      "episode: 118 - cum reward 25.0\n",
      "3.9815333\n",
      "episode: 119 - cum reward 12.0\n",
      "episode: 120 - cum reward 48.0\n",
      "episode: 121 - cum reward 31.0\n",
      "episode: 122 - cum reward 24.0\n",
      "5.395383\n",
      "episode: 123 - cum reward 57.0\n",
      "episode: 124 - cum reward 73.0\n",
      "episode: 125 - cum reward 23.0\n",
      "episode: 126 - cum reward 50.0\n",
      "3.1630688\n",
      "episode: 127 - cum reward 16.0\n",
      "episode: 128 - cum reward 22.0\n",
      "episode: 129 - cum reward 26.0\n",
      "episode: 130 - cum reward 18.0\n",
      "3.4587471\n",
      "episode: 131 - cum reward 40.0\n",
      "episode: 132 - cum reward 16.0\n",
      "episode: 133 - cum reward 11.0\n",
      "episode: 134 - cum reward 28.0\n",
      "3.1022358\n",
      "episode: 135 - cum reward 21.0\n",
      "episode: 136 - cum reward 18.0\n",
      "episode: 137 - cum reward 17.0\n",
      "episode: 138 - cum reward 23.0\n",
      "3.2084079\n",
      "episode: 139 - cum reward 23.0\n",
      "episode: 140 - cum reward 15.0\n",
      "episode: 141 - cum reward 46.0\n",
      "episode: 142 - cum reward 10.0\n",
      "4.2162805\n",
      "episode: 143 - cum reward 23.0\n",
      "episode: 144 - cum reward 15.0\n",
      "episode: 145 - cum reward 9.0\n",
      "episode: 146 - cum reward 19.0\n",
      "4.8735304\n",
      "episode: 147 - cum reward 43.0\n",
      "episode: 148 - cum reward 12.0\n",
      "episode: 149 - cum reward 55.0\n",
      "episode: 150 - cum reward 19.0\n",
      "4.50633\n",
      "episode: 151 - cum reward 35.0\n",
      "episode: 152 - cum reward 15.0\n",
      "episode: 153 - cum reward 17.0\n",
      "episode: 154 - cum reward 33.0\n",
      "4.15814\n",
      "episode: 155 - cum reward 17.0\n",
      "episode: 156 - cum reward 64.0\n",
      "episode: 157 - cum reward 28.0\n",
      "episode: 158 - cum reward 19.0\n",
      "4.9210715\n",
      "episode: 159 - cum reward 20.0\n",
      "episode: 160 - cum reward 15.0\n",
      "episode: 161 - cum reward 48.0\n",
      "episode: 162 - cum reward 23.0\n",
      "5.344077\n",
      "episode: 163 - cum reward 23.0\n",
      "episode: 164 - cum reward 14.0\n",
      "episode: 165 - cum reward 24.0\n",
      "episode: 166 - cum reward 35.0\n",
      "4.08932\n",
      "episode: 167 - cum reward 22.0\n",
      "episode: 168 - cum reward 23.0\n",
      "episode: 169 - cum reward 18.0\n",
      "episode: 170 - cum reward 20.0\n",
      "4.5683904\n",
      "episode: 171 - cum reward 13.0\n",
      "episode: 172 - cum reward 21.0\n",
      "episode: 173 - cum reward 19.0\n",
      "episode: 174 - cum reward 34.0\n",
      "5.665058\n",
      "episode: 175 - cum reward 37.0\n",
      "episode: 176 - cum reward 28.0\n",
      "episode: 177 - cum reward 29.0\n",
      "episode: 178 - cum reward 26.0\n",
      "4.6850133\n",
      "episode: 179 - cum reward 13.0\n",
      "episode: 180 - cum reward 18.0\n",
      "episode: 181 - cum reward 11.0\n",
      "episode: 182 - cum reward 15.0\n",
      "5.2064986\n",
      "episode: 183 - cum reward 28.0\n",
      "episode: 184 - cum reward 41.0\n",
      "episode: 185 - cum reward 22.0\n",
      "episode: 186 - cum reward 14.0\n",
      "5.57982\n",
      "episode: 187 - cum reward 72.0\n",
      "episode: 188 - cum reward 14.0\n",
      "episode: 189 - cum reward 49.0\n",
      "episode: 190 - cum reward 21.0\n",
      "4.4949584\n",
      "episode: 191 - cum reward 20.0\n",
      "episode: 192 - cum reward 17.0\n",
      "episode: 193 - cum reward 26.0\n",
      "episode: 194 - cum reward 16.0\n",
      "5.6250815\n",
      "episode: 195 - cum reward 20.0\n",
      "episode: 196 - cum reward 24.0\n",
      "episode: 197 - cum reward 26.0\n",
      "episode: 198 - cum reward 49.0\n",
      "4.3193774\n",
      "episode: 199 - cum reward 15.0\n",
      "episode: 200 - cum reward 29.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 201 - cum reward 24.0\n",
      "episode: 202 - cum reward 16.0\n",
      "3.8061812\n",
      "episode: 203 - cum reward 14.0\n",
      "episode: 204 - cum reward 13.0\n",
      "episode: 205 - cum reward 9.0\n",
      "episode: 206 - cum reward 30.0\n",
      "5.578916\n",
      "episode: 207 - cum reward 26.0\n",
      "episode: 208 - cum reward 25.0\n",
      "episode: 209 - cum reward 35.0\n",
      "episode: 210 - cum reward 16.0\n",
      "6.908552\n",
      "episode: 211 - cum reward 31.0\n",
      "episode: 212 - cum reward 15.0\n",
      "episode: 213 - cum reward 13.0\n",
      "episode: 214 - cum reward 20.0\n",
      "5.6664243\n",
      "episode: 215 - cum reward 13.0\n",
      "episode: 216 - cum reward 41.0\n",
      "episode: 217 - cum reward 15.0\n",
      "episode: 218 - cum reward 13.0\n",
      "5.7489457\n",
      "episode: 219 - cum reward 9.0\n",
      "episode: 220 - cum reward 25.0\n",
      "episode: 221 - cum reward 19.0\n",
      "episode: 222 - cum reward 12.0\n",
      "5.725097\n",
      "episode: 223 - cum reward 79.0\n",
      "episode: 224 - cum reward 28.0\n",
      "episode: 225 - cum reward 22.0\n",
      "episode: 226 - cum reward 10.0\n",
      "7.7267914\n",
      "episode: 227 - cum reward 13.0\n",
      "episode: 228 - cum reward 39.0\n",
      "episode: 229 - cum reward 23.0\n",
      "episode: 230 - cum reward 35.0\n",
      "6.062957\n",
      "episode: 231 - cum reward 19.0\n",
      "episode: 232 - cum reward 16.0\n",
      "episode: 233 - cum reward 11.0\n",
      "episode: 234 - cum reward 10.0\n",
      "7.3033504\n",
      "episode: 235 - cum reward 14.0\n",
      "episode: 236 - cum reward 14.0\n",
      "episode: 237 - cum reward 15.0\n",
      "episode: 238 - cum reward 33.0\n",
      "7.4539948\n",
      "episode: 239 - cum reward 31.0\n",
      "episode: 240 - cum reward 16.0\n",
      "episode: 241 - cum reward 28.0\n",
      "episode: 242 - cum reward 43.0\n",
      "10.792455\n",
      "episode: 243 - cum reward 19.0\n",
      "episode: 244 - cum reward 14.0\n",
      "episode: 245 - cum reward 32.0\n",
      "episode: 246 - cum reward 30.0\n",
      "6.7612834\n",
      "episode: 247 - cum reward 61.0\n",
      "episode: 248 - cum reward 16.0\n",
      "episode: 249 - cum reward 25.0\n",
      "episode: 250 - cum reward 80.0\n",
      "5.7122917\n",
      "episode: 251 - cum reward 18.0\n",
      "episode: 252 - cum reward 18.0\n",
      "episode: 253 - cum reward 12.0\n",
      "episode: 254 - cum reward 15.0\n",
      "6.1873155\n",
      "episode: 255 - cum reward 40.0\n",
      "episode: 256 - cum reward 23.0\n",
      "episode: 257 - cum reward 29.0\n",
      "episode: 258 - cum reward 33.0\n",
      "6.1672306\n",
      "episode: 259 - cum reward 22.0\n",
      "episode: 260 - cum reward 19.0\n",
      "episode: 261 - cum reward 22.0\n",
      "episode: 262 - cum reward 26.0\n",
      "7.277944\n",
      "episode: 263 - cum reward 17.0\n",
      "episode: 264 - cum reward 20.0\n",
      "episode: 265 - cum reward 15.0\n",
      "episode: 266 - cum reward 53.0\n",
      "6.7732244\n",
      "episode: 267 - cum reward 17.0\n",
      "episode: 268 - cum reward 30.0\n",
      "episode: 269 - cum reward 21.0\n",
      "episode: 270 - cum reward 15.0\n",
      "6.327368\n",
      "episode: 271 - cum reward 14.0\n",
      "episode: 272 - cum reward 24.0\n",
      "episode: 273 - cum reward 26.0\n",
      "episode: 274 - cum reward 16.0\n",
      "7.9675865\n",
      "episode: 275 - cum reward 14.0\n",
      "episode: 276 - cum reward 10.0\n",
      "episode: 277 - cum reward 15.0\n",
      "episode: 278 - cum reward 20.0\n",
      "7.1460805\n",
      "episode: 279 - cum reward 15.0\n",
      "episode: 280 - cum reward 13.0\n",
      "episode: 281 - cum reward 20.0\n",
      "episode: 282 - cum reward 13.0\n",
      "7.633936\n",
      "episode: 283 - cum reward 25.0\n",
      "episode: 284 - cum reward 56.0\n",
      "episode: 285 - cum reward 14.0\n",
      "episode: 286 - cum reward 41.0\n",
      "7.1670337\n",
      "episode: 287 - cum reward 17.0\n",
      "episode: 288 - cum reward 17.0\n",
      "episode: 289 - cum reward 71.0\n",
      "episode: 290 - cum reward 23.0\n",
      "10.979949\n",
      "episode: 291 - cum reward 21.0\n",
      "episode: 292 - cum reward 15.0\n",
      "episode: 293 - cum reward 17.0\n",
      "episode: 294 - cum reward 20.0\n",
      "7.682413\n",
      "episode: 295 - cum reward 20.0\n",
      "episode: 296 - cum reward 12.0\n",
      "episode: 297 - cum reward 11.0\n",
      "episode: 298 - cum reward 16.0\n",
      "7.8563757\n",
      "episode: 299 - cum reward 21.0\n",
      "episode: 300 - cum reward 14.0\n",
      "episode: 301 - cum reward 9.0\n",
      "episode: 302 - cum reward 15.0\n",
      "7.232003\n",
      "episode: 303 - cum reward 21.0\n",
      "episode: 304 - cum reward 15.0\n",
      "episode: 305 - cum reward 54.0\n",
      "episode: 306 - cum reward 10.0\n",
      "9.409284\n",
      "episode: 307 - cum reward 18.0\n",
      "episode: 308 - cum reward 30.0\n",
      "episode: 309 - cum reward 14.0\n",
      "episode: 310 - cum reward 59.0\n",
      "8.572307\n",
      "episode: 311 - cum reward 15.0\n",
      "episode: 312 - cum reward 13.0\n",
      "episode: 313 - cum reward 14.0\n",
      "episode: 314 - cum reward 20.0\n",
      "6.773398\n",
      "episode: 315 - cum reward 10.0\n",
      "episode: 316 - cum reward 13.0\n",
      "episode: 317 - cum reward 31.0\n",
      "episode: 318 - cum reward 26.0\n",
      "6.9527955\n",
      "episode: 319 - cum reward 11.0\n",
      "episode: 320 - cum reward 42.0\n",
      "episode: 321 - cum reward 48.0\n",
      "episode: 322 - cum reward 35.0\n",
      "8.65222\n",
      "episode: 323 - cum reward 11.0\n",
      "episode: 324 - cum reward 14.0\n",
      "episode: 325 - cum reward 16.0\n",
      "episode: 326 - cum reward 24.0\n",
      "7.9400177\n",
      "episode: 327 - cum reward 24.0\n",
      "episode: 328 - cum reward 40.0\n",
      "episode: 329 - cum reward 36.0\n",
      "episode: 330 - cum reward 12.0\n",
      "9.021488\n",
      "episode: 331 - cum reward 10.0\n",
      "episode: 332 - cum reward 22.0\n",
      "episode: 333 - cum reward 24.0\n",
      "episode: 334 - cum reward 9.0\n",
      "7.526346\n",
      "episode: 335 - cum reward 19.0\n",
      "episode: 336 - cum reward 10.0\n",
      "episode: 337 - cum reward 13.0\n",
      "episode: 338 - cum reward 22.0\n",
      "7.27712\n",
      "episode: 339 - cum reward 11.0\n",
      "episode: 340 - cum reward 12.0\n",
      "episode: 341 - cum reward 29.0\n",
      "episode: 342 - cum reward 17.0\n",
      "8.579771\n",
      "episode: 343 - cum reward 69.0\n",
      "episode: 344 - cum reward 58.0\n",
      "episode: 345 - cum reward 20.0\n",
      "episode: 346 - cum reward 15.0\n",
      "8.980951\n",
      "episode: 347 - cum reward 26.0\n",
      "episode: 348 - cum reward 11.0\n",
      "episode: 349 - cum reward 18.0\n",
      "episode: 350 - cum reward 21.0\n",
      "7.3498764\n",
      "episode: 351 - cum reward 14.0\n",
      "episode: 352 - cum reward 28.0\n",
      "episode: 353 - cum reward 18.0\n",
      "episode: 354 - cum reward 31.0\n",
      "8.161653\n",
      "episode: 355 - cum reward 18.0\n",
      "episode: 356 - cum reward 11.0\n",
      "episode: 357 - cum reward 36.0\n",
      "episode: 358 - cum reward 59.0\n",
      "10.667681\n",
      "episode: 359 - cum reward 15.0\n",
      "episode: 360 - cum reward 26.0\n",
      "episode: 361 - cum reward 10.0\n",
      "episode: 362 - cum reward 44.0\n",
      "9.009814\n",
      "episode: 363 - cum reward 19.0\n",
      "episode: 364 - cum reward 68.0\n",
      "episode: 365 - cum reward 21.0\n",
      "episode: 366 - cum reward 15.0\n",
      "8.70613\n",
      "episode: 367 - cum reward 13.0\n",
      "episode: 368 - cum reward 42.0\n",
      "episode: 369 - cum reward 30.0\n",
      "episode: 370 - cum reward 11.0\n",
      "7.868017\n",
      "episode: 371 - cum reward 11.0\n",
      "episode: 372 - cum reward 17.0\n",
      "episode: 373 - cum reward 20.0\n",
      "episode: 374 - cum reward 34.0\n",
      "8.93485\n",
      "episode: 375 - cum reward 15.0\n",
      "episode: 376 - cum reward 17.0\n",
      "episode: 377 - cum reward 25.0\n",
      "episode: 378 - cum reward 31.0\n",
      "7.5807667\n",
      "episode: 379 - cum reward 10.0\n",
      "episode: 380 - cum reward 11.0\n",
      "episode: 381 - cum reward 25.0\n",
      "episode: 382 - cum reward 24.0\n",
      "7.179198\n",
      "episode: 383 - cum reward 23.0\n",
      "episode: 384 - cum reward 20.0\n",
      "episode: 385 - cum reward 24.0\n",
      "episode: 386 - cum reward 13.0\n",
      "8.735035\n",
      "episode: 387 - cum reward 29.0\n",
      "episode: 388 - cum reward 35.0\n",
      "episode: 389 - cum reward 10.0\n",
      "episode: 390 - cum reward 60.0\n",
      "7.9657927\n",
      "episode: 391 - cum reward 40.0\n",
      "episode: 392 - cum reward 22.0\n",
      "episode: 393 - cum reward 18.0\n",
      "episode: 394 - cum reward 22.0\n",
      "8.856618\n",
      "episode: 395 - cum reward 15.0\n",
      "episode: 396 - cum reward 31.0\n",
      "episode: 397 - cum reward 14.0\n",
      "episode: 398 - cum reward 22.0\n",
      "8.674911\n",
      "episode: 399 - cum reward 20.0\n",
      "episode: 400 - cum reward 15.0\n",
      "episode: 401 - cum reward 16.0\n",
      "episode: 402 - cum reward 16.0\n",
      "9.349326\n",
      "episode: 403 - cum reward 18.0\n",
      "episode: 404 - cum reward 79.0\n",
      "episode: 405 - cum reward 13.0\n",
      "episode: 406 - cum reward 10.0\n",
      "14.356126\n",
      "episode: 407 - cum reward 40.0\n",
      "episode: 408 - cum reward 29.0\n",
      "episode: 409 - cum reward 18.0\n",
      "episode: 410 - cum reward 63.0\n",
      "9.61995\n",
      "episode: 411 - cum reward 28.0\n",
      "episode: 412 - cum reward 40.0\n",
      "episode: 413 - cum reward 13.0\n",
      "episode: 414 - cum reward 18.0\n",
      "9.673289\n",
      "episode: 415 - cum reward 28.0\n",
      "episode: 416 - cum reward 10.0\n",
      "episode: 417 - cum reward 15.0\n",
      "episode: 418 - cum reward 17.0\n",
      "9.827763\n",
      "episode: 419 - cum reward 35.0\n",
      "episode: 420 - cum reward 20.0\n",
      "episode: 421 - cum reward 11.0\n",
      "episode: 422 - cum reward 50.0\n",
      "8.288855\n",
      "episode: 423 - cum reward 24.0\n",
      "episode: 424 - cum reward 46.0\n",
      "episode: 425 - cum reward 14.0\n",
      "episode: 426 - cum reward 32.0\n",
      "9.091997\n",
      "episode: 427 - cum reward 16.0\n",
      "episode: 428 - cum reward 18.0\n",
      "episode: 429 - cum reward 59.0\n",
      "episode: 430 - cum reward 9.0\n",
      "8.912527\n",
      "episode: 431 - cum reward 23.0\n",
      "episode: 432 - cum reward 19.0\n",
      "episode: 433 - cum reward 37.0\n",
      "episode: 434 - cum reward 40.0\n",
      "9.48418\n",
      "episode: 435 - cum reward 25.0\n",
      "episode: 436 - cum reward 18.0\n",
      "episode: 437 - cum reward 21.0\n",
      "episode: 438 - cum reward 53.0\n",
      "11.279745\n",
      "episode: 439 - cum reward 33.0\n",
      "episode: 440 - cum reward 63.0\n",
      "episode: 441 - cum reward 23.0\n",
      "episode: 442 - cum reward 30.0\n",
      "11.890805\n",
      "episode: 443 - cum reward 15.0\n",
      "episode: 444 - cum reward 21.0\n",
      "episode: 445 - cum reward 23.0\n",
      "episode: 446 - cum reward 31.0\n",
      "11.328765\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 447 - cum reward 16.0\n",
      "episode: 448 - cum reward 15.0\n",
      "episode: 449 - cum reward 19.0\n",
      "episode: 450 - cum reward 13.0\n",
      "11.394376\n",
      "episode: 451 - cum reward 32.0\n",
      "episode: 452 - cum reward 24.0\n",
      "episode: 453 - cum reward 20.0\n",
      "episode: 454 - cum reward 19.0\n",
      "12.406743\n",
      "episode: 455 - cum reward 36.0\n",
      "episode: 456 - cum reward 23.0\n",
      "episode: 457 - cum reward 13.0\n",
      "episode: 458 - cum reward 21.0\n",
      "12.0948305\n",
      "episode: 459 - cum reward 29.0\n",
      "episode: 460 - cum reward 14.0\n",
      "episode: 461 - cum reward 32.0\n",
      "episode: 462 - cum reward 14.0\n",
      "10.6876\n",
      "episode: 463 - cum reward 14.0\n",
      "episode: 464 - cum reward 29.0\n",
      "episode: 465 - cum reward 26.0\n",
      "episode: 466 - cum reward 12.0\n",
      "12.262808\n",
      "episode: 467 - cum reward 14.0\n",
      "episode: 468 - cum reward 28.0\n",
      "episode: 469 - cum reward 40.0\n",
      "episode: 470 - cum reward 33.0\n",
      "12.830342\n",
      "episode: 471 - cum reward 19.0\n",
      "episode: 472 - cum reward 22.0\n",
      "episode: 473 - cum reward 27.0\n",
      "episode: 474 - cum reward 15.0\n",
      "12.78282\n",
      "episode: 475 - cum reward 10.0\n",
      "episode: 476 - cum reward 22.0\n",
      "episode: 477 - cum reward 11.0\n",
      "episode: 478 - cum reward 38.0\n",
      "10.8803425\n",
      "episode: 479 - cum reward 13.0\n",
      "episode: 480 - cum reward 13.0\n",
      "episode: 481 - cum reward 24.0\n",
      "episode: 482 - cum reward 48.0\n",
      "11.362924\n",
      "episode: 483 - cum reward 14.0\n",
      "episode: 484 - cum reward 36.0\n",
      "episode: 485 - cum reward 18.0\n",
      "episode: 486 - cum reward 26.0\n",
      "10.76586\n",
      "episode: 487 - cum reward 25.0\n",
      "episode: 488 - cum reward 18.0\n",
      "episode: 489 - cum reward 13.0\n",
      "episode: 490 - cum reward 14.0\n",
      "13.148901\n",
      "episode: 491 - cum reward 35.0\n",
      "episode: 492 - cum reward 13.0\n",
      "episode: 493 - cum reward 13.0\n",
      "episode: 494 - cum reward 24.0\n",
      "13.649011\n",
      "episode: 495 - cum reward 16.0\n",
      "episode: 496 - cum reward 13.0\n",
      "episode: 497 - cum reward 64.0\n",
      "episode: 498 - cum reward 30.0\n",
      "15.865334\n",
      "episode: 499 - cum reward 19.0\n",
      "episode: 500 - cum reward 38.0\n",
      "episode: 501 - cum reward 18.0\n",
      "episode: 502 - cum reward 37.0\n",
      "13.859918\n",
      "episode: 503 - cum reward 26.0\n",
      "episode: 504 - cum reward 34.0\n",
      "episode: 505 - cum reward 29.0\n",
      "episode: 506 - cum reward 23.0\n",
      "13.546996\n",
      "episode: 507 - cum reward 18.0\n",
      "episode: 508 - cum reward 13.0\n",
      "episode: 509 - cum reward 16.0\n",
      "episode: 510 - cum reward 38.0\n",
      "11.68445\n",
      "episode: 511 - cum reward 22.0\n",
      "episode: 512 - cum reward 17.0\n",
      "episode: 513 - cum reward 13.0\n",
      "episode: 514 - cum reward 18.0\n",
      "11.096266\n",
      "episode: 515 - cum reward 28.0\n",
      "episode: 516 - cum reward 12.0\n",
      "episode: 517 - cum reward 18.0\n",
      "episode: 518 - cum reward 32.0\n",
      "14.012128\n",
      "episode: 519 - cum reward 17.0\n",
      "episode: 520 - cum reward 11.0\n",
      "episode: 521 - cum reward 21.0\n",
      "episode: 522 - cum reward 16.0\n",
      "10.720576\n",
      "episode: 523 - cum reward 38.0\n",
      "episode: 524 - cum reward 18.0\n",
      "episode: 525 - cum reward 12.0\n",
      "episode: 526 - cum reward 26.0\n",
      "12.487584\n",
      "episode: 527 - cum reward 18.0\n",
      "episode: 528 - cum reward 17.0\n",
      "episode: 529 - cum reward 20.0\n",
      "episode: 530 - cum reward 24.0\n",
      "13.31446\n",
      "episode: 531 - cum reward 26.0\n",
      "episode: 532 - cum reward 25.0\n",
      "episode: 533 - cum reward 21.0\n",
      "episode: 534 - cum reward 22.0\n",
      "14.670038\n",
      "episode: 535 - cum reward 13.0\n",
      "episode: 536 - cum reward 20.0\n",
      "episode: 537 - cum reward 14.0\n",
      "episode: 538 - cum reward 10.0\n",
      "11.181489\n",
      "episode: 539 - cum reward 17.0\n",
      "episode: 540 - cum reward 10.0\n",
      "episode: 541 - cum reward 29.0\n",
      "episode: 542 - cum reward 12.0\n",
      "13.006317\n",
      "episode: 543 - cum reward 52.0\n",
      "episode: 544 - cum reward 37.0\n",
      "episode: 545 - cum reward 56.0\n",
      "episode: 546 - cum reward 17.0\n",
      "13.296788\n",
      "episode: 547 - cum reward 9.0\n",
      "episode: 548 - cum reward 23.0\n",
      "episode: 549 - cum reward 20.0\n",
      "episode: 550 - cum reward 27.0\n",
      "13.066093\n",
      "episode: 551 - cum reward 21.0\n",
      "episode: 552 - cum reward 13.0\n",
      "episode: 553 - cum reward 14.0\n",
      "episode: 554 - cum reward 30.0\n",
      "12.255877\n",
      "episode: 555 - cum reward 20.0\n",
      "episode: 556 - cum reward 16.0\n",
      "episode: 557 - cum reward 10.0\n",
      "episode: 558 - cum reward 23.0\n",
      "15.549115\n",
      "episode: 559 - cum reward 13.0\n",
      "episode: 560 - cum reward 14.0\n",
      "episode: 561 - cum reward 17.0\n",
      "episode: 562 - cum reward 19.0\n",
      "12.374808\n",
      "episode: 563 - cum reward 45.0\n",
      "episode: 564 - cum reward 24.0\n",
      "episode: 565 - cum reward 19.0\n",
      "episode: 566 - cum reward 13.0\n",
      "12.6355715\n",
      "episode: 567 - cum reward 21.0\n",
      "episode: 568 - cum reward 70.0\n",
      "episode: 569 - cum reward 65.0\n",
      "episode: 570 - cum reward 14.0\n",
      "16.624744\n",
      "episode: 571 - cum reward 11.0\n",
      "episode: 572 - cum reward 30.0\n",
      "episode: 573 - cum reward 8.0\n",
      "episode: 574 - cum reward 22.0\n",
      "13.030472\n",
      "episode: 575 - cum reward 13.0\n",
      "episode: 576 - cum reward 23.0\n",
      "episode: 577 - cum reward 18.0\n",
      "episode: 578 - cum reward 14.0\n",
      "12.49481\n",
      "episode: 579 - cum reward 24.0\n",
      "episode: 580 - cum reward 22.0\n",
      "episode: 581 - cum reward 23.0\n",
      "episode: 582 - cum reward 16.0\n",
      "12.45067\n",
      "episode: 583 - cum reward 41.0\n",
      "episode: 584 - cum reward 39.0\n",
      "episode: 585 - cum reward 17.0\n",
      "episode: 586 - cum reward 26.0\n",
      "14.685739\n",
      "episode: 587 - cum reward 42.0\n",
      "episode: 588 - cum reward 23.0\n",
      "episode: 589 - cum reward 30.0\n",
      "episode: 590 - cum reward 10.0\n",
      "13.953611\n",
      "episode: 591 - cum reward 26.0\n",
      "episode: 592 - cum reward 22.0\n",
      "episode: 593 - cum reward 11.0\n",
      "episode: 594 - cum reward 12.0\n",
      "12.719603\n",
      "episode: 595 - cum reward 13.0\n",
      "episode: 596 - cum reward 14.0\n",
      "episode: 597 - cum reward 10.0\n",
      "episode: 598 - cum reward 20.0\n",
      "13.504869\n",
      "episode: 599 - cum reward 14.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'cumulative reward per episode - rand_agent')"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 600)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 3):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                episodes = np.asarray(self.episodes)\n",
    "                self.episodes = []\n",
    "                Xs = np.vstack(episodes[:,0])\n",
    "                Ys = np.vstack(episodes[:,1])\n",
    "                Y_values = np.vstack(episodes[:,3])\n",
    "                discounted_returns = np.hstack(episodes[:,2])\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                loss = policy_gradient_loss(baselines)\n",
    "                print(baselines.max(), baselines.min())\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.model_critic.train_on_batch(Xs, Y_values)#discounted_returns.astype(np.dtype('float32')))\n",
    "            else:\n",
    "                self.episodes.append((X,Y,discounted_return, Y_value))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_38\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_110 (Dense)            (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_111 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_112 (Dense)            (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_39\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_113 (Dense)            (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_114 (Dense)            (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_115 (Dense)            (None, 1)                 5         \n",
      "=================================================================\n",
      "Total params: 81\n",
      "Trainable params: 81\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 15.0\n",
      "episode: 1 - cum reward 16.0\n",
      "episode: 2 - cum reward 37.0\n",
      "-0.010166829 -0.50805956\n",
      "episode: 3 - cum reward 20.0\n",
      "episode: 4 - cum reward 65.0\n",
      "episode: 5 - cum reward 15.0\n",
      "episode: 6 - cum reward 22.0\n",
      "0.020962672 -0.38333604\n",
      "episode: 7 - cum reward 16.0\n",
      "episode: 8 - cum reward 30.0\n",
      "episode: 9 - cum reward 18.0\n",
      "episode: 10 - cum reward 44.0\n",
      "0.03250976 -0.18045193\n",
      "episode: 11 - cum reward 13.0\n",
      "episode: 12 - cum reward 20.0\n",
      "episode: 13 - cum reward 12.0\n",
      "episode: 14 - cum reward 13.0\n",
      "0.042586423 -0.29078078\n",
      "episode: 15 - cum reward 17.0\n",
      "episode: 16 - cum reward 12.0\n",
      "episode: 17 - cum reward 34.0\n",
      "episode: 18 - cum reward 10.0\n",
      "0.0782873 -0.20809157\n",
      "episode: 19 - cum reward 15.0\n",
      "episode: 20 - cum reward 10.0\n",
      "episode: 21 - cum reward 11.0\n",
      "episode: 22 - cum reward 15.0\n",
      "0.10993743 -0.07874347\n",
      "episode: 23 - cum reward 20.0\n",
      "episode: 24 - cum reward 10.0\n",
      "episode: 25 - cum reward 13.0\n",
      "episode: 26 - cum reward 13.0\n",
      "0.1600163 0.07061467\n",
      "episode: 27 - cum reward 17.0\n",
      "episode: 28 - cum reward 13.0\n",
      "episode: 29 - cum reward 26.0\n",
      "episode: 30 - cum reward 18.0\n",
      "0.23414177 0.08779559\n",
      "episode: 31 - cum reward 17.0\n",
      "episode: 32 - cum reward 16.0\n",
      "episode: 33 - cum reward 11.0\n",
      "episode: 34 - cum reward 24.0\n",
      "0.23346657 0.0964365\n",
      "episode: 35 - cum reward 11.0\n",
      "episode: 36 - cum reward 18.0\n",
      "episode: 37 - cum reward 12.0\n",
      "episode: 38 - cum reward 10.0\n",
      "0.30529684 0.12830287\n",
      "episode: 39 - cum reward 15.0\n",
      "episode: 40 - cum reward 10.0\n",
      "episode: 41 - cum reward 17.0\n",
      "episode: 42 - cum reward 13.0\n",
      "0.3989663 0.15854931\n",
      "episode: 43 - cum reward 13.0\n",
      "episode: 44 - cum reward 23.0\n",
      "episode: 45 - cum reward 11.0\n",
      "episode: 46 - cum reward 41.0\n",
      "0.41901496 0.17440106\n",
      "episode: 47 - cum reward 39.0\n",
      "episode: 48 - cum reward 14.0\n",
      "episode: 49 - cum reward 18.0\n",
      "episode: 50 - cum reward 12.0\n",
      "0.39361924 0.18995436\n",
      "episode: 51 - cum reward 31.0\n",
      "episode: 52 - cum reward 16.0\n",
      "episode: 53 - cum reward 21.0\n",
      "episode: 54 - cum reward 10.0\n",
      "0.6582732 0.21388793\n",
      "episode: 55 - cum reward 16.0\n",
      "episode: 56 - cum reward 27.0\n",
      "episode: 57 - cum reward 24.0\n",
      "episode: 58 - cum reward 14.0\n",
      "0.6071201 0.2164705\n",
      "episode: 59 - cum reward 18.0\n",
      "episode: 60 - cum reward 9.0\n",
      "episode: 61 - cum reward 20.0\n",
      "episode: 62 - cum reward 11.0\n",
      "0.8134749 0.25640634\n",
      "episode: 63 - cum reward 15.0\n",
      "episode: 64 - cum reward 15.0\n",
      "episode: 65 - cum reward 17.0\n",
      "episode: 66 - cum reward 17.0\n",
      "0.6079206 0.26567373\n",
      "episode: 67 - cum reward 30.0\n",
      "episode: 68 - cum reward 13.0\n",
      "episode: 69 - cum reward 27.0\n",
      "episode: 70 - cum reward 28.0\n",
      "0.689294 0.30157313\n",
      "episode: 71 - cum reward 13.0\n",
      "episode: 72 - cum reward 13.0\n",
      "episode: 73 - cum reward 14.0\n",
      "episode: 74 - cum reward 11.0\n",
      "0.9273273 0.338234\n",
      "episode: 75 - cum reward 16.0\n",
      "episode: 76 - cum reward 18.0\n",
      "episode: 77 - cum reward 15.0\n",
      "episode: 78 - cum reward 9.0\n",
      "0.8765893 0.3533377\n",
      "episode: 79 - cum reward 16.0\n",
      "episode: 80 - cum reward 20.0\n",
      "episode: 81 - cum reward 27.0\n",
      "episode: 82 - cum reward 12.0\n",
      "0.99153554 0.3427908\n",
      "episode: 83 - cum reward 10.0\n",
      "episode: 84 - cum reward 12.0\n",
      "episode: 85 - cum reward 13.0\n",
      "episode: 86 - cum reward 14.0\n",
      "1.1503803 0.42143825\n",
      "episode: 87 - cum reward 11.0\n",
      "episode: 88 - cum reward 10.0\n",
      "episode: 89 - cum reward 18.0\n",
      "episode: 90 - cum reward 10.0\n",
      "1.1264066 0.43715706\n",
      "episode: 91 - cum reward 11.0\n",
      "episode: 92 - cum reward 14.0\n",
      "episode: 93 - cum reward 17.0\n",
      "episode: 94 - cum reward 22.0\n",
      "1.267247 0.48161262\n",
      "episode: 95 - cum reward 30.0\n",
      "episode: 96 - cum reward 15.0\n",
      "episode: 97 - cum reward 29.0\n",
      "episode: 98 - cum reward 11.0\n",
      "1.4100187 0.49811718\n",
      "episode: 99 - cum reward 20.0\n",
      "episode: 100 - cum reward 16.0\n",
      "episode: 101 - cum reward 32.0\n",
      "episode: 102 - cum reward 12.0\n",
      "1.8036747 0.5028118\n",
      "episode: 103 - cum reward 14.0\n",
      "episode: 104 - cum reward 25.0\n",
      "episode: 105 - cum reward 22.0\n",
      "episode: 106 - cum reward 14.0\n",
      "1.7708675 0.55958605\n",
      "episode: 107 - cum reward 9.0\n",
      "episode: 108 - cum reward 10.0\n",
      "episode: 109 - cum reward 15.0\n",
      "episode: 110 - cum reward 23.0\n",
      "1.9044374 0.5978832\n",
      "episode: 111 - cum reward 16.0\n",
      "episode: 112 - cum reward 13.0\n",
      "episode: 113 - cum reward 9.0\n",
      "episode: 114 - cum reward 17.0\n",
      "1.6086946 0.64702535\n",
      "episode: 115 - cum reward 10.0\n",
      "episode: 116 - cum reward 15.0\n",
      "episode: 117 - cum reward 12.0\n",
      "episode: 118 - cum reward 19.0\n",
      "1.4653478 0.6342468\n",
      "episode: 119 - cum reward 24.0\n",
      "episode: 120 - cum reward 11.0\n",
      "episode: 121 - cum reward 11.0\n",
      "episode: 122 - cum reward 20.0\n",
      "2.3587227 0.69975317\n",
      "episode: 123 - cum reward 22.0\n",
      "episode: 124 - cum reward 9.0\n",
      "episode: 125 - cum reward 13.0\n",
      "episode: 126 - cum reward 14.0\n",
      "2.3427687 0.76058084\n",
      "episode: 127 - cum reward 23.0\n",
      "episode: 128 - cum reward 9.0\n",
      "episode: 129 - cum reward 17.0\n",
      "episode: 130 - cum reward 15.0\n",
      "2.555629 0.78093225\n",
      "episode: 131 - cum reward 22.0\n",
      "episode: 132 - cum reward 16.0\n",
      "episode: 133 - cum reward 29.0\n",
      "episode: 134 - cum reward 18.0\n",
      "2.2246268 0.76862824\n",
      "episode: 135 - cum reward 15.0\n",
      "episode: 136 - cum reward 27.0\n",
      "episode: 137 - cum reward 15.0\n",
      "episode: 138 - cum reward 13.0\n",
      "2.8896346 0.86285144\n",
      "episode: 139 - cum reward 21.0\n",
      "episode: 140 - cum reward 18.0\n",
      "episode: 141 - cum reward 10.0\n",
      "episode: 142 - cum reward 17.0\n",
      "2.9797819 0.92956674\n",
      "episode: 143 - cum reward 13.0\n",
      "episode: 144 - cum reward 24.0\n",
      "episode: 145 - cum reward 18.0\n",
      "episode: 146 - cum reward 12.0\n",
      "3.0631883 0.97419703\n",
      "episode: 147 - cum reward 24.0\n",
      "episode: 148 - cum reward 28.0\n",
      "episode: 149 - cum reward 10.0\n",
      "episode: 150 - cum reward 20.0\n",
      "3.8147216 1.0215135\n",
      "episode: 151 - cum reward 32.0\n",
      "episode: 152 - cum reward 12.0\n",
      "episode: 153 - cum reward 11.0\n",
      "episode: 154 - cum reward 20.0\n",
      "4.246315 1.0809398\n",
      "episode: 155 - cum reward 28.0\n",
      "episode: 156 - cum reward 18.0\n",
      "episode: 157 - cum reward 30.0\n",
      "episode: 158 - cum reward 16.0\n",
      "3.6542459 1.1028416\n",
      "episode: 159 - cum reward 12.0\n",
      "episode: 160 - cum reward 13.0\n",
      "episode: 161 - cum reward 10.0\n",
      "episode: 162 - cum reward 14.0\n",
      "4.2143264 1.1945106\n",
      "episode: 163 - cum reward 25.0\n",
      "episode: 164 - cum reward 9.0\n",
      "episode: 165 - cum reward 15.0\n",
      "episode: 166 - cum reward 10.0\n",
      "4.456773 1.2204752\n",
      "episode: 167 - cum reward 14.0\n",
      "episode: 168 - cum reward 21.0\n",
      "episode: 169 - cum reward 21.0\n",
      "episode: 170 - cum reward 14.0\n",
      "3.881506 1.2180471\n",
      "episode: 171 - cum reward 16.0\n",
      "episode: 172 - cum reward 18.0\n",
      "episode: 173 - cum reward 32.0\n",
      "episode: 174 - cum reward 15.0\n",
      "5.6935153 1.298167\n",
      "episode: 175 - cum reward 17.0\n",
      "episode: 176 - cum reward 17.0\n",
      "episode: 177 - cum reward 10.0\n",
      "episode: 178 - cum reward 17.0\n",
      "5.864293 1.3719121\n",
      "episode: 179 - cum reward 15.0\n",
      "episode: 180 - cum reward 20.0\n",
      "episode: 181 - cum reward 26.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 182 - cum reward 18.0\n",
      "4.300969 1.4642961\n",
      "episode: 183 - cum reward 23.0\n",
      "episode: 184 - cum reward 19.0\n",
      "episode: 185 - cum reward 39.0\n",
      "episode: 186 - cum reward 26.0\n",
      "6.9247036 1.4608411\n",
      "episode: 187 - cum reward 50.0\n",
      "episode: 188 - cum reward 16.0\n",
      "episode: 189 - cum reward 21.0\n",
      "episode: 190 - cum reward 17.0\n",
      "6.230141 1.6099274\n",
      "episode: 191 - cum reward 14.0\n",
      "episode: 192 - cum reward 12.0\n",
      "episode: 193 - cum reward 13.0\n",
      "episode: 194 - cum reward 10.0\n",
      "5.902809 1.6815988\n",
      "episode: 195 - cum reward 21.0\n",
      "episode: 196 - cum reward 8.0\n",
      "episode: 197 - cum reward 20.0\n",
      "episode: 198 - cum reward 22.0\n",
      "7.2704525 1.7319174\n",
      "episode: 199 - cum reward 26.0\n",
      "episode: 200 - cum reward 31.0\n",
      "episode: 201 - cum reward 18.0\n",
      "episode: 202 - cum reward 14.0\n",
      "5.381885 1.7804586\n",
      "episode: 203 - cum reward 14.0\n",
      "episode: 204 - cum reward 13.0\n",
      "episode: 205 - cum reward 12.0\n",
      "episode: 206 - cum reward 31.0\n",
      "8.533211 1.8830397\n",
      "episode: 207 - cum reward 15.0\n",
      "episode: 208 - cum reward 28.0\n",
      "episode: 209 - cum reward 10.0\n",
      "episode: 210 - cum reward 15.0\n",
      "8.735411 1.9444172\n",
      "episode: 211 - cum reward 13.0\n",
      "episode: 212 - cum reward 10.0\n",
      "episode: 213 - cum reward 17.0\n",
      "episode: 214 - cum reward 18.0\n",
      "8.979381 2.0853724\n",
      "episode: 215 - cum reward 14.0\n",
      "episode: 216 - cum reward 11.0\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-155a006b3ae5>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'CartPole-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m400\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-56-e16deee0f198>\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-62-d26c4b975409>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     49\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     50\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 51\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     52\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    424\u001b[0m           \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    425\u001b[0m           \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 426\u001b[0;31m           use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    427\u001b[0m       \u001b[0mtotal_samples\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_get_total_number_of_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0madapter\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    428\u001b[0m       \u001b[0muse_sample\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtotal_samples\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_process_inputs\u001b[0;34m(model, mode, x, y, batch_size, epochs, sample_weights, class_weights, shuffle, steps, distribution_strategy, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m    704\u001b[0m       \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    705\u001b[0m       \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 706\u001b[0;31m       use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m    707\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    708\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0madapter\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, x, y, sample_weights, sample_weight_modes, batch_size, epochs, steps, shuffle, **kwargs)\u001b[0m\n\u001b[1;32m    355\u001b[0m     \u001b[0mindices_dataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindices_dataset\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mslice_batch_indices\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    356\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 357\u001b[0;31m     \u001b[0mdataset\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mslice_inputs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindices_dataset\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    358\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    359\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mshuffle\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m\"batch\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/data_adapter.py\u001b[0m in \u001b[0;36mslice_inputs\u001b[0;34m(self, indices_dataset, inputs)\u001b[0m\n\u001b[1;32m    388\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    389\u001b[0m     dataset = dataset.map(\n\u001b[0;32m--> 390\u001b[0;31m         grab_batch, num_parallel_calls=dataset_ops.AUTOTUNE)\n\u001b[0m\u001b[1;32m    391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    392\u001b[0m     \u001b[0;31m# Default optimizations are disabled to avoid the overhead of (unnecessary)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36mmap\u001b[0;34m(self, map_func, num_parallel_calls)\u001b[0m\n\u001b[1;32m   1589\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1590\u001b[0m       return ParallelMapDataset(\n\u001b[0;32m-> 1591\u001b[0;31m           self, map_func, num_parallel_calls, preserve_cardinality=True)\n\u001b[0m\u001b[1;32m   1592\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1593\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mflat_map\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmap_func\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, input_dataset, map_func, num_parallel_calls, use_inter_op_parallelism, preserve_cardinality, use_legacy_function)\u001b[0m\n\u001b[1;32m   3924\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_transformation_name\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3925\u001b[0m         \u001b[0mdataset\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0minput_dataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3926\u001b[0;31m         use_legacy_function=use_legacy_function)\n\u001b[0m\u001b[1;32m   3927\u001b[0m     self._num_parallel_calls = ops.convert_to_tensor(\n\u001b[1;32m   3928\u001b[0m         num_parallel_calls, dtype=dtypes.int32, name=\"num_parallel_calls\")\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/data/ops/dataset_ops.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, func, transformation_name, dataset, input_classes, input_shapes, input_types, input_structure, add_to_graph, use_legacy_function, defun_kwargs)\u001b[0m\n\u001b[1;32m   3145\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0mtracking\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource_tracker_scope\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mresource_tracker\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3146\u001b[0m         \u001b[0;31m# TODO(b/141462134): Switch to using garbage collection.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3147\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mwrapper_fn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_concrete_function_internal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3148\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3149\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0madd_to_graph\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2393\u001b[0m     \u001b[0;34m\"\"\"Bypasses error checking when getting a graph function.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2394\u001b[0m     graph_function = self._get_concrete_function_internal_garbage_collected(\n\u001b[0;32m-> 2395\u001b[0;31m         *args, **kwargs)\n\u001b[0m\u001b[1;32m   2396\u001b[0m     \u001b[0;31m# We're returning this concrete function to someone, and they may keep a\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2397\u001b[0m     \u001b[0;31m# reference to the FuncGraph without keeping a reference to the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_get_concrete_function_internal_garbage_collected\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2387\u001b[0m       \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2388\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2389\u001b[0;31m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2390\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2391\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_maybe_define_function\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   2701\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2702\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmissed\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcall_context_key\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2703\u001b[0;31m       \u001b[0mgraph_function\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_create_graph_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2704\u001b[0m       \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_cache\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mprimary\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcache_key\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2705\u001b[0m       \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_create_graph_function\u001b[0;34m(self, args, kwargs, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   2591\u001b[0m             \u001b[0marg_names\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0marg_names\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2592\u001b[0m             \u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moverride_flat_arg_shapes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2593\u001b[0;31m             capture_by_value=self._capture_by_value),\n\u001b[0m\u001b[1;32m   2594\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_function_attributes\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2595\u001b[0m         \u001b[0;31m# Tell the ConcreteFunction to clean up its graph once it goes out of\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/func_graph.py\u001b[0m in \u001b[0;36mfunc_graph_from_py_func\u001b[0;34m(name, python_func, args, kwargs, signature, func_graph, autograph, autograph_options, add_control_dependencies, arg_names, op_return_value, collections, capture_by_value, override_flat_arg_shapes)\u001b[0m\n\u001b[1;32m   1017\u001b[0m         if x is not None)\n\u001b[1;32m   1018\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1019\u001b[0;31m     \u001b[0mfunc_graph\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvariables\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvariables\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1020\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m   \u001b[0;32mif\u001b[0m \u001b[0madd_control_dependencies\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/auto_control_deps.py\u001b[0m in \u001b[0;36m__exit__\u001b[0;34m(self, unused_type, unused_value, unused_traceback)\u001b[0m\n\u001b[1;32m    322\u001b[0m       \u001b[0;31m# Check for any resource inputs. If we find any, we update control_inputs\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    323\u001b[0m       \u001b[0;31m# and last_op_using_resource_tensor.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 324\u001b[0;31m       \u001b[0;32mfor\u001b[0m \u001b[0minp\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    325\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0minp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdtype\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0mdtypes_module\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mresource\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    326\u001b[0m           \u001b[0;32mcontinue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36minputs\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m   2161\u001b[0m       \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2162\u001b[0m       self._inputs_val = tuple(map(self.graph._get_tensor_by_tf_output,\n\u001b[0;32m-> 2163\u001b[0;31m                                    c_api.GetOperationInputs(self._c_op)))\n\u001b[0m\u001b[1;32m   2164\u001b[0m       \u001b[0;31m# pylint: enable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2165\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_inputs_val\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/framework/ops.py\u001b[0m in \u001b[0;36m_get_tensor_by_tf_output\u001b[0;34m(self, tf_output)\u001b[0m\n\u001b[1;32m   3695\u001b[0m     \"\"\"\n\u001b[1;32m   3696\u001b[0m     \u001b[0mop\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_operation_by_tf_operation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moper\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3697\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mop\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mtf_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3698\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3699\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 400)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "import time\n",
    "import spinup.algos.vpg.core as core\n",
    "from spinup.utils.logx import EpochLogger\n",
    "from spinup.utils.mpi_tf import MpiAdamOptimizer, sync_all_params\n",
    "from spinup.utils.mpi_tools import mpi_fork, mpi_avg, proc_id, mpi_statistics_scalar, num_procs\n",
    "\n",
    "\n",
    "class VPGBuffer:\n",
    "    \"\"\"\n",
    "    A buffer for storing trajectories experienced by a VPG agent interacting\n",
    "    with the environment, and using Generalized Advantage Estimation (GAE-Lambda)\n",
    "    for calculating the advantages of state-action pairs.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, obs_dim, act_dim, size, gamma=0.99, lam=0.95):\n",
    "        self.obs_buf = np.zeros(core.combined_shape(size, obs_dim), dtype=np.float32)\n",
    "        self.act_buf = np.zeros(core.combined_shape(size, act_dim), dtype=np.float32)\n",
    "        self.adv_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.rew_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.ret_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.val_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.logp_buf = np.zeros(size, dtype=np.float32)\n",
    "        self.gamma, self.lam = gamma, lam\n",
    "        self.ptr, self.path_start_idx, self.max_size = 0, 0, size\n",
    "\n",
    "    def store(self, obs, act, rew, val, logp):\n",
    "        \"\"\"\n",
    "        Append one timestep of agent-environment interaction to the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr < self.max_size     # buffer has to have room so you can store\n",
    "        self.obs_buf[self.ptr] = obs\n",
    "        self.act_buf[self.ptr] = act\n",
    "        self.rew_buf[self.ptr] = rew\n",
    "        self.val_buf[self.ptr] = val\n",
    "        self.logp_buf[self.ptr] = logp\n",
    "        self.ptr += 1\n",
    "\n",
    "    def finish_path(self, last_val=0):\n",
    "        \"\"\"\n",
    "        Call this at the end of a trajectory, or when one gets cut off\n",
    "        by an epoch ending. This looks back in the buffer to where the\n",
    "        trajectory started, and uses rewards and value estimates from\n",
    "        the whole trajectory to compute advantage estimates with GAE-Lambda,\n",
    "        as well as compute the rewards-to-go for each state, to use as\n",
    "        the targets for the value function.\n",
    "        The \"last_val\" argument should be 0 if the trajectory ended\n",
    "        because the agent reached a terminal state (died), and otherwise\n",
    "        should be V(s_T), the value function estimated for the last state.\n",
    "        This allows us to bootstrap the reward-to-go calculation to account\n",
    "        for timesteps beyond the arbitrary episode horizon (or epoch cutoff).\n",
    "        \"\"\"\n",
    "\n",
    "        path_slice = slice(self.path_start_idx, self.ptr)\n",
    "        rews = np.append(self.rew_buf[path_slice], last_val)\n",
    "        vals = np.append(self.val_buf[path_slice], last_val)\n",
    "        \n",
    "        # the next two lines implement GAE-Lambda advantage calculation\n",
    "        deltas = rews[:-1] + self.gamma * vals[1:] - vals[:-1]\n",
    "        self.adv_buf[path_slice] = core.discount_cumsum(deltas, self.gamma * self.lam)\n",
    "        \n",
    "        # the next line computes rewards-to-go, to be targets for the value function\n",
    "        self.ret_buf[path_slice] = core.discount_cumsum(rews, self.gamma)[:-1]\n",
    "        \n",
    "        self.path_start_idx = self.ptr\n",
    "\n",
    "    def get(self):\n",
    "        \"\"\"\n",
    "        Call this at the end of an epoch to get all of the data from\n",
    "        the buffer, with advantages appropriately normalized (shifted to have\n",
    "        mean zero and std one). Also, resets some pointers in the buffer.\n",
    "        \"\"\"\n",
    "        assert self.ptr == self.max_size    # buffer has to be full before you can get\n",
    "        self.ptr, self.path_start_idx = 0, 0\n",
    "        # the next two lines implement the advantage normalization trick\n",
    "        adv_mean, adv_std = mpi_statistics_scalar(self.adv_buf)\n",
    "        self.adv_buf = (self.adv_buf - adv_mean) / adv_std\n",
    "        return [self.obs_buf, self.act_buf, self.adv_buf, \n",
    "                self.ret_buf, self.logp_buf]\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "Vanilla Policy Gradient\n",
    "(with GAE-Lambda for advantage estimation)\n",
    "\"\"\"\n",
    "def vpg(env_fn, actor_critic=core.mlp_actor_critic, ac_kwargs=dict(), seed=0, \n",
    "        steps_per_epoch=4000, epochs=50, gamma=0.99, pi_lr=3e-4,\n",
    "        vf_lr=1e-3, train_v_iters=80, lam=0.97, max_ep_len=1000,\n",
    "        logger_kwargs=dict(), save_freq=10):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        env_fn : A function which creates a copy of the environment.\n",
    "            The environment must satisfy the OpenAI Gym API.\n",
    "        actor_critic: A function which takes in placeholder symbols \n",
    "            for state, ``x_ph``, and action, ``a_ph``, and returns the main \n",
    "            outputs from the agent's Tensorflow computation graph:\n",
    "            ===========  ================  ======================================\n",
    "            Symbol       Shape             Description\n",
    "            ===========  ================  ======================================\n",
    "            ``pi``       (batch, act_dim)  | Samples actions from policy given \n",
    "                                           | states.\n",
    "            ``logp``     (batch,)          | Gives log probability, according to\n",
    "                                           | the policy, of taking actions ``a_ph``\n",
    "                                           | in states ``x_ph``.\n",
    "            ``logp_pi``  (batch,)          | Gives log probability, according to\n",
    "                                           | the policy, of the action sampled by\n",
    "                                           | ``pi``.\n",
    "            ``v``        (batch,)          | Gives the value estimate for states\n",
    "                                           | in ``x_ph``. (Critical: make sure \n",
    "                                           | to flatten this!)\n",
    "            ===========  ================  ======================================\n",
    "        ac_kwargs (dict): Any kwargs appropriate for the actor_critic \n",
    "            function you provided to VPG.\n",
    "        seed (int): Seed for random number generators.\n",
    "        steps_per_epoch (int): Number of steps of interaction (state-action pairs) \n",
    "            for the agent and the environment in each epoch.\n",
    "        epochs (int): Number of epochs of interaction (equivalent to\n",
    "            number of policy updates) to perform.\n",
    "        gamma (float): Discount factor. (Always between 0 and 1.)\n",
    "        pi_lr (float): Learning rate for policy optimizer.\n",
    "        vf_lr (float): Learning rate for value function optimizer.\n",
    "        train_v_iters (int): Number of gradient descent steps to take on \n",
    "            value function per epoch.\n",
    "        lam (float): Lambda for GAE-Lambda. (Always between 0 and 1,\n",
    "            close to 1.)\n",
    "        max_ep_len (int): Maximum length of trajectory / episode / rollout.\n",
    "        logger_kwargs (dict): Keyword args for EpochLogger.\n",
    "        save_freq (int): How often (in terms of gap between epochs) to save\n",
    "            the current policy and value function.\n",
    "    \"\"\"\n",
    "\n",
    "    logger = EpochLogger(**logger_kwargs)\n",
    "    logger.save_config(locals())\n",
    "\n",
    "    seed += 10000 * proc_id()\n",
    "    tf.set_random_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "    env = env_fn()\n",
    "    obs_dim = env.observation_space.shape\n",
    "    act_dim = env.action_space.shape\n",
    "    \n",
    "    # Share information about action space with policy architecture\n",
    "    ac_kwargs['action_space'] = env.action_space\n",
    "\n",
    "    # Inputs to computation graph\n",
    "    x_ph, a_ph = core.placeholders_from_spaces(env.observation_space, env.action_space)\n",
    "    adv_ph, ret_ph, logp_old_ph = core.placeholders(None, None, None)\n",
    "\n",
    "    # Main outputs from computation graph\n",
    "    pi, logp, logp_pi, v = actor_critic(x_ph, a_ph, **ac_kwargs)\n",
    "\n",
    "    # Need all placeholders in *this* order later (to zip with data from buffer)\n",
    "    all_phs = [x_ph, a_ph, adv_ph, ret_ph, logp_old_ph]\n",
    "\n",
    "    # Every step, get: action, value, and logprob\n",
    "    get_action_ops = [pi, v, logp_pi]\n",
    "\n",
    "    # Experience buffer\n",
    "    local_steps_per_epoch = int(steps_per_epoch / num_procs())\n",
    "    buf = VPGBuffer(obs_dim, act_dim, local_steps_per_epoch, gamma, lam)\n",
    "\n",
    "    # Count variables\n",
    "    var_counts = tuple(core.count_vars(scope) for scope in ['pi', 'v'])\n",
    "    logger.log('\\nNumber of parameters: \\t pi: %d, \\t v: %d\\n'%var_counts)\n",
    "\n",
    "    # VPG objectives\n",
    "    pi_loss = -tf.reduce_mean(logp * adv_ph)\n",
    "    v_loss = tf.reduce_mean((ret_ph - v)**2)\n",
    "\n",
    "    # Info (useful to watch during learning)\n",
    "    approx_kl = tf.reduce_mean(logp_old_ph - logp)      # a sample estimate for KL-divergence, easy to compute\n",
    "    approx_ent = tf.reduce_mean(-logp)                  # a sample estimate for entropy, also easy to compute\n",
    "\n",
    "    # Optimizers\n",
    "    train_pi = MpiAdamOptimizer(learning_rate=pi_lr).minimize(pi_loss)\n",
    "    train_v = MpiAdamOptimizer(learning_rate=vf_lr).minimize(v_loss)\n",
    "\n",
    "    sess = tf.Session()\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "\n",
    "    # Sync params across processes\n",
    "    sess.run(sync_all_params())\n",
    "\n",
    "    # Setup model saving\n",
    "    logger.setup_tf_saver(sess, inputs={'x': x_ph}, outputs={'pi': pi, 'v': v})\n",
    "\n",
    "    def update():\n",
    "        inputs = {k:v for k,v in zip(all_phs, buf.get())}\n",
    "        pi_l_old, v_l_old, ent = sess.run([pi_loss, v_loss, approx_ent], feed_dict=inputs)\n",
    "\n",
    "        # Policy gradient step\n",
    "        sess.run(train_pi, feed_dict=inputs)\n",
    "\n",
    "        # Value function learning\n",
    "        for _ in range(train_v_iters):\n",
    "            sess.run(train_v, feed_dict=inputs)\n",
    "\n",
    "        # Log changes from update\n",
    "        pi_l_new, v_l_new, kl = sess.run([pi_loss, v_loss, approx_kl], feed_dict=inputs)\n",
    "        logger.store(LossPi=pi_l_old, LossV=v_l_old, \n",
    "                     KL=kl, Entropy=ent, \n",
    "                     DeltaLossPi=(pi_l_new - pi_l_old),\n",
    "                     DeltaLossV=(v_l_new - v_l_old))\n",
    "\n",
    "    start_time = time.time()\n",
    "    o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "    # Main loop: collect experience in env and update/log each epoch\n",
    "    for epoch in range(epochs):\n",
    "        for t in range(local_steps_per_epoch):\n",
    "            a, v_t, logp_t = sess.run(get_action_ops, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "\n",
    "            # save and log\n",
    "            buf.store(o, a, r, v_t, logp_t)\n",
    "            logger.store(VVals=v_t)\n",
    "\n",
    "            o, r, d, _ = env.step(a[0])\n",
    "            ep_ret += r\n",
    "            ep_len += 1\n",
    "\n",
    "            terminal = d or (ep_len == max_ep_len)\n",
    "            if terminal or (t==local_steps_per_epoch-1):\n",
    "                if not(terminal):\n",
    "                    print('Warning: trajectory cut off by epoch at %d steps.'%ep_len)\n",
    "                # if trajectory didn't reach terminal state, bootstrap value target\n",
    "                last_val = r if d else sess.run(v, feed_dict={x_ph: o.reshape(1,-1)})\n",
    "                buf.finish_path(last_val)\n",
    "                if terminal:\n",
    "                    # only save EpRet / EpLen if trajectory finished\n",
    "                    logger.store(EpRet=ep_ret, EpLen=ep_len)\n",
    "                o, r, d, ep_ret, ep_len = env.reset(), 0, False, 0, 0\n",
    "\n",
    "        # Save model\n",
    "        if (epoch % save_freq == 0) or (epoch == epochs-1):\n",
    "            logger.save_state({'env': env}, None)\n",
    "\n",
    "        # Perform VPG update!\n",
    "        update()\n",
    "\n",
    "        # Log info about epoch\n",
    "        logger.log_tabular('Epoch', epoch)\n",
    "        logger.log_tabular('EpRet', with_min_and_max=True)\n",
    "        logger.log_tabular('EpLen', average_only=True)\n",
    "        logger.log_tabular('VVals', with_min_and_max=True)\n",
    "        logger.log_tabular('TotalEnvInteracts', (epoch+1)*steps_per_epoch)\n",
    "        logger.log_tabular('LossPi', average_only=True)\n",
    "        logger.log_tabular('LossV', average_only=True)\n",
    "        logger.log_tabular('DeltaLossPi', average_only=True)\n",
    "        logger.log_tabular('DeltaLossV', average_only=True)\n",
    "        logger.log_tabular('Entropy', average_only=True)\n",
    "        logger.log_tabular('KL', average_only=True)\n",
    "        logger.log_tabular('Time', time.time()-start_time)\n",
    "        logger.dump_tabular()\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    import argparse\n",
    "    parser = argparse.ArgumentParser()\n",
    "    parser.add_argument('--env', type=str, default='HalfCheetah-v2')\n",
    "    parser.add_argument('--hid', type=int, default=64)\n",
    "    parser.add_argument('--l', type=int, default=2)\n",
    "    parser.add_argument('--gamma', type=float, default=0.99)\n",
    "    parser.add_argument('--seed', '-s', type=int, default=0)\n",
    "    parser.add_argument('--cpu', type=int, default=4)\n",
    "    parser.add_argument('--steps', type=int, default=4000)\n",
    "    parser.add_argument('--epochs', type=int, default=50)\n",
    "    parser.add_argument('--exp_name', type=str, default='vpg')\n",
    "    args = parser.parse_args()\n",
    "\n",
    "    mpi_fork(args.cpu)  # run parallel code with mpi\n",
    "\n",
    "    from spinup.utils.run_utils import setup_logger_kwargs\n",
    "    logger_kwargs = setup_logger_kwargs(args.exp_name, args.seed)\n",
    "\n",
    "    vpg(lambda : gym.make(args.env), actor_critic=core.mlp_actor_critic,\n",
    "        ac_kwargs=dict(hidden_sizes=[args.hid]*args.l), gamma=args.gamma, \n",
    "        seed=args.seed, steps_per_epoch=args.steps, epochs=args.epochs,\n",
    "        logger_kwargs=logger_kwargs)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
