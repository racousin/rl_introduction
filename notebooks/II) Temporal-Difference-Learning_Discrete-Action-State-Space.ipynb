{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "env = gym.make('NChain-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference-Learning-Action-State-Space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NChain- environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NChain-illustration.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dynamic Programming"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Temporal-Difference-Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SARSA"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "“SARSA” refers to the procedure of updaing Q-value by following a sequence of $…,S_t,A_t,R_{t+1},S_{t+1},A_{t+1},…$. The idea follows the same route of GPI:\n",
    "\n",
    "1. At time step $t$, we start from state $S_t$ and pick action according to $Q$ values, $A_t=\\arg\\max_{a\\in A}Q(S_t,a)$; $\\epsilon$-greedy is commonly applied.\n",
    "2. With action $A_t$, we observe reward $R_{t+1}$ and get into the next state $S_{t+1}$.\n",
    "3. Then pick the next action in the same way as in step 1.: $A_{t+1}=\\arg\\max_{a\\in A}Q(S_{t+1},a)$.\n",
    "4. Update the action-value function: $Q(S_t,A_t)=Q(S_t,A_t)+ \\alpha(R_{t+1}+\\gamma Q(S_{t+1},A_{t+1})−Q(S_t,A_t))$.   $t = t+1$ and repeat from step 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(S_t, A_t) = Q(S_t, A_t) + \\alpha (R_{t+1} + \\gamma Q(S_{t+1}, A_{t+1}) - Q(S_t, A_t))$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q-Learning technique is an Off Policy technique and uses the greedy approach to learn the Q-value. SARSA technique, on the other hand, is an On Policy and uses the action performed by the current policy to learn the Q-value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Defining the different parameters \n",
    "epsilon = 0.9\n",
    "total_episodes = 10000\n",
    "max_steps = 100\n",
    "alpha = 0.85\n",
    "gamma = 0.95\n",
    "  \n",
    "#Initializing the Q-matrix \n",
    "Q = np.zeros((env.observation_space.n, env.action_space.n)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Function to choose the next action \n",
    "def choose_action(state): \n",
    "    action=0\n",
    "    if np.random.uniform(0, 1) < epsilon: \n",
    "        action = env.action_space.sample() \n",
    "    else: \n",
    "        action = np.argmax(Q[state, :]) \n",
    "    return action \n",
    "  \n",
    "#Function to learn the Q-value \n",
    "def update(state, state2, reward, action, action2): \n",
    "    predict = Q[state, action] \n",
    "    target = reward + gamma * Q[state2, action2] \n",
    "    Q[state, action] = Q[state, action] + alpha * (target - predict) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Initializing the reward \n",
    "reward=0\n",
    "  \n",
    "# Starting the SARSA learning \n",
    "for episode in range(total_episodes): \n",
    "    t = 0\n",
    "    state1 = env.reset() \n",
    "    action1 = choose_action(state1) \n",
    "  \n",
    "    while t < max_steps: \n",
    "        #Getting the next state \n",
    "        state2, reward, done, info = env.step(action1) \n",
    "  \n",
    "        #Choosing the next action \n",
    "        action2 = choose_action(state2) \n",
    "          \n",
    "        #Learning the Q-value \n",
    "        update(state1, state2, reward, action1, action2) \n",
    "  \n",
    "        state1 = state2 \n",
    "        action1 = action2 \n",
    "          \n",
    "        #Updating the respective vaLues \n",
    "        t += 1\n",
    "        reward += 1\n",
    "          \n",
    "        #If at the end of learning process \n",
    "        if done: \n",
    "            break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performace :  0.0001\n",
      "[[19.45661715 16.9136222 ]\n",
      " [20.19172285 19.27388429]\n",
      " [32.76469682 21.216865  ]\n",
      " [18.48082876 21.25478373]\n",
      " [32.68564449 19.34836536]]\n"
     ]
    }
   ],
   "source": [
    "#Evaluating the performance \n",
    "print (\"Performace : \", reward/total_episodes) \n",
    "  \n",
    "#Visualizing the Q-matrix \n",
    "print(Q) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q function update"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$Q(s, a) = Q(s, a) + \\alpha (r + \\gamma \\max\\limits_{a’} Q(s’, a’) – Q(s, a))$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    lr = 0.8\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        done = False\n",
    "        while not done:\n",
    "            if np.sum(q_table[s,:]) == 0:\n",
    "                # make a random selection of actions\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                # select the action with largest q value in state s\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr*(y*np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def eps_greedy_q_learning_with_table(env, num_episodes=500):\n",
    "    q_table = np.zeros((5, 2))\n",
    "    y = 0.95\n",
    "    eps = 0.5\n",
    "    lr = 0.8\n",
    "    decay_factor = 0.999\n",
    "    for i in range(num_episodes):\n",
    "        s = env.reset()\n",
    "        eps *= decay_factor\n",
    "        done = False\n",
    "        while not done:\n",
    "            # select the action with highest cummulative reward\n",
    "            if np.random.random() < eps or np.sum(q_table[s, :]) == 0:\n",
    "                a = np.random.randint(0, 2)\n",
    "            else:\n",
    "                a = np.argmax(q_table[s, :])\n",
    "            # pdb.set_trace()\n",
    "            new_s, r, done, _ = env.step(a)\n",
    "            q_table[s, a] += r + lr * (y * np.max(q_table[new_s, :]) - q_table[s, a])\n",
    "            s = new_s\n",
    "    return q_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_game(table, env):\n",
    "    s = env.reset()\n",
    "    tot_reward = 0\n",
    "    done = False\n",
    "    while not done:\n",
    "        a = np.argmax(table[s, :])\n",
    "        s, r, done, _ = env.step(a)\n",
    "        tot_reward += r\n",
    "    return tot_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_methods(env, num_iterations=100):\n",
    "    winner = np.zeros((2,))\n",
    "    for g in range(num_iterations):\n",
    "        m1_table = q_learning_with_table(env, 500)\n",
    "        m2_table = eps_greedy_q_learning_with_table(env, 500)\n",
    "        m1 = run_game(m1_table, env)\n",
    "        m2 = run_game(m2_table, env)\n",
    "        w = np.argmax(np.array([m1, m2]))\n",
    "        winner[w] += 1\n",
    "        print(\"Game {} of {}\".format(g + 1, num_iterations))\n",
    "    return winner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game 1 of 100\n",
      "Game 2 of 100\n",
      "Game 3 of 100\n",
      "Game 4 of 100\n",
      "Game 5 of 100\n",
      "Game 6 of 100\n",
      "Game 7 of 100\n",
      "Game 8 of 100\n",
      "Game 9 of 100\n",
      "Game 10 of 100\n",
      "Game 11 of 100\n",
      "Game 12 of 100\n",
      "Game 13 of 100\n",
      "Game 14 of 100\n",
      "Game 15 of 100\n",
      "Game 16 of 100\n",
      "Game 17 of 100\n",
      "Game 18 of 100\n",
      "Game 19 of 100\n",
      "Game 20 of 100\n",
      "Game 21 of 100\n",
      "Game 22 of 100\n",
      "Game 23 of 100\n",
      "Game 24 of 100\n",
      "Game 25 of 100\n",
      "Game 26 of 100\n",
      "Game 27 of 100\n",
      "Game 28 of 100\n",
      "Game 29 of 100\n",
      "Game 30 of 100\n",
      "Game 31 of 100\n",
      "Game 32 of 100\n",
      "Game 33 of 100\n",
      "Game 34 of 100\n",
      "Game 35 of 100\n",
      "Game 36 of 100\n",
      "Game 37 of 100\n",
      "Game 38 of 100\n",
      "Game 39 of 100\n",
      "Game 40 of 100\n",
      "Game 41 of 100\n",
      "Game 42 of 100\n",
      "Game 43 of 100\n",
      "Game 44 of 100\n",
      "Game 45 of 100\n",
      "Game 46 of 100\n",
      "Game 47 of 100\n",
      "Game 48 of 100\n",
      "Game 49 of 100\n",
      "Game 50 of 100\n",
      "Game 51 of 100\n",
      "Game 52 of 100\n",
      "Game 53 of 100\n",
      "Game 54 of 100\n",
      "Game 55 of 100\n",
      "Game 56 of 100\n",
      "Game 57 of 100\n",
      "Game 58 of 100\n",
      "Game 59 of 100\n",
      "Game 60 of 100\n",
      "Game 61 of 100\n",
      "Game 62 of 100\n",
      "Game 63 of 100\n",
      "Game 64 of 100\n",
      "Game 65 of 100\n",
      "Game 66 of 100\n",
      "Game 67 of 100\n",
      "Game 68 of 100\n",
      "Game 69 of 100\n",
      "Game 70 of 100\n",
      "Game 71 of 100\n",
      "Game 72 of 100\n",
      "Game 73 of 100\n",
      "Game 74 of 100\n",
      "Game 75 of 100\n",
      "Game 76 of 100\n",
      "Game 77 of 100\n",
      "Game 78 of 100\n",
      "Game 79 of 100\n",
      "Game 80 of 100\n",
      "Game 81 of 100\n",
      "Game 82 of 100\n",
      "Game 83 of 100\n",
      "Game 84 of 100\n",
      "Game 85 of 100\n",
      "Game 86 of 100\n",
      "Game 87 of 100\n",
      "Game 88 of 100\n",
      "Game 89 of 100\n",
      "Game 90 of 100\n",
      "Game 91 of 100\n",
      "Game 92 of 100\n",
      "Game 93 of 100\n",
      "Game 94 of 100\n",
      "Game 95 of 100\n",
      "Game 96 of 100\n",
      "Game 97 of 100\n",
      "Game 98 of 100\n",
      "Game 99 of 100\n",
      "Game 100 of 100\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([20., 80.,  0.])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "test_methods(env, num_iterations=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
