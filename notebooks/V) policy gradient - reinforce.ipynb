{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we parametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) V^\\pi(s) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a \\vert s) Q^\\pi(s, a)$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian (identity Covariance) policy is given by:\n",
    "\n",
    "$-G\\sum_{j=1}^N[(a^j - \\hat{a}^j)^2]$\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/framework/dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:117: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorboard/compat/tensorflow_stub/dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from rl_introduction.tools import discount_cumsum, run_experiment_episode_train, DeepAgent\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_sim, action_dim):\n",
    "    input_state = Input(name='input_state', shape=(state_dim,), dtype='float32')\n",
    "    input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "    x = Dense(32, activation='relu')(input_state)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(action_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=input_state, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01):\n",
    "        super().__init__(env,gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            loss = policy_gradient_loss(discounted_return)\n",
    "            self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-3))\n",
    "            self.model.train_on_batch(X,Y)\n",
    "            self.episode = []\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/raphael/rl_test/venv/lib/python3.6/site-packages/tensorflow/python/ops/init_ops.py:1251: calling VarianceScaling.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_test/venv/lib/python3.6/site-packages/gym/logger.py:30: UserWarning: \u001b[33mWARN: Box bound precision lowered by casting to float32\u001b[0m\n",
      "  warnings.warn(colorize('%s: %s'%('WARN', msg % args), 'yellow'))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense (Dense)                (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 28.0\n",
      "episode: 1 - cum reward 11.0\n",
      "episode: 2 - cum reward 16.0\n",
      "episode: 3 - cum reward 36.0\n",
      "episode: 4 - cum reward 22.0\n",
      "episode: 5 - cum reward 14.0\n",
      "episode: 6 - cum reward 32.0\n",
      "episode: 7 - cum reward 22.0\n",
      "episode: 8 - cum reward 41.0\n",
      "episode: 9 - cum reward 53.0\n",
      "episode: 10 - cum reward 34.0\n",
      "episode: 11 - cum reward 53.0\n",
      "episode: 12 - cum reward 23.0\n",
      "episode: 13 - cum reward 33.0\n",
      "episode: 14 - cum reward 45.0\n",
      "episode: 15 - cum reward 68.0\n",
      "episode: 16 - cum reward 29.0\n",
      "episode: 17 - cum reward 17.0\n",
      "episode: 18 - cum reward 58.0\n",
      "episode: 19 - cum reward 44.0\n",
      "episode: 20 - cum reward 14.0\n",
      "episode: 21 - cum reward 33.0\n",
      "episode: 22 - cum reward 51.0\n",
      "episode: 23 - cum reward 37.0\n",
      "episode: 24 - cum reward 17.0\n",
      "episode: 25 - cum reward 112.0\n",
      "episode: 26 - cum reward 21.0\n",
      "episode: 27 - cum reward 13.0\n",
      "episode: 28 - cum reward 27.0\n",
      "episode: 29 - cum reward 40.0\n",
      "episode: 30 - cum reward 13.0\n",
      "episode: 31 - cum reward 36.0\n",
      "episode: 32 - cum reward 21.0\n",
      "episode: 33 - cum reward 59.0\n",
      "episode: 34 - cum reward 16.0\n",
      "episode: 35 - cum reward 11.0\n",
      "episode: 36 - cum reward 29.0\n",
      "episode: 37 - cum reward 12.0\n",
      "episode: 38 - cum reward 24.0\n",
      "episode: 39 - cum reward 45.0\n",
      "episode: 40 - cum reward 17.0\n",
      "episode: 41 - cum reward 18.0\n",
      "episode: 42 - cum reward 46.0\n",
      "episode: 43 - cum reward 30.0\n",
      "episode: 44 - cum reward 23.0\n",
      "episode: 45 - cum reward 27.0\n",
      "episode: 46 - cum reward 33.0\n",
      "episode: 47 - cum reward 30.0\n",
      "episode: 48 - cum reward 19.0\n",
      "episode: 49 - cum reward 47.0\n",
      "episode: 50 - cum reward 20.0\n",
      "episode: 51 - cum reward 82.0\n",
      "episode: 52 - cum reward 54.0\n",
      "episode: 53 - cum reward 35.0\n",
      "episode: 54 - cum reward 26.0\n",
      "episode: 55 - cum reward 19.0\n",
      "episode: 56 - cum reward 33.0\n",
      "episode: 57 - cum reward 14.0\n",
      "episode: 58 - cum reward 24.0\n",
      "episode: 59 - cum reward 55.0\n",
      "episode: 60 - cum reward 49.0\n",
      "episode: 61 - cum reward 63.0\n",
      "episode: 62 - cum reward 26.0\n",
      "episode: 63 - cum reward 21.0\n",
      "episode: 64 - cum reward 34.0\n",
      "episode: 65 - cum reward 61.0\n",
      "episode: 66 - cum reward 24.0\n",
      "episode: 67 - cum reward 39.0\n",
      "episode: 68 - cum reward 27.0\n",
      "episode: 69 - cum reward 49.0\n",
      "episode: 70 - cum reward 48.0\n",
      "episode: 71 - cum reward 21.0\n",
      "episode: 72 - cum reward 63.0\n",
      "episode: 73 - cum reward 36.0\n",
      "episode: 74 - cum reward 21.0\n",
      "episode: 75 - cum reward 31.0\n",
      "episode: 76 - cum reward 45.0\n",
      "episode: 77 - cum reward 14.0\n",
      "episode: 78 - cum reward 14.0\n",
      "episode: 79 - cum reward 28.0\n",
      "episode: 80 - cum reward 42.0\n",
      "episode: 81 - cum reward 77.0\n",
      "episode: 82 - cum reward 25.0\n",
      "episode: 83 - cum reward 32.0\n",
      "episode: 84 - cum reward 113.0\n",
      "episode: 85 - cum reward 39.0\n",
      "episode: 86 - cum reward 39.0\n",
      "episode: 87 - cum reward 35.0\n",
      "episode: 88 - cum reward 76.0\n",
      "episode: 89 - cum reward 44.0\n",
      "episode: 90 - cum reward 41.0\n",
      "episode: 91 - cum reward 40.0\n",
      "episode: 92 - cum reward 33.0\n",
      "episode: 93 - cum reward 81.0\n",
      "episode: 94 - cum reward 41.0\n",
      "episode: 95 - cum reward 51.0\n",
      "episode: 96 - cum reward 49.0\n",
      "episode: 97 - cum reward 75.0\n",
      "episode: 98 - cum reward 33.0\n",
      "episode: 99 - cum reward 41.0\n",
      "episode: 100 - cum reward 34.0\n",
      "episode: 101 - cum reward 38.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = ReinforceAgent(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgentWithMemory(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01, memory_size = 3):\n",
    "        super().__init__(env,  gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                discounted_returns -= discounted_returns.mean()\n",
    "                discounted_returns /= discounted_returns.std()\n",
    "                self.episodes = []\n",
    "                loss = policy_gradient_loss(discounted_returns)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = ReinforceAgentWithMemory(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# improvment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### actor critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "    $,\n",
    "    \\\\begin{align},\n",
    "    \\\\nabla_\\\\theta J(\\\\theta) &= \\\\mathbb{E}_{\\\\pi_\\\\theta}[\\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta(s,a)G_t] \\\\text{REINFORCE}\\\\\\\\\\n\",\n",
    "    \\\\nabla_\\\\theta J(\\\\theta) &= \\\\mathbb{E}_{\\\\pi_\\\\theta}[\\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta(s,a)V_w(s)] \\\\text{V actor-critic}\\\\\\\\\\n\",\n",
    "    \\\\nabla_\\\\theta J(\\\\theta) &= \\\\mathbb{E}_{\\\\pi_\\\\theta}[\\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta(s,a)Q_w(s,a)] \\\\text{Q actor-critic}\\\\\\\\\\n\",\n",
    "    \\\\nabla_\\\\theta J(\\\\theta) &= \\\\mathbb{E}_{\\\\pi_\\\\theta}[\\\\nabla_\\\\theta \\\\log \\\\pi_\\\\theta(s,a)A_w(s,a)] \\\\text{Advantage actor-critic}\\\\\\\\\\n\",\n",
    "    \\\\end{align},\n",
    "    $"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# resampleing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
