{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/5_policy_gradient-reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# uncomment 2 lines\n",
    "#!git clone https://github.com/racousin/rl_introduction.git\n",
    "#from rl_introduction.rl_introduction.tools import discount_cumsum, run_experiment_episode_train, DeepAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run locally"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "from rl_introduction.tools import discount_cumsum, run_experiment_episode_train, DeepAgent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Here we present an alternative of Q learning: policy gradient algorithm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we parametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{G(\\tau)}]$\n",
    "\n",
    "We can proof  that:\n",
    "\n",
    "\n",
    "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) G(\\tau)}]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian (identity Covariance) policy is given by:\n",
    "\n",
    "$-G\\sum_{j=1}^N[(a^j - \\hat{a}^j)^2]$\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce - discrete action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:44:03.955231: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/raphael/rl_introduction/venv/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-03-06 12:44:03.955256: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write custom loss for policy gradient\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = None #to complete\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done: write custom loss for policy gradient\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_model(state_sim, action_dim):\n",
    "    input_state = Input(name='input_state', shape=(state_dim,), dtype='float32')\n",
    "    input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "    x = Dense(32, activation='relu')(input_state)\n",
    "    x = Dense(32, activation='relu')(x)\n",
    "    x = Dense(action_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=input_state, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: complete training of vanilla policy gradient\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01):\n",
    "        super().__init__(env,gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            pass # complete here\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Done: complete training of vanilla policy gradient\n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01):\n",
    "        super().__init__(env,gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            states = np.vstack(episode[:,0])\n",
    "            actions = np.zeros((len(episode), self.action_dim))\n",
    "            actions[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            loss = policy_gradient_loss(discounted_return)\n",
    "            self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-3))\n",
    "            self.model.train_on_batch(states,actions)\n",
    "            self.episode = []\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_state (InputLayer)    [(None, 4)]               0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 32)                160       \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 32)                1056      \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 2)                 66        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:44:10.519125: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/raphael/rl_introduction/venv/lib/python3.7/site-packages/cv2/../../lib64:\n",
      "2022-03-06 12:44:10.519154: W tensorflow/stream_executor/cuda/cuda_driver.cc:269] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2022-03-06 12:44:10.519175: I tensorflow/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (raphael-XPS-13-9370): /proc/driver/nvidia/version does not exist\n",
      "2022-03-06 12:44:10.519346: I tensorflow/core/platform/cpu_feature_guard.cc:151] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 0 - cum reward 15.0\n",
      "episode: 1 - cum reward 28.0\n",
      "episode: 2 - cum reward 12.0\n",
      "episode: 3 - cum reward 24.0\n",
      "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_train_function.<locals>.train_function at 0x7feaac390320> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "episode: 4 - cum reward 21.0\n",
      "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_train_function.<locals>.train_function at 0x7feaa46d7cb0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has experimental_relax_shapes=True option that relaxes argument shapes that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n",
      "episode: 5 - cum reward 23.0\n",
      "episode: 6 - cum reward 18.0\n",
      "episode: 7 - cum reward 31.0\n",
      "episode: 8 - cum reward 22.0\n",
      "episode: 9 - cum reward 40.0\n",
      "episode: 10 - cum reward 36.0\n",
      "episode: 11 - cum reward 18.0\n",
      "episode: 12 - cum reward 39.0\n",
      "episode: 13 - cum reward 20.0\n",
      "episode: 14 - cum reward 41.0\n",
      "episode: 15 - cum reward 31.0\n",
      "episode: 16 - cum reward 54.0\n",
      "episode: 17 - cum reward 20.0\n",
      "episode: 18 - cum reward 45.0\n",
      "episode: 19 - cum reward 29.0\n",
      "episode: 20 - cum reward 26.0\n",
      "episode: 21 - cum reward 43.0\n",
      "episode: 22 - cum reward 33.0\n",
      "episode: 23 - cum reward 19.0\n",
      "episode: 24 - cum reward 26.0\n",
      "episode: 25 - cum reward 15.0\n",
      "episode: 26 - cum reward 25.0\n",
      "episode: 27 - cum reward 22.0\n",
      "episode: 28 - cum reward 47.0\n",
      "episode: 29 - cum reward 12.0\n",
      "episode: 30 - cum reward 27.0\n",
      "episode: 31 - cum reward 11.0\n",
      "episode: 32 - cum reward 19.0\n",
      "episode: 33 - cum reward 45.0\n",
      "episode: 34 - cum reward 22.0\n",
      "episode: 35 - cum reward 22.0\n",
      "episode: 36 - cum reward 22.0\n",
      "episode: 37 - cum reward 51.0\n",
      "episode: 38 - cum reward 11.0\n",
      "episode: 39 - cum reward 38.0\n",
      "episode: 40 - cum reward 39.0\n",
      "episode: 41 - cum reward 54.0\n",
      "episode: 42 - cum reward 43.0\n",
      "episode: 43 - cum reward 16.0\n",
      "episode: 44 - cum reward 35.0\n",
      "episode: 45 - cum reward 21.0\n",
      "episode: 46 - cum reward 47.0\n",
      "episode: 47 - cum reward 20.0\n",
      "episode: 48 - cum reward 40.0\n",
      "episode: 49 - cum reward 16.0\n",
      "episode: 50 - cum reward 21.0\n",
      "episode: 51 - cum reward 21.0\n",
      "episode: 52 - cum reward 12.0\n",
      "episode: 53 - cum reward 49.0\n",
      "episode: 54 - cum reward 39.0\n",
      "episode: 55 - cum reward 82.0\n",
      "episode: 56 - cum reward 20.0\n",
      "episode: 57 - cum reward 19.0\n",
      "episode: 58 - cum reward 36.0\n",
      "episode: 59 - cum reward 41.0\n",
      "episode: 60 - cum reward 31.0\n",
      "episode: 61 - cum reward 51.0\n",
      "episode: 62 - cum reward 64.0\n",
      "episode: 63 - cum reward 20.0\n",
      "episode: 64 - cum reward 24.0\n",
      "episode: 65 - cum reward 18.0\n",
      "episode: 66 - cum reward 85.0\n",
      "episode: 67 - cum reward 46.0\n",
      "episode: 68 - cum reward 19.0\n",
      "episode: 69 - cum reward 17.0\n",
      "episode: 70 - cum reward 24.0\n",
      "episode: 71 - cum reward 22.0\n",
      "episode: 72 - cum reward 28.0\n",
      "episode: 73 - cum reward 69.0\n",
      "episode: 74 - cum reward 54.0\n",
      "episode: 75 - cum reward 25.0\n",
      "episode: 76 - cum reward 21.0\n",
      "episode: 77 - cum reward 22.0\n",
      "episode: 78 - cum reward 129.0\n",
      "episode: 79 - cum reward 20.0\n",
      "episode: 80 - cum reward 32.0\n",
      "episode: 81 - cum reward 49.0\n",
      "episode: 82 - cum reward 29.0\n",
      "episode: 83 - cum reward 25.0\n",
      "episode: 84 - cum reward 41.0\n",
      "episode: 85 - cum reward 16.0\n",
      "episode: 86 - cum reward 35.0\n",
      "episode: 87 - cum reward 48.0\n",
      "episode: 88 - cum reward 14.0\n",
      "episode: 89 - cum reward 87.0\n",
      "episode: 90 - cum reward 35.0\n",
      "episode: 91 - cum reward 28.0\n",
      "episode: 92 - cum reward 42.0\n",
      "episode: 93 - cum reward 25.0\n",
      "episode: 94 - cum reward 28.0\n",
      "episode: 95 - cum reward 51.0\n",
      "episode: 96 - cum reward 14.0\n",
      "episode: 97 - cum reward 23.0\n",
      "episode: 98 - cum reward 83.0\n",
      "episode: 99 - cum reward 35.0\n",
      "episode: 100 - cum reward 28.0\n",
      "episode: 101 - cum reward 23.0\n",
      "episode: 102 - cum reward 29.0\n",
      "episode: 103 - cum reward 60.0\n",
      "episode: 104 - cum reward 23.0\n",
      "episode: 105 - cum reward 66.0\n",
      "episode: 106 - cum reward 72.0\n",
      "episode: 107 - cum reward 33.0\n",
      "episode: 108 - cum reward 43.0\n",
      "episode: 109 - cum reward 58.0\n",
      "episode: 110 - cum reward 15.0\n",
      "episode: 111 - cum reward 21.0\n",
      "episode: 112 - cum reward 14.0\n",
      "episode: 113 - cum reward 41.0\n",
      "episode: 114 - cum reward 45.0\n",
      "episode: 115 - cum reward 96.0\n",
      "episode: 116 - cum reward 38.0\n",
      "episode: 117 - cum reward 27.0\n",
      "episode: 118 - cum reward 34.0\n",
      "episode: 119 - cum reward 53.0\n",
      "episode: 120 - cum reward 56.0\n",
      "episode: 121 - cum reward 28.0\n",
      "episode: 122 - cum reward 46.0\n",
      "episode: 123 - cum reward 22.0\n",
      "episode: 124 - cum reward 66.0\n",
      "episode: 125 - cum reward 16.0\n",
      "episode: 126 - cum reward 47.0\n",
      "episode: 127 - cum reward 21.0\n",
      "episode: 128 - cum reward 35.0\n",
      "episode: 129 - cum reward 82.0\n",
      "episode: 130 - cum reward 58.0\n",
      "episode: 131 - cum reward 28.0\n",
      "episode: 132 - cum reward 18.0\n",
      "episode: 133 - cum reward 44.0\n",
      "episode: 134 - cum reward 30.0\n",
      "episode: 135 - cum reward 50.0\n",
      "episode: 136 - cum reward 28.0\n",
      "episode: 137 - cum reward 9.0\n",
      "episode: 138 - cum reward 15.0\n",
      "episode: 139 - cum reward 40.0\n",
      "episode: 140 - cum reward 12.0\n",
      "episode: 141 - cum reward 26.0\n",
      "episode: 142 - cum reward 28.0\n",
      "episode: 143 - cum reward 20.0\n",
      "episode: 144 - cum reward 18.0\n",
      "episode: 145 - cum reward 32.0\n",
      "episode: 146 - cum reward 26.0\n",
      "episode: 147 - cum reward 40.0\n",
      "episode: 148 - cum reward 37.0\n",
      "episode: 149 - cum reward 22.0\n",
      "episode: 150 - cum reward 56.0\n",
      "episode: 151 - cum reward 72.0\n",
      "episode: 152 - cum reward 44.0\n",
      "episode: 153 - cum reward 32.0\n",
      "episode: 154 - cum reward 52.0\n",
      "episode: 155 - cum reward 42.0\n",
      "episode: 156 - cum reward 30.0\n",
      "episode: 157 - cum reward 31.0\n",
      "episode: 158 - cum reward 42.0\n",
      "episode: 159 - cum reward 48.0\n",
      "episode: 160 - cum reward 74.0\n",
      "episode: 161 - cum reward 69.0\n",
      "episode: 162 - cum reward 75.0\n",
      "episode: 163 - cum reward 41.0\n",
      "episode: 164 - cum reward 37.0\n",
      "episode: 165 - cum reward 85.0\n",
      "episode: 166 - cum reward 37.0\n",
      "episode: 167 - cum reward 49.0\n",
      "episode: 168 - cum reward 45.0\n",
      "episode: 169 - cum reward 52.0\n",
      "episode: 170 - cum reward 23.0\n",
      "episode: 171 - cum reward 25.0\n",
      "episode: 172 - cum reward 60.0\n",
      "episode: 173 - cum reward 22.0\n",
      "episode: 174 - cum reward 24.0\n",
      "episode: 175 - cum reward 32.0\n",
      "episode: 176 - cum reward 61.0\n",
      "episode: 177 - cum reward 27.0\n",
      "episode: 178 - cum reward 41.0\n",
      "episode: 179 - cum reward 59.0\n",
      "episode: 180 - cum reward 84.0\n",
      "episode: 181 - cum reward 87.0\n",
      "episode: 182 - cum reward 18.0\n",
      "episode: 183 - cum reward 45.0\n",
      "episode: 184 - cum reward 73.0\n",
      "episode: 185 - cum reward 121.0\n",
      "episode: 186 - cum reward 173.0\n",
      "episode: 187 - cum reward 52.0\n",
      "episode: 188 - cum reward 69.0\n",
      "episode: 189 - cum reward 55.0\n",
      "episode: 190 - cum reward 29.0\n",
      "episode: 191 - cum reward 26.0\n",
      "episode: 192 - cum reward 112.0\n",
      "episode: 193 - cum reward 39.0\n",
      "episode: 194 - cum reward 44.0\n",
      "episode: 195 - cum reward 53.0\n",
      "episode: 196 - cum reward 69.0\n",
      "episode: 197 - cum reward 15.0\n",
      "episode: 198 - cum reward 35.0\n",
      "episode: 199 - cum reward 67.0\n",
      "episode: 200 - cum reward 23.0\n",
      "episode: 201 - cum reward 43.0\n",
      "episode: 202 - cum reward 42.0\n",
      "episode: 203 - cum reward 129.0\n",
      "episode: 204 - cum reward 54.0\n",
      "episode: 205 - cum reward 29.0\n",
      "episode: 206 - cum reward 102.0\n",
      "episode: 207 - cum reward 39.0\n",
      "episode: 208 - cum reward 51.0\n",
      "episode: 209 - cum reward 51.0\n",
      "episode: 210 - cum reward 32.0\n",
      "episode: 211 - cum reward 31.0\n",
      "episode: 212 - cum reward 40.0\n",
      "episode: 213 - cum reward 56.0\n",
      "episode: 214 - cum reward 86.0\n",
      "episode: 215 - cum reward 61.0\n",
      "episode: 216 - cum reward 49.0\n",
      "episode: 217 - cum reward 105.0\n",
      "episode: 218 - cum reward 27.0\n",
      "episode: 219 - cum reward 108.0\n",
      "episode: 220 - cum reward 49.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 221 - cum reward 107.0\n",
      "episode: 222 - cum reward 52.0\n",
      "episode: 223 - cum reward 35.0\n",
      "episode: 224 - cum reward 44.0\n",
      "episode: 225 - cum reward 85.0\n",
      "episode: 226 - cum reward 48.0\n",
      "episode: 227 - cum reward 83.0\n",
      "episode: 228 - cum reward 36.0\n",
      "episode: 229 - cum reward 81.0\n",
      "episode: 230 - cum reward 31.0\n",
      "episode: 231 - cum reward 41.0\n",
      "episode: 232 - cum reward 86.0\n",
      "episode: 233 - cum reward 33.0\n",
      "episode: 234 - cum reward 46.0\n",
      "episode: 235 - cum reward 66.0\n",
      "episode: 236 - cum reward 50.0\n",
      "episode: 237 - cum reward 36.0\n",
      "episode: 238 - cum reward 44.0\n",
      "episode: 239 - cum reward 21.0\n",
      "episode: 240 - cum reward 38.0\n",
      "episode: 241 - cum reward 33.0\n",
      "episode: 242 - cum reward 59.0\n",
      "episode: 243 - cum reward 57.0\n",
      "episode: 244 - cum reward 64.0\n",
      "episode: 245 - cum reward 33.0\n",
      "episode: 246 - cum reward 53.0\n",
      "episode: 247 - cum reward 74.0\n",
      "episode: 248 - cum reward 29.0\n",
      "episode: 249 - cum reward 25.0\n",
      "episode: 250 - cum reward 26.0\n",
      "episode: 251 - cum reward 47.0\n",
      "episode: 252 - cum reward 53.0\n",
      "episode: 253 - cum reward 162.0\n",
      "episode: 254 - cum reward 49.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = ReinforceAgent(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - vpg_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce with memory - discrete action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgentWithMemory(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01, memory_size = 3):\n",
    "        super().__init__(env,  gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                discounted_returns -= discounted_returns.mean()\n",
    "                discounted_returns /= discounted_returns.std()\n",
    "                self.episodes = []\n",
    "                loss = policy_gradient_loss(discounted_returns)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = ReinforceAgentWithMemory(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - vpg_agent large memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE(general advantage estimation) actor critic\n",
    "We can rewrite the policy gradient\n",
    "\n",
    "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\Phi_t}]$,\n",
    "\n",
    "whith $\\Phi_t$ could be any of:\n",
    "- $\\Phi_t =  G_t$\n",
    "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - V(s_t)$\n",
    "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - Q(s_t,a_t)$\n",
    "\n",
    "\n",
    "For the last 2 cases we need to estimate V or Q (the critics). We do it as the same way at deepQ.\n",
    "https://arxiv.org/pdf/1506.02438.pdf\n",
    "\n",
    "$\\phi_k = \\arg \\min_{\\phi} E_{s_t, G_t \\sim \\pi_k}[{\\left( V_{\\phi}(s_t) - G_t \\right)^2}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### off policy\n",
    "To build an experience replay for policy gradient, it is necessary to unbias the experiences.\n",
    "https://arxiv.org/pdf/1205.4839.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
