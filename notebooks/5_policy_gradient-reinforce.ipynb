{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/5_policy_gradient-reinforce.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
    "!pip install pygame==2.1.0 > /dev/null 2>&1\n",
    "!pip install gym==0.23.1 > /dev/null 2>&1\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1\n",
    "from rl_introduction.rl_introduction.tools import Agent, DeepAgent, plot_values_lake, policy_improvement, discount_cumsum, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We will experiment our algo with CartPole\n",
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Here we present an alternative of Q learning: policy gradient algorithm\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we parametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{G(\\tau)}]$\n",
    "\n",
    "We can proof  that:\n",
    "\n",
    "\n",
    "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) G(\\tau)}]$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "\n",
    "2. In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In keras, it is easier to pass the loss than the gradient.\n",
    "1. It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "2. It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian (identity Covariance) policy is given by:\n",
    "\n",
    "$-G\\sum_{j=1}^N[(a^j - \\hat{a}^j)^2]$\n",
    "\n",
    "$\\hat{a}^j = \\pi_\\theta(s_t)^j$.\n",
    "\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce - discrete action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow.keras.backend as K\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 0): write policy gradient interaction with the environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write a keras model that represent our parametrized pi function\n",
    "# We should be able to run pi.predict([s]) and it should return [[P(a_0|s), P(a_1|s) .. P(a_m|s)]] where m is action size\n",
    "def build_model(state_dim, action_dim):\n",
    "    return model\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "model.predict(np.random.rand(1,state_dim))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write the action choosen by our initial policy gradient function.\n",
    "# It should be a ~ P(.|s) = U(pi_fonction(s))\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01):\n",
    "        super().__init__(env,gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "\n",
    "    def act(self, state):\n",
    "        # complete here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: interact with the environment through episode and display the return"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 1): write custom loss for policy gradient"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: write custom loss for policy gradient\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = None #to complete\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 2): complete training of vanilla policy gradient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TODO: complete training of vanilla policy gradient\n",
    "# \n",
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01):\n",
    "        super().__init__(env,gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1, verbose=0).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward])) # save the trajectory\n",
    "        if done is True: # Compute and use the discouted_reward at the end of the episode and train\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            pass # complete here\n",
    "            # Compute the custom loss\n",
    "            # train\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "model = build_model(state_dim, action_dim)\n",
    "r_agent = ReinforceAgent(env, model)\n",
    "rewards = run_experiment_episode_train(env, r_agent, 300)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - vpg_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TODO 3) : Try different hyerparamters models (number of layers, nodes) and compare learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of our parametrize policy function discrete action space\n",
    "def your_build_model(state_sim, action_dim):\n",
    "    input_state = Input(name='input_state', shape=(state_dim,), dtype='float32')\n",
    "    x = Dense(1, activation='relu')(input_state)\n",
    "    x = Dense(1, activation='relu')(x)\n",
    "    x = Dense(action_dim, activation='softmax')(x)\n",
    "    model = Model(inputs=input_state, outputs=x)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "state_dim = env.observation_space.shape[0]\n",
    "action_dim = env.action_space.n\n",
    "your_model = your_build_model(state_dim, action_dim)\n",
    "your_r_agent = ReinforceAgent(env, your_model)\n",
    "your_rewards = run_experiment_episode_train(env, your_r_agent, 300)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,label='initial_model')\n",
    "ax.plot(your_rewards,label='your_model')\n",
    "ax.set_title('cumulative reward per episode - deep_reinforce_agent')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce with memory - discrete action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In opposite as Q learning, policy optimization is an on-policy algorithm, so we are training directly on the policy output and we need to compute them first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgentWithMemory(DeepAgent):\n",
    "    def __init__(self, env, compiled_model, gamma = .99, epsilon = .01, memory_size = 3):\n",
    "        super().__init__(env,  gamma, epsilon)\n",
    "        \n",
    "        self.model = compiled_model\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1, verbose=0).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        self.episode.append(np.array([current_state, action, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                discounted_returns -= discounted_returns.mean()\n",
    "                discounted_returns /= discounted_returns.std()\n",
    "                self.episodes = []\n",
    "                loss = policy_gradient_loss(discounted_returns)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "model = build_model(state_dim, action_dim)\n",
    "q_agent = ReinforceAgentWithMemory(env, model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "ax.plot(rewards,'+')\n",
    "ax.set_title('cumulative reward per episode - vpg_agent large memory')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# other improvements "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### GAE(general advantage estimation) actor critic\n",
    "We can rewrite the policy gradient\n",
    "\n",
    "$\\nabla_{\\theta} J(\\pi_{\\theta}) = E_{\\tau \\sim \\pi_{\\theta}}[{\\sum_{t=0}^{T} \\nabla_{\\theta} \\log \\pi_{\\theta}(a_t |s_t) \\Phi_t}]$,\n",
    "\n",
    "whith $\\Phi_t$ could be any of:\n",
    "- $\\Phi_t =  G_t$\n",
    "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - V(s_t)$\n",
    "- $\\Phi_t = \\sum_{t'=t}^T R_{t+1} - Q(s_t,a_t)$\n",
    "\n",
    "\n",
    "For the last 2 cases we need to estimate V or Q (the critics). We do it as the same way at deepQ.\n",
    "https://arxiv.org/pdf/1506.02438.pdf\n",
    "\n",
    "$\\phi_k = \\arg \\min_{\\phi} E_{s_t, G_t \\sim \\pi_k}[{\\left( V_{\\phi}(s_t) - G_t \\right)^2}]$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### off policy\n",
    "To build an experience replay for policy gradient, it is necessary to unbias the experiences.\n",
    "https://arxiv.org/pdf/1205.4839.pdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clipping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
