{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xtGIu7r9SwKZ"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U9A-cUMtSwKd"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/6_RL_framework.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Namayrr-SwKf",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!apt-get install build-essential python-dev swig python-pygame > /dev/null 2>&1\n",
    "!pip install box2d-py==2.3.8  > /dev/null 2>&1\n",
    "!pip install 'gymnasium[box2d,atari,accept-rom-license]'==0.29.1\n",
    "!pip install stable_baselines3 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gNuNPhHISwKg"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium as gym\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "UN5s0lxZSwKh"
   },
   "source": [
    "# baselines framework\n",
    "see code:\n",
    "- https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "And doc:\n",
    "- https://stable-baselines3.readthedocs.io/en/master/index.html\n",
    "\n",
    "Pre-trained agent:\n",
    "- https://github.com/DLR-RM/rl-baselines3-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ewkyyWxdSwKh"
   },
   "source": [
    "### Train, Save, Load (Example DQN): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r39yCEsaSwKi"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Create environment\n",
    "\n",
    "env = gym.make('CartPole-v1') # if cartPole is too easy go for env = gym.make('LunarLander-v2')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3)#, prioritized_replay=True, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "# Save the agent\n",
    "model.save(\"dqn_save\")\n",
    "del model  # delete trained model to demonstrate loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "oxlj6LfLSwKj"
   },
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_save\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WuLfXBiKSwKk",
    "outputId": "4228d336-04d4-4fe1-a537-1d7b051d7fe6"
   },
   "outputs": [],
   "source": [
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cIgenlqASwKk"
   },
   "source": [
    "### Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lBPCPHEOSwKl"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import DQN, A2C\n",
    "import torch as th"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6ZuD4tstSwKl"
   },
   "source": [
    "#### The policy:\n",
    "\n",
    "You can use available policies 'MlpPolicy' (fully connected), 'CnnPolicy' (convolutional) and configure them. For example:\n",
    "- 3 layers of 32, 16, 8 neurons for dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QJp32puTSwKl",
    "outputId": "3e62e591-8dc6-4986-8756-ae65f47e5627"
   },
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU, net_arch=[32, 16, 8])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "pbLTLKUuTBAI",
    "outputId": "b57c7fcc-ae26-4e66-f289-9b226cba99fa"
   },
   "outputs": [],
   "source": [
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mrn58K44SwKm"
   },
   "source": [
    "- 1 share layer of 64 neurons\n",
    "- 2 specifics layers od 32, 16 neurons for policy model\n",
    "- 3 specifics layers of 64, 16, 16 neurons for value function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "N63DutGGSwKn",
    "outputId": "1882fae2-7a03-4693-a42c-05ee630dcb2a"
   },
   "outputs": [],
   "source": [
    "policy_kwargs = dict(activation_fn=th.nn.ReLU,\n",
    "                     net_arch=dict(pi=[32, 32], vf=[32, 32]))\n",
    "# Create the agent\n",
    "model = A2C(\"MlpPolicy\", \"CartPole-v1\", policy_kwargs=policy_kwargs, verbose=1)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Gv0scLc1TFVh",
    "outputId": "813324a5-858d-4fe0-bcf0-ca860a6ea6bd"
   },
   "outputs": [],
   "source": [
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6FHiB2xZSwKn"
   },
   "source": [
    "#### Specific parameters of algorithms\n",
    "\n",
    "For example for DQN:\n",
    "- buffer_size – (int) size of the replay buffer\n",
    "- batch_size – (int) size of a batched sampled from replay buffer for training\n",
    "- double_q – (bool) Whether to enable Double-Q learning or not.\n",
    "- prioritized_replay – (bool) if True prioritized replay buffer will be used.\n",
    "- learning_rate – (float) learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "yKx0pNI2SwKo",
    "outputId": "7d944426-ab16-4b6f-e700-61588da9a87a"
   },
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env,\n",
    "            learning_rate=0.0005,\n",
    "            buffer_size=50000,\n",
    "            batch_size=32)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "mW7IBGUQSwKo"
   },
   "source": [
    "For example for A2C:\n",
    "- n_steps – (int) The number of steps to run for each environment per update (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "- max_grad_norm – (float) The maximum value for the gradient clipping\n",
    "- learning_rate – (float) The learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "v86xIvlESwKo",
    "outputId": "1017f987-811d-4e07-bc2d-957393712bf6"
   },
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env,\n",
    "            learning_rate=0.0007,\n",
    "            n_steps=5,\n",
    "            max_grad_norm=0.5)\n",
    "model.learn(total_timesteps=500)\n",
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "mean_reward"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hAeFMrwySwKp"
   },
   "source": [
    "### monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4B_ZiY7PSwKp"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.monitor import Monitor\n",
    "from stable_baselines3 import DDPG, DQN\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "qxVisuh5SwKp",
    "outputId": "e82d679a-70b6-4128-bdf9-0124cebfd0c4"
   },
   "outputs": [],
   "source": [
    "\n",
    "#from stable_baselines3.deepq.policies import MlpPolicy\n",
    "from stable_baselines3 import PPO, DQN\n",
    "# Create log dir\n",
    "log_dir = \"tmp/test\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make('CartPole-v1')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=0)\n",
    "time_steps = 5000\n",
    "model.learn(total_timesteps=time_steps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 638
    },
    "id": "XWUVK2HPSwKp",
    "outputId": "d641f50c-cfad-4770-ae65-75e4560b062d"
   },
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "res = pd.read_csv(log_dir+'/monitor.csv', skiprows=1)\n",
    "ax.plot(res['l'].cumsum(), res['r'], label = 'test')\n",
    "ax.set_xlabel('timesteps')\n",
    "ax.set_ylabel('episode reward')\n",
    "ax.set_title('monitoring test')\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RYmJBAvySwKq"
   },
   "source": [
    "# TODO 1) Benchmark agents\n",
    "Start with CartPole-v1\n",
    "\n",
    "Try 1. differents algorithms (DQN, A2C https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)\n",
    "\n",
    "Try 2. differents hyperparameters\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GcmGNpl5SwKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NSfqznqaSwKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x1k41nTrSwKq"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TODO 2) Join a Competition in RLArena.com\n",
    "1. create acount on rlarena.com\n",
    "2. train your model on env = gym.make('LunarLander-v2')\n",
    "3. save your model with \"{model_type}_model.zip\" example \"DQN_model.zip\"\n",
    "4. export it with the function bellow\n",
    "5. activate it in https://rlarena.com/my-subscriptions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def attach_agent(username, password, agent_attach_name, model_path):\n",
    "    \"\"\"\n",
    "    Attach an agent to the RL Arena competition.\n",
    "\n",
    "    Args:\n",
    "    - username (str): Your username.\n",
    "    - password (str): Your password.\n",
    "    - agent_attach_name (str): The name to attach the agent with.\n",
    "    - model_path (str): The file path to the agent's model.\n",
    "\n",
    "    Returns:\n",
    "    - dict: The JSON response from the server.\n",
    "    \"\"\"\n",
    "    # Endpoint URL\n",
    "    url = 'https://rlarena.com/api/direct_attache_agents_notebook/competition/1'\n",
    "\n",
    "    # Your credentials and agent details\n",
    "    data = {\n",
    "        'username': username,\n",
    "        'password': password,\n",
    "        'agent_attach_name': agent_attach_name,\n",
    "    }\n",
    "\n",
    "    # Files to upload: agent.py and the model\n",
    "    files = {\n",
    "        'model': (model_path, open(model_path, 'rb')),  # Adjust if necessary\n",
    "        'agent_code': ('agent.py', open('rl_introduction/rl_introduction/agent.py', 'r')),   # The agent code\n",
    "    }\n",
    "\n",
    "    # Make the POST request\n",
    "    try:\n",
    "        response = requests.post(url, data=data, files=files)\n",
    "        # Ensure files are closed properly\n",
    "        files['model'][1].close()\n",
    "        files['agent_code'][1].close()\n",
    "        return response.json()  # Return the JSON response\n",
    "    except Exception as e:\n",
    "        print(f\"Failed to attach agent due to: {str(e)}\")\n",
    "        return None\n",
    "\n",
    "# Example usage\n",
    "model_path = \"DQN_model.zip\"\n",
    "response = attach_agent(\"username\", \"password\", \"My LunarLander Agent\", model_path)\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "a_EMJnrLSwKr"
   },
   "source": [
    "### More complete benchmark for doing better!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "57tS4FNxSwKs"
   },
   "source": [
    "DQN parameter\n",
    "https://github.com/openai/baselines-results/blob/master/dqn_results.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4_ats2f_SwKs"
   },
   "source": [
    "atari baselines scores https://github.com/araffin/rl-baselines-zoo/blob/master/benchmark.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rtQ6-0WuSwKs"
   },
   "source": [
    "atari spinningup scores https://spinningup.openai.com/en/latest/spinningup/bench.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pBAEM_McSwKs"
   },
   "source": [
    "compute other metrics https://github.com/deepmind/bsuite"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HIQu29mRSwKs"
   },
   "source": [
    "### Example of train with atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "T0US-yTXSwKs"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.cmd_util import make_atari_env\n",
    "from stable_baselines3.common.vec_env import VecFrameStack\n",
    "from stable_baselines3 import A2C\n",
    "\n",
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multiprocessing training (num_env=4 => 4 processes)\n",
    "env = make_atari_env('PongNoFrameskip-v4', n_envs=4, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=8)\n",
    "\n",
    "model = A2C('CnnPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=250000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZDdpjIuMSwKt"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
