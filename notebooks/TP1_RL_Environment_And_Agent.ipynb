{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb02GRl9w9Jf"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/1_Environment_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DiNMStdCnWk0"
   },
   "outputs": [],
   "source": [
    "%%capture\n",
    "!pip install swig==4.2.1\n",
    "!pip install box2d-py==2.3.8\n",
    "!pip install gymnasium[box2d,atari,accept-rom-license]==0.29.1\n",
    "!pip install pyvirtualdisplay==3.0\n",
    "!pip install opencv-python-headless\n",
    "!pip install imageio imageio-ffmpeg\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryEDr1wCw9Jh"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "from time import time,sleep\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPktUkoZw9Jj"
   },
   "source": [
    "# 1_Environment_and_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5MzQYXUw9Jh"
   },
   "source": [
    "### Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "In RL, we study the interaction between an **agent** and an **environment**. The agent takes actions to achieve a goal, guided by rewards from the environment. Our aim is to develop agents that can learn optimal behaviors through these interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hEQfAzCw9Jj"
   },
   "source": [
    "### Creating an Environment\n",
    "\n",
    "An environment in RL defines the space in which the agent operates. It returns a new state and a reward for each action taken by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ojTF6eXEmj86"
   },
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.done = False\n",
    "\n",
    "    def step(self, action):\n",
    "        if (action % 2 == self.state):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.state = np.random.randint(2)\n",
    "        return self.state, reward, self.done, {}, {}\n",
    "\n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.done = False\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKJx1oBQw9Jj"
   },
   "source": [
    "### Building an Agent\n",
    "Agents in RL decide which actions to take in an environment. A simple agent might act randomly or follow a predetermined policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V5bOAdaw9Jk"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "\n",
    "    def act(self, state):\n",
    "        return np.random.randint(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DzL7BauHmj87"
   },
   "source": [
    "### Running an Experiment\n",
    "\n",
    "To evaluate our agent's performance, we generate trajectories of state-action-reward sequences and compute the total reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kh3S5_Mimj87"
   },
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_steps):\n",
    "    state = env.reset()\n",
    "    res = [state]\n",
    "    for _ in range(nb_steps):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info, _ = env.step(action)\n",
    "        res += [action, reward, state]\n",
    "\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kqSWD-vbmj88"
   },
   "source": [
    "## Understanding the Environment and Agent\n",
    "\n",
    "**Question 1:** What is the **state space** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 2:** What is the **action space** in the provided `Env`/`Agent` class?\n",
    "\n",
    "\n",
    "**Question 3:** What is the **Transition model** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 4:** What is the **Policy** in the provided `Agent` class?\n",
    "\n",
    "\n",
    "**Question 5:** What is the **Reward Function** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 6:** What object **run_experiment** is returning?\n",
    "\n",
    "\n",
    "**Exercise 1:** Instantiating the class `Agent` and `Env` to `run_experiment` on **100 steps**.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 2:** Compute the **cumulative reward** and **discouted cumultative reward**, also known as the return value. You can return more information from `run_experiment` to help.\n",
    "\n",
    "\n",
    "**Question 7:** In this `MDP`, what is the **Expected Return** when following the random policy of the `Agent`?\n",
    "\n",
    "\n",
    "**Question 8:** what would be the **best policy** function for the `Env` environment?\n",
    "\n",
    "\n",
    "**Exercise 3:** Implement the best policy function and use it to run the best agent. Compare its performance to the random agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQugMTYw9Jt"
   },
   "source": [
    "## Start with Gymnasium's Environment\n",
    "\n",
    "\n",
    "In this section, we delve into the diverse range of environments offered by Gymnasium, which is recognized as the gold standard for defining reinforcement learning environments. Our exploration will provide insights into the dynamics of different systems and how they can be modeled and understood within the framework of reinforcement learning.\n",
    "\n",
    "Execute the code below to initiate and observe experiments across various environments: **'FrozenLake-v1'**, **'CartPole-v1'**, **'LunarLanderContinuous-v2'**, and **'PongNoFrameskip-v4'**. While these experiments run, visit the Gymnasium documentation to acquaint yourself with the detailed characteristics and nuances of each environment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "JUTfVmPr1Mce",
    "outputId": "98a3a497-507d-49d5-a29d-f5df1842439c"
   },
   "outputs": [],
   "source": [
    "from rl_introduction.rl_introduction.render_colab import exp_render\n",
    "# Environments to run experiments on\n",
    "env_render_configs = [{\"name\":'FrozenLake-v1', \"fps\":2, \"nb_step\":30},\n",
    " {\"name\":'CartPole-v1', \"fps\":17, \"nb_step\":120},\n",
    "  {\"name\":'LunarLanderContinuous-v2', \"fps\":30, \"nb_step\":300},\n",
    "   {\"name\":'PongNoFrameskip-v4', \"fps\":40, \"nb_step\":800}]\n",
    "for env_render_config in env_render_configs:\n",
    "  exp_render(env_render_config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Z41pmWGM7D1w"
   },
   "source": [
    "### Questions on Environment Dynamics\n",
    "\n",
    "\n",
    "**Question 1:** Actions and States\n",
    "For each environment (FrozenLake-v1, CartPole-v1, LunarLanderContinuous-v2, PongNoFrameskip-v4), identify the action space and state space. Specify whether each is discrete or continuous, and provide their sizes.\n",
    "\n",
    "\n",
    "**Question 2:** Transition Models\n",
    "\n",
    "\n",
    "**Question 3:** Reward Functions\n",
    "\n",
    "### Exercises on Agent Performance\n",
    "\n",
    "**Exercise 1:** Running an Experiment\n",
    "\n",
    "**Exercise 2:** Running Experiments and compute cumulative reward\n",
    "Conduct 20 experiments for each environment using a random agent. For each environement display the cumulative reward with a discount factor of 0.5 and without discount factor of (=1).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
