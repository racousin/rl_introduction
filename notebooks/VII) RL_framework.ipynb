{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# baselines framework\n",
    "see code:\n",
    "- https://github.com/openai/baselines\n",
    "- https://github.com/hill-a/stable-baselines\n",
    "\n",
    "And doc:\n",
    "- https://stable-baselines.readthedocs.io/en/master/index.html\n",
    "\n",
    "Pre-trained agent:\n",
    "- https://github.com/araffin/rl-baselines-zoo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train, Save, Load (Example DQN): "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "\n",
    "from stable_baselines import DQN\n",
    "from stable_baselines.common.evaluation import evaluate_policy\n",
    "\n",
    "\n",
    "# Create environment\n",
    "env = gym.make('CartPole-v0')\n",
    "\n",
    "# Instantiate the agent\n",
    "model = DQN('MlpPolicy', env, learning_rate=1e-3, prioritized_replay=True, verbose=1)\n",
    "# Train the agent\n",
    "model.learn(total_timesteps=10000)\n",
    "# Save the agent\n",
    "model.save(\"dqn_lunar\")\n",
    "del model  # delete trained model to demonstrate loading\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the trained agent\n",
    "model = DQN.load(\"dqn_lunar\")\n",
    "\n",
    "# Evaluate the agent\n",
    "mean_reward, n_steps = evaluate_policy(model, env, n_eval_episodes=10)\n",
    "\n",
    "# Enjoy trained agent\n",
    "obs = env.reset()\n",
    "for i in range(1000):\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper parameters:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import DQN, A2C\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### The policy:\n",
    "\n",
    "You can use available policies 'MlpPolicy' (fully connected), 'CnnPolicy' (convolutional) and configure them. For example:\n",
    "- 3 layers of 32, 16, 8 neurons for dqn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(act_fun=tf.nn.relu, layers=[32, 16, 8])\n",
    "model = DQN('MlpPolicy', env, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- 1 share layer of 64 neurons\n",
    "- 2 specifics layers od 32, 16 neurons for policy model\n",
    "- 3 specifics layers of 64, 16, 16 neurons for value function model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "policy_kwargs = dict(act_fun=tf.nn.tanh, net_arch=[64, dict(pi=[32, 16],\n",
    "                                                          vf=[64, 16, 16])])\n",
    "model = A2C('MlpPolicy', env, policy_kwargs=policy_kwargs)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Specific parameters of algorithms\n",
    "\n",
    "For example for DQN:\n",
    "- buffer_size – (int) size of the replay buffer\n",
    "- batch_size – (int) size of a batched sampled from replay buffer for training\n",
    "- double_q – (bool) Whether to enable Double-Q learning or not.\n",
    "- prioritized_replay – (bool) if True prioritized replay buffer will be used.\n",
    "- learning_rate – (float) learning rate\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = DQN('MlpPolicy', env,\n",
    "            learning_rate=0.0005,\n",
    "            buffer_size=50000,\n",
    "            batch_size=32,\n",
    "            double_q=True)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example for A2C:\n",
    "- n_steps – (int) The number of steps to run for each environment per update (i.e. batch size is n_steps * n_env where n_env is number of environment copies running in parallel)\n",
    "- max_grad_norm – (float) The maximum value for the gradient clipping\n",
    "- learning_rate – (float) The learning rate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = A2C('MlpPolicy', env,\n",
    "            learning_rate=0.0007,\n",
    "            n_steps=5,\n",
    "            max_grad_norm=0.5)\n",
    "model.learn(total_timesteps=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### monitoring "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.bench import Monitor\n",
    "from stable_baselines import DDPG, DQN\n",
    "import seaborn as sns\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from stable_baselines.deepq.policies import MlpPolicy\n",
    "from stable_baselines import PPO2, DQN\n",
    "# Create log dir\n",
    "log_dir = \"tmp/test\"\n",
    "os.makedirs(log_dir, exist_ok=True)\n",
    "\n",
    "env = gym.make('CartPole-v0')\n",
    "env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "\n",
    "model = DQN('MlpPolicy', env, verbose=0)\n",
    "time_steps = 5000\n",
    "model.learn(total_timesteps=timesteps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(figsize=(10,10))\n",
    "res = pd.read_csv(log_dir+'/monitor.csv', skiprows=1)\n",
    "ax.plot(res['l'].cumsum(), res['r'], label = alg_names[i]+ '_' + str(len(layer)))\n",
    "ax.set_xlabel('timesteps')\n",
    "ax.set_ylabel('episode reward')\n",
    "ax.set_title(env_name)\n",
    "ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env_names = ['MountainCar-v0','CartPole-v1','LunarLander-v2']\n",
    "algs = [A2C,DQN]\n",
    "alg_names = ['a2c','dqn']\n",
    "layers = [[32],[64,32],[64,32,16]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for env_name in env_names:\n",
    "    for i,alg in enumerate(algs):\n",
    "        print(alg_names[i])\n",
    "        for l,layer in enumerate(layers):\n",
    "            tf.reset_default_graph()\n",
    "            \n",
    "            log_dir= output_dir='tmp/'+alg_names[i]+'_'+env_name + '_' + str(len(layer))\n",
    "            os.makedirs(log_dir, exist_ok=True) \n",
    "            \n",
    "            env = gym.make(env_name)\n",
    "            env = Monitor(env, log_dir, allow_early_resets=True)\n",
    "            \n",
    "            if alg_names[i] == 'dqn':\n",
    "                policy_kwargs = dict(act_fun=tf.nn.relu, layers=layer)\n",
    "            else:\n",
    "                policy_kwargs = dict(act_fun=tf.nn.relu, net_arch=layer)\n",
    "            \n",
    "            model = alg('MlpPolicy', env, verbose=0, policy_kwargs=policy_kwargs)\n",
    "\n",
    "            model.learn(total_timesteps=time_steps)# , callback=callback)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "glob_res = {}\n",
    "for env_name in env_names:\n",
    "    fig,ax = plt.subplots(figsize=(10,10))\n",
    "    glob_res['env_name'] = {}\n",
    "    for i,alg in enumerate(algs):\n",
    "        glob_res['env_name'][alg_names[i]] = {}\n",
    "        print(alg_names[i])\n",
    "        for l,layer in enumerate(layers):\n",
    "            tf.reset_default_graph()\n",
    "\n",
    "            log_dir= output_dir='tmp/'+alg_names[i]+'_'+env_name + '_' + str(len(layer))\n",
    "            res = pd.read_csv(log_dir+'/monitor.csv', skiprows=1)\n",
    "            ax.plot(res['l'].cumsum(), res['r'], label = alg_names[i]+ '_' + str(len(layer)))\n",
    "            \n",
    "            glob_res['env_name'][alg_names[i]][str(len(layer))] = res\n",
    "    ax.set_xlabel('timesteps')\n",
    "    ax.set_ylabel('episode reward')\n",
    "    ax.set_title(env_name)\n",
    "    ax.legend()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of train with atari"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines import A2C\n",
    "\n",
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multiprocessing training (num_env=4 => 4 processes)\n",
    "env = make_atari_env('PongNoFrameskip-v4', num_env=4, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "model = A2C('CnnPolicy', env, verbose=1)\n",
    "model.learn(total_timesteps=25000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Some pre-trained model -> TODO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('PongNoFrameskip-v4', )\n",
    "model = A2C.load(\"PongNoFrameskip-v4_1.pkl\")#, env= env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "from stable_baselines.common.cmd_util import make_atari_env"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Frame-stacking with 4 frames\n",
    "env = make_atari_env('PongNoFrameskip-v4', num_env=4, seed=0)\n",
    "#env = gym.make('PongNoFrameskip-v4')\n",
    "env = VecFrameStack(env, n_stack=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines import A2C,DQN\n",
    "env = gym.make('CartPole-v1')\n",
    "model = A2C.load_parameters(\"CartPole-v1.pkl\", 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "obs = env.reset()\n",
    "while True:\n",
    "    action, _states = model.predict(obs)\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from stable_baselines.common.cmd_util import make_atari_env\n",
    "from stable_baselines.common.vec_env import VecFrameStack\n",
    "\n",
    "# There already exists an environment generator\n",
    "# that will make and wrap atari environments correctly.\n",
    "# Here we are also multiprocessing training (num_env=4 => 4 processes)\n",
    "env = make_atari_env('PongNoFrameskip-v4', num_env=4, seed=0)\n",
    "# Frame-stacking with 4 frames\n",
    "env = VecFrameStack(env, n_stack=4)\n",
    "\n",
    "model = ACER('CnnPolicy', env, verbose=1)\n",
    "\n",
    "obs = env.reset()\n",
    "while True:\n",
    "    action = env.action_space.sample()\n",
    "    obs, rewards, dones, info = env.step(action)\n",
    "    env.render()\n",
    "env.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
