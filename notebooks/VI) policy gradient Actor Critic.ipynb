{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# simplify clean"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\n",
    "\\begin{align}\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)G_t] \\text{REINFORCE}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)V_w(s)] \\text{V actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)Q_w(s,a)] \\text{Q actor-critic}\\\\\n",
    "\\nabla_\\theta J(\\theta) &= \\mathbb{E}_{\\pi_\\theta}[\\nabla_\\theta \\log \\pi_\\theta(s,a)A_w(s,a)] \\text{Advantage actor-critic}\\\\\n",
    "\\end{align}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q actor critic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum, run_experiment_episode_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class QActorCriticAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 25):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model_actor()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "        self.model_critic.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.turn = 0\n",
    "        \n",
    "\n",
    "    def _build_model_actor(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-4))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, next_state, done]))#, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, next_state, done]))#, reward]))\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            rewards = episode[:,2]\n",
    "            dones = episode[:,4]\n",
    "            X = np.vstack(episode[:,0])\n",
    "            X_next = np.vstack(episode[:,3])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            Y[np.arange(len(episode)), episode[:,1].astype(int)] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Xs_next = np.vstack([ep[3] for ep in self.episodes])\n",
    "                rewardss = np.hstack([ep[4] for ep in self.episodes])\n",
    "                doness = np.hstack([ep[5] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                Ys_values = rewardss + (1 - doness) * self.model_critic.predict(Xs_next).ravel()\n",
    "                #discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs, Ys_values.astype('float32'))\n",
    "                print(early_stopping)\n",
    "                baselines = rewardss + (1 - doness) * self.model_critic.predict(Xs_next).ravel() - self.model_critic.predict(Xs).ravel() \n",
    "                baselines -= baselines.mean()\n",
    "                baselines /= baselines.std()\n",
    "                loss = policy_gradient_loss(baselines.astype('float32'))\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-3))\n",
    "                loss = self.model.train_on_batch(Xs,Ys)\n",
    "                print('loss ', loss)\n",
    "                #Y_values -= Y_values.mean()\n",
    "                #Y_values /= Y_values.std()\n",
    "                #discounted_returns -= discounted_returns.mean()\n",
    "                #discounted_returns /= discounted_returns.std()\n",
    "                #early_stopping = self.model_critic.train_on_batch(Xs,Y_values)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, X_next, rewards, dones])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_8\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_24 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_25 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_26 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,282\n",
      "Trainable params: 1,282\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_9\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_27 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_28 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 1,249\n",
      "Trainable params: 1,249\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 12.0\n",
      "episode: 1 - cum reward 22.0\n",
      "episode: 2 - cum reward 10.0\n",
      "episode: 3 - cum reward 47.0\n",
      "episode: 4 - cum reward 18.0\n",
      "episode: 5 - cum reward 32.0\n",
      "episode: 6 - cum reward 14.0\n",
      "episode: 7 - cum reward 31.0\n",
      "episode: 8 - cum reward 13.0\n",
      "episode: 9 - cum reward 42.0\n",
      "episode: 10 - cum reward 40.0\n",
      "episode: 11 - cum reward 13.0\n",
      "episode: 12 - cum reward 15.0\n",
      "episode: 13 - cum reward 23.0\n",
      "episode: 14 - cum reward 26.0\n",
      "episode: 15 - cum reward 14.0\n",
      "episode: 16 - cum reward 9.0\n",
      "episode: 17 - cum reward 12.0\n",
      "episode: 18 - cum reward 17.0\n",
      "episode: 19 - cum reward 23.0\n",
      "episode: 20 - cum reward 15.0\n",
      "episode: 21 - cum reward 16.0\n",
      "episode: 22 - cum reward 11.0\n",
      "episode: 23 - cum reward 16.0\n",
      "episode: 24 - cum reward 19.0\n",
      "1.003575\n",
      "loss  -0.009596882\n",
      "episode: 25 - cum reward 12.0\n",
      "episode: 26 - cum reward 18.0\n",
      "episode: 27 - cum reward 21.0\n",
      "episode: 28 - cum reward 10.0\n",
      "episode: 29 - cum reward 12.0\n",
      "episode: 30 - cum reward 31.0\n",
      "episode: 31 - cum reward 13.0\n",
      "episode: 32 - cum reward 13.0\n",
      "episode: 33 - cum reward 22.0\n",
      "episode: 34 - cum reward 37.0\n",
      "episode: 35 - cum reward 27.0\n",
      "episode: 36 - cum reward 17.0\n",
      "episode: 37 - cum reward 25.0\n",
      "episode: 38 - cum reward 35.0\n",
      "episode: 39 - cum reward 13.0\n",
      "episode: 40 - cum reward 12.0\n",
      "episode: 41 - cum reward 18.0\n",
      "episode: 42 - cum reward 14.0\n",
      "episode: 43 - cum reward 9.0\n",
      "episode: 44 - cum reward 31.0\n",
      "episode: 45 - cum reward 15.0\n",
      "episode: 46 - cum reward 11.0\n",
      "episode: 47 - cum reward 16.0\n",
      "episode: 48 - cum reward 11.0\n",
      "episode: 49 - cum reward 21.0\n",
      "episode: 50 - cum reward 13.0\n",
      "1.0034447\n",
      "loss  -0.0012010941\n",
      "episode: 51 - cum reward 21.0\n",
      "episode: 52 - cum reward 38.0\n",
      "episode: 53 - cum reward 15.0\n",
      "episode: 54 - cum reward 40.0\n",
      "episode: 55 - cum reward 8.0\n",
      "episode: 56 - cum reward 11.0\n",
      "episode: 57 - cum reward 46.0\n",
      "episode: 58 - cum reward 11.0\n",
      "episode: 59 - cum reward 15.0\n",
      "episode: 60 - cum reward 15.0\n",
      "episode: 61 - cum reward 31.0\n",
      "episode: 62 - cum reward 44.0\n",
      "episode: 63 - cum reward 24.0\n",
      "episode: 64 - cum reward 28.0\n",
      "episode: 65 - cum reward 21.0\n",
      "episode: 66 - cum reward 23.0\n",
      "episode: 67 - cum reward 52.0\n",
      "episode: 68 - cum reward 26.0\n",
      "episode: 69 - cum reward 16.0\n",
      "episode: 70 - cum reward 47.0\n",
      "episode: 71 - cum reward 11.0\n",
      "episode: 72 - cum reward 19.0\n",
      "episode: 73 - cum reward 34.0\n",
      "episode: 74 - cum reward 44.0\n",
      "episode: 75 - cum reward 11.0\n",
      "episode: 76 - cum reward 26.0\n",
      "1.002626\n",
      "loss  -0.05215419\n",
      "episode: 77 - cum reward 21.0\n",
      "episode: 78 - cum reward 14.0\n",
      "episode: 79 - cum reward 15.0\n",
      "episode: 80 - cum reward 12.0\n",
      "episode: 81 - cum reward 19.0\n",
      "episode: 82 - cum reward 16.0\n",
      "episode: 83 - cum reward 23.0\n",
      "episode: 84 - cum reward 12.0\n",
      "episode: 85 - cum reward 16.0\n",
      "episode: 86 - cum reward 14.0\n",
      "episode: 87 - cum reward 21.0\n",
      "episode: 88 - cum reward 34.0\n",
      "episode: 89 - cum reward 39.0\n",
      "episode: 90 - cum reward 34.0\n",
      "episode: 91 - cum reward 47.0\n",
      "episode: 92 - cum reward 42.0\n",
      "episode: 93 - cum reward 35.0\n",
      "episode: 94 - cum reward 18.0\n",
      "episode: 95 - cum reward 16.0\n",
      "episode: 96 - cum reward 20.0\n",
      "episode: 97 - cum reward 12.0\n",
      "episode: 98 - cum reward 26.0\n",
      "episode: 99 - cum reward 20.0\n",
      "episode: 100 - cum reward 17.0\n",
      "episode: 101 - cum reward 15.0\n",
      "episode: 102 - cum reward 18.0\n",
      "1.0027041\n",
      "loss  -0.0136975\n",
      "episode: 103 - cum reward 11.0\n",
      "episode: 104 - cum reward 23.0\n",
      "episode: 105 - cum reward 13.0\n",
      "episode: 106 - cum reward 8.0\n",
      "episode: 107 - cum reward 25.0\n",
      "episode: 108 - cum reward 18.0\n",
      "episode: 109 - cum reward 15.0\n",
      "episode: 110 - cum reward 23.0\n",
      "episode: 111 - cum reward 32.0\n",
      "episode: 112 - cum reward 19.0\n",
      "episode: 113 - cum reward 20.0\n",
      "episode: 114 - cum reward 16.0\n",
      "episode: 115 - cum reward 32.0\n",
      "episode: 116 - cum reward 14.0\n",
      "episode: 117 - cum reward 16.0\n",
      "episode: 118 - cum reward 38.0\n",
      "episode: 119 - cum reward 23.0\n",
      "episode: 120 - cum reward 25.0\n",
      "episode: 121 - cum reward 23.0\n",
      "episode: 122 - cum reward 18.0\n",
      "episode: 123 - cum reward 12.0\n",
      "episode: 124 - cum reward 24.0\n",
      "episode: 125 - cum reward 15.0\n",
      "episode: 126 - cum reward 27.0\n",
      "episode: 127 - cum reward 18.0\n",
      "episode: 128 - cum reward 17.0\n",
      "1.0027887\n",
      "loss  -0.058957156\n",
      "episode: 129 - cum reward 13.0\n",
      "episode: 130 - cum reward 22.0\n",
      "episode: 131 - cum reward 21.0\n",
      "episode: 132 - cum reward 20.0\n",
      "episode: 133 - cum reward 17.0\n",
      "episode: 134 - cum reward 10.0\n",
      "episode: 135 - cum reward 20.0\n",
      "episode: 136 - cum reward 10.0\n",
      "episode: 137 - cum reward 10.0\n",
      "episode: 138 - cum reward 27.0\n",
      "episode: 139 - cum reward 35.0\n",
      "episode: 140 - cum reward 15.0\n",
      "episode: 141 - cum reward 9.0\n",
      "episode: 142 - cum reward 37.0\n",
      "episode: 143 - cum reward 12.0\n",
      "episode: 144 - cum reward 8.0\n",
      "episode: 145 - cum reward 18.0\n",
      "episode: 146 - cum reward 17.0\n",
      "episode: 147 - cum reward 18.0\n",
      "episode: 148 - cum reward 10.0\n",
      "episode: 149 - cum reward 10.0\n",
      "episode: 150 - cum reward 32.0\n",
      "episode: 151 - cum reward 32.0\n",
      "episode: 152 - cum reward 11.0\n",
      "episode: 153 - cum reward 16.0\n",
      "episode: 154 - cum reward 26.0\n",
      "1.0027074\n",
      "loss  -0.063710704\n",
      "episode: 155 - cum reward 37.0\n",
      "episode: 156 - cum reward 15.0\n",
      "episode: 157 - cum reward 23.0\n",
      "episode: 158 - cum reward 20.0\n",
      "episode: 159 - cum reward 19.0\n",
      "episode: 160 - cum reward 63.0\n",
      "episode: 161 - cum reward 9.0\n",
      "episode: 162 - cum reward 19.0\n",
      "episode: 163 - cum reward 20.0\n",
      "episode: 164 - cum reward 27.0\n",
      "episode: 165 - cum reward 15.0\n",
      "episode: 166 - cum reward 14.0\n",
      "episode: 167 - cum reward 17.0\n",
      "episode: 168 - cum reward 11.0\n",
      "episode: 169 - cum reward 23.0\n",
      "episode: 170 - cum reward 14.0\n",
      "episode: 171 - cum reward 11.0\n",
      "episode: 172 - cum reward 18.0\n",
      "episode: 173 - cum reward 24.0\n",
      "episode: 174 - cum reward 30.0\n",
      "episode: 175 - cum reward 10.0\n",
      "episode: 176 - cum reward 21.0\n",
      "episode: 177 - cum reward 16.0\n",
      "episode: 178 - cum reward 29.0\n",
      "episode: 179 - cum reward 41.0\n",
      "episode: 180 - cum reward 19.0\n",
      "1.0021367\n",
      "loss  -0.07467159\n",
      "episode: 181 - cum reward 25.0\n",
      "episode: 182 - cum reward 31.0\n",
      "episode: 183 - cum reward 20.0\n",
      "episode: 184 - cum reward 13.0\n",
      "episode: 185 - cum reward 24.0\n",
      "episode: 186 - cum reward 45.0\n",
      "episode: 187 - cum reward 17.0\n",
      "episode: 188 - cum reward 13.0\n",
      "episode: 189 - cum reward 23.0\n",
      "episode: 190 - cum reward 12.0\n",
      "episode: 191 - cum reward 41.0\n",
      "episode: 192 - cum reward 17.0\n",
      "episode: 193 - cum reward 14.0\n",
      "episode: 194 - cum reward 12.0\n",
      "episode: 195 - cum reward 26.0\n",
      "episode: 196 - cum reward 27.0\n",
      "episode: 197 - cum reward 44.0\n",
      "episode: 198 - cum reward 20.0\n",
      "episode: 199 - cum reward 40.0\n",
      "episode: 200 - cum reward 13.0\n",
      "episode: 201 - cum reward 28.0\n",
      "episode: 202 - cum reward 16.0\n",
      "episode: 203 - cum reward 35.0\n",
      "episode: 204 - cum reward 54.0\n",
      "episode: 205 - cum reward 32.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 206 - cum reward 16.0\n",
      "1.0018365\n",
      "loss  -0.13626537\n",
      "episode: 207 - cum reward 17.0\n",
      "episode: 208 - cum reward 45.0\n",
      "episode: 209 - cum reward 28.0\n",
      "episode: 210 - cum reward 36.0\n",
      "episode: 211 - cum reward 14.0\n",
      "episode: 212 - cum reward 35.0\n",
      "episode: 213 - cum reward 21.0\n",
      "episode: 214 - cum reward 20.0\n",
      "episode: 215 - cum reward 17.0\n",
      "episode: 216 - cum reward 19.0\n",
      "episode: 217 - cum reward 13.0\n",
      "episode: 218 - cum reward 26.0\n",
      "episode: 219 - cum reward 12.0\n",
      "episode: 220 - cum reward 25.0\n",
      "episode: 221 - cum reward 18.0\n",
      "episode: 222 - cum reward 33.0\n",
      "episode: 223 - cum reward 15.0\n",
      "episode: 224 - cum reward 37.0\n",
      "episode: 225 - cum reward 30.0\n",
      "episode: 226 - cum reward 36.0\n",
      "episode: 227 - cum reward 18.0\n",
      "episode: 228 - cum reward 8.0\n",
      "episode: 229 - cum reward 15.0\n",
      "episode: 230 - cum reward 29.0\n",
      "episode: 231 - cum reward 28.0\n",
      "episode: 232 - cum reward 16.0\n",
      "1.0020596\n",
      "loss  -0.1602754\n",
      "episode: 233 - cum reward 24.0\n",
      "episode: 234 - cum reward 15.0\n",
      "episode: 235 - cum reward 35.0\n",
      "episode: 236 - cum reward 20.0\n",
      "episode: 237 - cum reward 28.0\n",
      "episode: 238 - cum reward 15.0\n",
      "episode: 239 - cum reward 11.0\n",
      "episode: 240 - cum reward 18.0\n",
      "episode: 241 - cum reward 16.0\n",
      "episode: 242 - cum reward 39.0\n",
      "episode: 243 - cum reward 26.0\n",
      "episode: 244 - cum reward 11.0\n",
      "episode: 245 - cum reward 37.0\n",
      "episode: 246 - cum reward 16.0\n",
      "episode: 247 - cum reward 14.0\n",
      "episode: 248 - cum reward 21.0\n",
      "episode: 249 - cum reward 12.0\n",
      "episode: 250 - cum reward 27.0\n",
      "episode: 251 - cum reward 38.0\n",
      "episode: 252 - cum reward 15.0\n",
      "episode: 253 - cum reward 55.0\n",
      "episode: 254 - cum reward 11.0\n",
      "episode: 255 - cum reward 23.0\n",
      "episode: 256 - cum reward 17.0\n",
      "episode: 257 - cum reward 16.0\n",
      "episode: 258 - cum reward 38.0\n",
      "1.0018045\n",
      "loss  -0.12857105\n",
      "episode: 259 - cum reward 54.0\n",
      "episode: 260 - cum reward 38.0\n",
      "episode: 261 - cum reward 49.0\n",
      "episode: 262 - cum reward 35.0\n",
      "episode: 263 - cum reward 50.0\n",
      "episode: 264 - cum reward 10.0\n",
      "episode: 265 - cum reward 29.0\n",
      "episode: 266 - cum reward 15.0\n",
      "episode: 267 - cum reward 22.0\n",
      "episode: 268 - cum reward 12.0\n",
      "episode: 269 - cum reward 17.0\n",
      "episode: 270 - cum reward 16.0\n",
      "episode: 271 - cum reward 21.0\n",
      "episode: 272 - cum reward 13.0\n",
      "episode: 273 - cum reward 34.0\n",
      "episode: 274 - cum reward 11.0\n",
      "episode: 275 - cum reward 26.0\n",
      "episode: 276 - cum reward 20.0\n",
      "episode: 277 - cum reward 33.0\n",
      "episode: 278 - cum reward 28.0\n",
      "episode: 279 - cum reward 23.0\n",
      "episode: 280 - cum reward 13.0\n",
      "episode: 281 - cum reward 30.0\n",
      "episode: 282 - cum reward 20.0\n",
      "episode: 283 - cum reward 21.0\n",
      "episode: 284 - cum reward 32.0\n",
      "1.0011486\n",
      "loss  -0.116462015\n",
      "episode: 285 - cum reward 12.0\n",
      "episode: 286 - cum reward 22.0\n",
      "episode: 287 - cum reward 40.0\n",
      "episode: 288 - cum reward 19.0\n",
      "episode: 289 - cum reward 8.0\n",
      "episode: 290 - cum reward 15.0\n",
      "episode: 291 - cum reward 25.0\n",
      "episode: 292 - cum reward 13.0\n",
      "episode: 293 - cum reward 26.0\n",
      "episode: 294 - cum reward 22.0\n",
      "episode: 295 - cum reward 23.0\n",
      "episode: 296 - cum reward 18.0\n",
      "episode: 297 - cum reward 27.0\n",
      "episode: 298 - cum reward 15.0\n",
      "episode: 299 - cum reward 11.0\n",
      "episode: 300 - cum reward 10.0\n",
      "episode: 301 - cum reward 19.0\n",
      "episode: 302 - cum reward 46.0\n",
      "episode: 303 - cum reward 17.0\n",
      "episode: 304 - cum reward 12.0\n",
      "episode: 305 - cum reward 21.0\n",
      "episode: 306 - cum reward 44.0\n",
      "episode: 307 - cum reward 16.0\n",
      "episode: 308 - cum reward 11.0\n",
      "episode: 309 - cum reward 39.0\n",
      "episode: 310 - cum reward 11.0\n",
      "1.0013167\n",
      "loss  -0.20325585\n",
      "episode: 311 - cum reward 22.0\n",
      "episode: 312 - cum reward 49.0\n",
      "episode: 313 - cum reward 21.0\n",
      "episode: 314 - cum reward 21.0\n",
      "episode: 315 - cum reward 30.0\n",
      "episode: 316 - cum reward 19.0\n",
      "episode: 317 - cum reward 13.0\n",
      "episode: 318 - cum reward 34.0\n",
      "episode: 319 - cum reward 33.0\n",
      "episode: 320 - cum reward 20.0\n",
      "episode: 321 - cum reward 49.0\n",
      "episode: 322 - cum reward 12.0\n",
      "episode: 323 - cum reward 29.0\n",
      "episode: 324 - cum reward 16.0\n",
      "episode: 325 - cum reward 26.0\n",
      "episode: 326 - cum reward 13.0\n",
      "episode: 327 - cum reward 12.0\n",
      "episode: 328 - cum reward 18.0\n",
      "episode: 329 - cum reward 20.0\n",
      "episode: 330 - cum reward 45.0\n",
      "episode: 331 - cum reward 14.0\n",
      "episode: 332 - cum reward 44.0\n",
      "episode: 333 - cum reward 38.0\n",
      "episode: 334 - cum reward 62.0\n",
      "episode: 335 - cum reward 14.0\n",
      "episode: 336 - cum reward 13.0\n",
      "1.0013846\n",
      "loss  -0.18667974\n",
      "episode: 337 - cum reward 11.0\n",
      "episode: 338 - cum reward 10.0\n",
      "episode: 339 - cum reward 12.0\n",
      "episode: 340 - cum reward 22.0\n",
      "episode: 341 - cum reward 23.0\n",
      "episode: 342 - cum reward 14.0\n",
      "episode: 343 - cum reward 14.0\n",
      "episode: 344 - cum reward 15.0\n",
      "episode: 345 - cum reward 10.0\n",
      "episode: 346 - cum reward 23.0\n",
      "episode: 347 - cum reward 18.0\n",
      "episode: 348 - cum reward 26.0\n",
      "episode: 349 - cum reward 25.0\n",
      "episode: 350 - cum reward 16.0\n",
      "episode: 351 - cum reward 15.0\n",
      "episode: 352 - cum reward 57.0\n",
      "episode: 353 - cum reward 19.0\n",
      "episode: 354 - cum reward 13.0\n",
      "episode: 355 - cum reward 23.0\n",
      "episode: 356 - cum reward 29.0\n",
      "episode: 357 - cum reward 17.0\n",
      "episode: 358 - cum reward 17.0\n",
      "episode: 359 - cum reward 18.0\n",
      "episode: 360 - cum reward 16.0\n",
      "episode: 361 - cum reward 35.0\n",
      "episode: 362 - cum reward 34.0\n",
      "1.001129\n",
      "loss  -0.19405638\n",
      "episode: 363 - cum reward 33.0\n",
      "episode: 364 - cum reward 38.0\n",
      "episode: 365 - cum reward 18.0\n",
      "episode: 366 - cum reward 13.0\n",
      "episode: 367 - cum reward 30.0\n",
      "episode: 368 - cum reward 21.0\n",
      "episode: 369 - cum reward 36.0\n",
      "episode: 370 - cum reward 21.0\n",
      "episode: 371 - cum reward 59.0\n",
      "episode: 372 - cum reward 13.0\n",
      "episode: 373 - cum reward 19.0\n",
      "episode: 374 - cum reward 13.0\n",
      "episode: 375 - cum reward 15.0\n",
      "episode: 376 - cum reward 42.0\n",
      "episode: 377 - cum reward 37.0\n",
      "episode: 378 - cum reward 17.0\n",
      "episode: 379 - cum reward 17.0\n",
      "episode: 380 - cum reward 14.0\n",
      "episode: 381 - cum reward 40.0\n",
      "episode: 382 - cum reward 14.0\n",
      "episode: 383 - cum reward 12.0\n",
      "episode: 384 - cum reward 43.0\n",
      "episode: 385 - cum reward 35.0\n",
      "episode: 386 - cum reward 20.0\n",
      "episode: 387 - cum reward 54.0\n",
      "episode: 388 - cum reward 16.0\n",
      "1.0006889\n",
      "loss  -0.16156946\n",
      "episode: 389 - cum reward 25.0\n",
      "episode: 390 - cum reward 16.0\n",
      "episode: 391 - cum reward 15.0\n",
      "episode: 392 - cum reward 17.0\n",
      "episode: 393 - cum reward 8.0\n",
      "episode: 394 - cum reward 44.0\n",
      "episode: 395 - cum reward 24.0\n",
      "episode: 396 - cum reward 22.0\n",
      "episode: 397 - cum reward 35.0\n",
      "episode: 398 - cum reward 20.0\n",
      "episode: 399 - cum reward 14.0\n",
      "episode: 400 - cum reward 21.0\n",
      "episode: 401 - cum reward 12.0\n",
      "episode: 402 - cum reward 34.0\n",
      "episode: 403 - cum reward 20.0\n",
      "episode: 404 - cum reward 19.0\n",
      "episode: 405 - cum reward 22.0\n",
      "episode: 406 - cum reward 34.0\n",
      "episode: 407 - cum reward 61.0\n",
      "episode: 408 - cum reward 17.0\n",
      "episode: 409 - cum reward 35.0\n",
      "episode: 410 - cum reward 44.0\n",
      "episode: 411 - cum reward 19.0\n",
      "episode: 412 - cum reward 12.0\n",
      "episode: 413 - cum reward 11.0\n",
      "episode: 414 - cum reward 9.0\n",
      "1.0006325\n",
      "loss  -0.20133714\n",
      "episode: 415 - cum reward 13.0\n",
      "episode: 416 - cum reward 9.0\n",
      "episode: 417 - cum reward 14.0\n",
      "episode: 418 - cum reward 31.0\n",
      "episode: 419 - cum reward 17.0\n",
      "episode: 420 - cum reward 38.0\n",
      "episode: 421 - cum reward 40.0\n",
      "episode: 422 - cum reward 22.0\n",
      "episode: 423 - cum reward 22.0\n",
      "episode: 424 - cum reward 10.0\n",
      "episode: 425 - cum reward 21.0\n",
      "episode: 426 - cum reward 17.0\n",
      "episode: 427 - cum reward 9.0\n",
      "episode: 428 - cum reward 37.0\n",
      "episode: 429 - cum reward 14.0\n",
      "episode: 430 - cum reward 33.0\n",
      "episode: 431 - cum reward 43.0\n",
      "episode: 432 - cum reward 78.0\n",
      "episode: 433 - cum reward 12.0\n",
      "episode: 434 - cum reward 68.0\n",
      "episode: 435 - cum reward 25.0\n",
      "episode: 436 - cum reward 18.0\n",
      "episode: 437 - cum reward 23.0\n",
      "episode: 438 - cum reward 46.0\n",
      "episode: 439 - cum reward 48.0\n",
      "episode: 440 - cum reward 28.0\n",
      "1.0007901\n",
      "loss  -0.20559561\n",
      "episode: 441 - cum reward 20.0\n",
      "episode: 442 - cum reward 16.0\n",
      "episode: 443 - cum reward 22.0\n",
      "episode: 444 - cum reward 51.0\n",
      "episode: 445 - cum reward 10.0\n",
      "episode: 446 - cum reward 32.0\n",
      "episode: 447 - cum reward 24.0\n",
      "episode: 448 - cum reward 9.0\n",
      "episode: 449 - cum reward 50.0\n",
      "episode: 450 - cum reward 24.0\n",
      "episode: 451 - cum reward 33.0\n",
      "episode: 452 - cum reward 16.0\n",
      "episode: 453 - cum reward 15.0\n",
      "episode: 454 - cum reward 30.0\n",
      "episode: 455 - cum reward 28.0\n",
      "episode: 456 - cum reward 14.0\n",
      "episode: 457 - cum reward 29.0\n",
      "episode: 458 - cum reward 40.0\n",
      "episode: 459 - cum reward 76.0\n",
      "episode: 460 - cum reward 52.0\n",
      "episode: 461 - cum reward 61.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 462 - cum reward 29.0\n",
      "episode: 463 - cum reward 13.0\n",
      "episode: 464 - cum reward 60.0\n",
      "episode: 465 - cum reward 16.0\n",
      "episode: 466 - cum reward 19.0\n",
      "1.0007082\n",
      "loss  -0.24962495\n",
      "episode: 467 - cum reward 52.0\n",
      "episode: 468 - cum reward 43.0\n",
      "episode: 469 - cum reward 10.0\n",
      "episode: 470 - cum reward 23.0\n",
      "episode: 471 - cum reward 23.0\n",
      "episode: 472 - cum reward 12.0\n",
      "episode: 473 - cum reward 63.0\n",
      "episode: 474 - cum reward 28.0\n",
      "episode: 475 - cum reward 27.0\n",
      "episode: 476 - cum reward 26.0\n",
      "episode: 477 - cum reward 34.0\n",
      "episode: 478 - cum reward 25.0\n",
      "episode: 479 - cum reward 22.0\n",
      "episode: 480 - cum reward 18.0\n",
      "episode: 481 - cum reward 19.0\n",
      "episode: 482 - cum reward 26.0\n",
      "episode: 483 - cum reward 27.0\n",
      "episode: 484 - cum reward 61.0\n",
      "episode: 485 - cum reward 36.0\n",
      "episode: 486 - cum reward 18.0\n",
      "episode: 487 - cum reward 28.0\n",
      "episode: 488 - cum reward 31.0\n",
      "episode: 489 - cum reward 25.0\n",
      "episode: 490 - cum reward 56.0\n",
      "episode: 491 - cum reward 22.0\n",
      "episode: 492 - cum reward 28.0\n",
      "1.0004805\n",
      "loss  -0.25671935\n",
      "episode: 493 - cum reward 19.0\n",
      "episode: 494 - cum reward 13.0\n",
      "episode: 495 - cum reward 60.0\n",
      "episode: 496 - cum reward 24.0\n",
      "episode: 497 - cum reward 18.0\n",
      "episode: 498 - cum reward 16.0\n",
      "episode: 499 - cum reward 12.0\n",
      "episode: 500 - cum reward 15.0\n",
      "episode: 501 - cum reward 29.0\n",
      "episode: 502 - cum reward 15.0\n",
      "episode: 503 - cum reward 23.0\n",
      "episode: 504 - cum reward 12.0\n",
      "episode: 505 - cum reward 26.0\n",
      "episode: 506 - cum reward 26.0\n",
      "episode: 507 - cum reward 19.0\n",
      "episode: 508 - cum reward 56.0\n",
      "episode: 509 - cum reward 13.0\n",
      "episode: 510 - cum reward 32.0\n",
      "episode: 511 - cum reward 19.0\n",
      "episode: 512 - cum reward 37.0\n",
      "episode: 513 - cum reward 32.0\n",
      "episode: 514 - cum reward 25.0\n",
      "episode: 515 - cum reward 29.0\n",
      "episode: 516 - cum reward 13.0\n",
      "episode: 517 - cum reward 32.0\n",
      "episode: 518 - cum reward 20.0\n",
      "1.0001415\n",
      "loss  -0.19331382\n",
      "episode: 519 - cum reward 45.0\n",
      "episode: 520 - cum reward 21.0\n",
      "episode: 521 - cum reward 14.0\n",
      "episode: 522 - cum reward 24.0\n",
      "episode: 523 - cum reward 28.0\n",
      "episode: 524 - cum reward 14.0\n",
      "episode: 525 - cum reward 24.0\n",
      "episode: 526 - cum reward 14.0\n",
      "episode: 527 - cum reward 15.0\n",
      "episode: 528 - cum reward 12.0\n",
      "episode: 529 - cum reward 24.0\n",
      "episode: 530 - cum reward 13.0\n",
      "episode: 531 - cum reward 41.0\n",
      "episode: 532 - cum reward 48.0\n",
      "episode: 533 - cum reward 33.0\n",
      "episode: 534 - cum reward 27.0\n",
      "episode: 535 - cum reward 45.0\n",
      "episode: 536 - cum reward 23.0\n",
      "episode: 537 - cum reward 14.0\n",
      "episode: 538 - cum reward 15.0\n",
      "episode: 539 - cum reward 34.0\n",
      "episode: 540 - cum reward 13.0\n",
      "episode: 541 - cum reward 17.0\n",
      "episode: 542 - cum reward 11.0\n",
      "episode: 543 - cum reward 18.0\n",
      "episode: 544 - cum reward 26.0\n",
      "0.9998645\n",
      "loss  -0.21683747\n",
      "episode: 545 - cum reward 29.0\n",
      "episode: 546 - cum reward 23.0\n",
      "episode: 547 - cum reward 45.0\n",
      "episode: 548 - cum reward 24.0\n",
      "episode: 549 - cum reward 48.0\n",
      "episode: 550 - cum reward 12.0\n",
      "episode: 551 - cum reward 10.0\n",
      "episode: 552 - cum reward 24.0\n",
      "episode: 553 - cum reward 44.0\n",
      "episode: 554 - cum reward 14.0\n",
      "episode: 555 - cum reward 22.0\n",
      "episode: 556 - cum reward 47.0\n",
      "episode: 557 - cum reward 31.0\n",
      "episode: 558 - cum reward 21.0\n",
      "episode: 559 - cum reward 23.0\n",
      "episode: 560 - cum reward 31.0\n",
      "episode: 561 - cum reward 15.0\n",
      "episode: 562 - cum reward 12.0\n",
      "episode: 563 - cum reward 12.0\n",
      "episode: 564 - cum reward 32.0\n",
      "episode: 565 - cum reward 45.0\n",
      "episode: 566 - cum reward 27.0\n",
      "episode: 567 - cum reward 17.0\n",
      "episode: 568 - cum reward 18.0\n",
      "episode: 569 - cum reward 20.0\n",
      "episode: 570 - cum reward 29.0\n",
      "1.0002285\n",
      "loss  -0.2770602\n",
      "episode: 571 - cum reward 27.0\n",
      "episode: 572 - cum reward 36.0\n",
      "episode: 573 - cum reward 34.0\n",
      "episode: 574 - cum reward 24.0\n",
      "episode: 575 - cum reward 16.0\n",
      "episode: 576 - cum reward 63.0\n",
      "episode: 577 - cum reward 22.0\n",
      "episode: 578 - cum reward 24.0\n",
      "episode: 579 - cum reward 36.0\n",
      "episode: 580 - cum reward 20.0\n",
      "episode: 581 - cum reward 15.0\n",
      "episode: 582 - cum reward 12.0\n",
      "episode: 583 - cum reward 47.0\n",
      "episode: 584 - cum reward 15.0\n",
      "episode: 585 - cum reward 37.0\n",
      "episode: 586 - cum reward 78.0\n",
      "episode: 587 - cum reward 23.0\n",
      "episode: 588 - cum reward 19.0\n",
      "episode: 589 - cum reward 34.0\n",
      "episode: 590 - cum reward 97.0\n",
      "episode: 591 - cum reward 41.0\n",
      "episode: 592 - cum reward 80.0\n",
      "episode: 593 - cum reward 51.0\n",
      "episode: 594 - cum reward 18.0\n",
      "episode: 595 - cum reward 18.0\n",
      "episode: 596 - cum reward 15.0\n",
      "1.0002863\n",
      "loss  -0.2780044\n",
      "episode: 597 - cum reward 70.0\n",
      "episode: 598 - cum reward 49.0\n",
      "episode: 599 - cum reward 11.0\n",
      "episode: 600 - cum reward 11.0\n",
      "episode: 601 - cum reward 12.0\n",
      "episode: 602 - cum reward 13.0\n",
      "episode: 603 - cum reward 13.0\n",
      "episode: 604 - cum reward 35.0\n",
      "episode: 605 - cum reward 39.0\n",
      "episode: 606 - cum reward 15.0\n",
      "episode: 607 - cum reward 20.0\n",
      "episode: 608 - cum reward 13.0\n",
      "episode: 609 - cum reward 15.0\n",
      "episode: 610 - cum reward 29.0\n",
      "episode: 611 - cum reward 27.0\n",
      "episode: 612 - cum reward 24.0\n",
      "episode: 613 - cum reward 9.0\n",
      "episode: 614 - cum reward 18.0\n",
      "episode: 615 - cum reward 17.0\n",
      "episode: 616 - cum reward 19.0\n",
      "episode: 617 - cum reward 14.0\n",
      "episode: 618 - cum reward 20.0\n",
      "episode: 619 - cum reward 13.0\n",
      "episode: 620 - cum reward 10.0\n",
      "episode: 621 - cum reward 19.0\n",
      "episode: 622 - cum reward 9.0\n",
      "0.9995094\n",
      "loss  -0.2577696\n",
      "episode: 623 - cum reward 16.0\n",
      "episode: 624 - cum reward 31.0\n",
      "episode: 625 - cum reward 13.0\n",
      "episode: 626 - cum reward 9.0\n",
      "episode: 627 - cum reward 29.0\n",
      "episode: 628 - cum reward 29.0\n",
      "episode: 629 - cum reward 76.0\n",
      "episode: 630 - cum reward 12.0\n",
      "episode: 631 - cum reward 11.0\n",
      "episode: 632 - cum reward 19.0\n",
      "episode: 633 - cum reward 28.0\n",
      "episode: 634 - cum reward 14.0\n",
      "episode: 635 - cum reward 38.0\n",
      "episode: 636 - cum reward 21.0\n",
      "episode: 637 - cum reward 19.0\n",
      "episode: 638 - cum reward 22.0\n",
      "episode: 639 - cum reward 12.0\n",
      "episode: 640 - cum reward 16.0\n",
      "episode: 641 - cum reward 41.0\n",
      "episode: 642 - cum reward 39.0\n",
      "episode: 643 - cum reward 50.0\n",
      "episode: 644 - cum reward 26.0\n",
      "episode: 645 - cum reward 48.0\n",
      "episode: 646 - cum reward 27.0\n",
      "episode: 647 - cum reward 11.0\n",
      "episode: 648 - cum reward 23.0\n",
      "1.0000904\n",
      "loss  -0.3010132\n",
      "episode: 649 - cum reward 35.0\n",
      "episode: 650 - cum reward 15.0\n",
      "episode: 651 - cum reward 42.0\n",
      "episode: 652 - cum reward 19.0\n",
      "episode: 653 - cum reward 35.0\n",
      "episode: 654 - cum reward 19.0\n",
      "episode: 655 - cum reward 25.0\n",
      "episode: 656 - cum reward 9.0\n",
      "episode: 657 - cum reward 11.0\n",
      "episode: 658 - cum reward 13.0\n",
      "episode: 659 - cum reward 8.0\n",
      "episode: 660 - cum reward 39.0\n",
      "episode: 661 - cum reward 14.0\n",
      "episode: 662 - cum reward 9.0\n",
      "episode: 663 - cum reward 47.0\n",
      "episode: 664 - cum reward 27.0\n",
      "episode: 665 - cum reward 20.0\n",
      "episode: 666 - cum reward 19.0\n",
      "episode: 667 - cum reward 24.0\n",
      "episode: 668 - cum reward 22.0\n",
      "episode: 669 - cum reward 20.0\n",
      "episode: 670 - cum reward 11.0\n",
      "episode: 671 - cum reward 11.0\n",
      "episode: 672 - cum reward 25.0\n",
      "episode: 673 - cum reward 43.0\n",
      "episode: 674 - cum reward 45.0\n",
      "0.99988484\n",
      "loss  -0.3670748\n",
      "episode: 675 - cum reward 31.0\n",
      "episode: 676 - cum reward 21.0\n",
      "episode: 677 - cum reward 27.0\n",
      "episode: 678 - cum reward 29.0\n",
      "episode: 679 - cum reward 40.0\n",
      "episode: 680 - cum reward 40.0\n",
      "episode: 681 - cum reward 18.0\n",
      "episode: 682 - cum reward 39.0\n",
      "episode: 683 - cum reward 49.0\n",
      "episode: 684 - cum reward 25.0\n",
      "episode: 685 - cum reward 13.0\n",
      "episode: 686 - cum reward 19.0\n",
      "episode: 687 - cum reward 39.0\n",
      "episode: 688 - cum reward 16.0\n",
      "episode: 689 - cum reward 20.0\n",
      "episode: 690 - cum reward 13.0\n",
      "episode: 691 - cum reward 17.0\n",
      "episode: 692 - cum reward 17.0\n",
      "episode: 693 - cum reward 23.0\n",
      "episode: 694 - cum reward 17.0\n",
      "episode: 695 - cum reward 15.0\n",
      "episode: 696 - cum reward 18.0\n",
      "episode: 697 - cum reward 8.0\n",
      "episode: 698 - cum reward 15.0\n",
      "episode: 699 - cum reward 25.0\n",
      "episode: 700 - cum reward 70.0\n",
      "0.9999918\n",
      "loss  -0.34066775\n",
      "episode: 701 - cum reward 26.0\n",
      "episode: 702 - cum reward 12.0\n",
      "episode: 703 - cum reward 27.0\n",
      "episode: 704 - cum reward 11.0\n",
      "episode: 705 - cum reward 66.0\n",
      "episode: 706 - cum reward 11.0\n",
      "episode: 707 - cum reward 86.0\n",
      "episode: 708 - cum reward 15.0\n",
      "episode: 709 - cum reward 21.0\n",
      "episode: 710 - cum reward 58.0\n",
      "episode: 711 - cum reward 57.0\n",
      "episode: 712 - cum reward 23.0\n",
      "episode: 713 - cum reward 12.0\n",
      "episode: 714 - cum reward 52.0\n",
      "episode: 715 - cum reward 21.0\n",
      "episode: 716 - cum reward 34.0\n",
      "episode: 717 - cum reward 10.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 718 - cum reward 48.0\n",
      "episode: 719 - cum reward 15.0\n",
      "episode: 720 - cum reward 69.0\n",
      "episode: 721 - cum reward 26.0\n",
      "episode: 722 - cum reward 43.0\n",
      "episode: 723 - cum reward 15.0\n",
      "episode: 724 - cum reward 18.0\n",
      "episode: 725 - cum reward 20.0\n",
      "episode: 726 - cum reward 14.0\n",
      "1.0000684\n",
      "loss  -0.35448384\n",
      "episode: 727 - cum reward 11.0\n",
      "episode: 728 - cum reward 21.0\n",
      "episode: 729 - cum reward 24.0\n",
      "episode: 730 - cum reward 19.0\n",
      "episode: 731 - cum reward 15.0\n",
      "episode: 732 - cum reward 11.0\n",
      "episode: 733 - cum reward 32.0\n",
      "episode: 734 - cum reward 25.0\n",
      "episode: 735 - cum reward 12.0\n",
      "episode: 736 - cum reward 11.0\n",
      "episode: 737 - cum reward 35.0\n",
      "episode: 738 - cum reward 28.0\n",
      "episode: 739 - cum reward 40.0\n",
      "episode: 740 - cum reward 11.0\n",
      "episode: 741 - cum reward 25.0\n",
      "episode: 742 - cum reward 11.0\n",
      "episode: 743 - cum reward 17.0\n",
      "episode: 744 - cum reward 15.0\n",
      "episode: 745 - cum reward 26.0\n",
      "episode: 746 - cum reward 14.0\n",
      "episode: 747 - cum reward 24.0\n",
      "episode: 748 - cum reward 45.0\n",
      "episode: 749 - cum reward 12.0\n",
      "episode: 750 - cum reward 44.0\n",
      "episode: 751 - cum reward 8.0\n",
      "episode: 752 - cum reward 17.0\n",
      "0.9993395\n",
      "loss  -0.3717376\n",
      "episode: 753 - cum reward 20.0\n",
      "episode: 754 - cum reward 27.0\n",
      "episode: 755 - cum reward 14.0\n",
      "episode: 756 - cum reward 16.0\n",
      "episode: 757 - cum reward 16.0\n",
      "episode: 758 - cum reward 20.0\n",
      "episode: 759 - cum reward 17.0\n",
      "episode: 760 - cum reward 20.0\n",
      "episode: 761 - cum reward 9.0\n",
      "episode: 762 - cum reward 26.0\n",
      "episode: 763 - cum reward 14.0\n",
      "episode: 764 - cum reward 43.0\n",
      "episode: 765 - cum reward 37.0\n",
      "episode: 766 - cum reward 15.0\n",
      "episode: 767 - cum reward 26.0\n",
      "episode: 768 - cum reward 21.0\n",
      "episode: 769 - cum reward 15.0\n",
      "episode: 770 - cum reward 50.0\n",
      "episode: 771 - cum reward 15.0\n",
      "episode: 772 - cum reward 17.0\n",
      "episode: 773 - cum reward 26.0\n",
      "episode: 774 - cum reward 33.0\n",
      "episode: 775 - cum reward 31.0\n",
      "episode: 776 - cum reward 20.0\n",
      "episode: 777 - cum reward 16.0\n",
      "episode: 778 - cum reward 22.0\n",
      "0.99952424\n",
      "loss  -0.36101997\n",
      "episode: 779 - cum reward 30.0\n",
      "episode: 780 - cum reward 17.0\n",
      "episode: 781 - cum reward 9.0\n",
      "episode: 782 - cum reward 24.0\n",
      "episode: 783 - cum reward 41.0\n",
      "episode: 784 - cum reward 43.0\n",
      "episode: 785 - cum reward 42.0\n",
      "episode: 786 - cum reward 48.0\n",
      "episode: 787 - cum reward 11.0\n",
      "episode: 788 - cum reward 13.0\n",
      "episode: 789 - cum reward 18.0\n",
      "episode: 790 - cum reward 12.0\n",
      "episode: 791 - cum reward 16.0\n",
      "episode: 792 - cum reward 37.0\n",
      "episode: 793 - cum reward 18.0\n",
      "episode: 794 - cum reward 22.0\n",
      "episode: 795 - cum reward 49.0\n",
      "episode: 796 - cum reward 19.0\n",
      "episode: 797 - cum reward 20.0\n",
      "episode: 798 - cum reward 20.0\n",
      "episode: 799 - cum reward 18.0\n",
      "episode: 800 - cum reward 20.0\n",
      "episode: 801 - cum reward 21.0\n",
      "episode: 802 - cum reward 13.0\n",
      "episode: 803 - cum reward 16.0\n",
      "episode: 804 - cum reward 34.0\n",
      "0.99919647\n",
      "loss  -0.36218408\n",
      "episode: 805 - cum reward 19.0\n",
      "episode: 806 - cum reward 19.0\n",
      "episode: 807 - cum reward 14.0\n",
      "episode: 808 - cum reward 17.0\n",
      "episode: 809 - cum reward 14.0\n",
      "episode: 810 - cum reward 14.0\n",
      "episode: 811 - cum reward 20.0\n",
      "episode: 812 - cum reward 8.0\n",
      "episode: 813 - cum reward 13.0\n",
      "episode: 814 - cum reward 13.0\n",
      "episode: 815 - cum reward 13.0\n",
      "episode: 816 - cum reward 39.0\n",
      "episode: 817 - cum reward 13.0\n",
      "episode: 818 - cum reward 13.0\n",
      "episode: 819 - cum reward 13.0\n",
      "episode: 820 - cum reward 27.0\n",
      "episode: 821 - cum reward 16.0\n",
      "episode: 822 - cum reward 24.0\n",
      "episode: 823 - cum reward 10.0\n",
      "episode: 824 - cum reward 28.0\n",
      "episode: 825 - cum reward 17.0\n",
      "episode: 826 - cum reward 19.0\n",
      "episode: 827 - cum reward 20.0\n",
      "episode: 828 - cum reward 67.0\n",
      "episode: 829 - cum reward 13.0\n",
      "episode: 830 - cum reward 26.0\n",
      "0.9990265\n",
      "loss  -0.38562125\n",
      "episode: 831 - cum reward 16.0\n",
      "episode: 832 - cum reward 17.0\n",
      "episode: 833 - cum reward 21.0\n",
      "episode: 834 - cum reward 21.0\n",
      "episode: 835 - cum reward 10.0\n",
      "episode: 836 - cum reward 13.0\n",
      "episode: 837 - cum reward 24.0\n",
      "episode: 838 - cum reward 45.0\n",
      "episode: 839 - cum reward 12.0\n",
      "episode: 840 - cum reward 14.0\n",
      "episode: 841 - cum reward 21.0\n",
      "episode: 842 - cum reward 47.0\n",
      "episode: 843 - cum reward 10.0\n",
      "episode: 844 - cum reward 11.0\n",
      "episode: 845 - cum reward 11.0\n",
      "episode: 846 - cum reward 21.0\n",
      "episode: 847 - cum reward 13.0\n",
      "episode: 848 - cum reward 28.0\n",
      "episode: 849 - cum reward 18.0\n",
      "episode: 850 - cum reward 23.0\n",
      "episode: 851 - cum reward 26.0\n",
      "episode: 852 - cum reward 20.0\n",
      "episode: 853 - cum reward 18.0\n",
      "episode: 854 - cum reward 16.0\n",
      "episode: 855 - cum reward 34.0\n",
      "episode: 856 - cum reward 13.0\n",
      "0.99867636\n",
      "loss  -0.39071918\n",
      "episode: 857 - cum reward 10.0\n",
      "episode: 858 - cum reward 25.0\n",
      "episode: 859 - cum reward 20.0\n",
      "episode: 860 - cum reward 25.0\n",
      "episode: 861 - cum reward 15.0\n",
      "episode: 862 - cum reward 12.0\n",
      "episode: 863 - cum reward 20.0\n",
      "episode: 864 - cum reward 9.0\n",
      "episode: 865 - cum reward 22.0\n",
      "episode: 866 - cum reward 10.0\n",
      "episode: 867 - cum reward 8.0\n",
      "episode: 868 - cum reward 13.0\n",
      "episode: 869 - cum reward 34.0\n",
      "episode: 870 - cum reward 20.0\n",
      "episode: 871 - cum reward 19.0\n",
      "episode: 872 - cum reward 17.0\n",
      "episode: 873 - cum reward 15.0\n",
      "episode: 874 - cum reward 11.0\n",
      "episode: 875 - cum reward 22.0\n",
      "episode: 876 - cum reward 22.0\n",
      "episode: 877 - cum reward 27.0\n",
      "episode: 878 - cum reward 16.0\n",
      "episode: 879 - cum reward 9.0\n",
      "episode: 880 - cum reward 21.0\n",
      "episode: 881 - cum reward 9.0\n",
      "episode: 882 - cum reward 23.0\n",
      "0.998619\n",
      "loss  -0.34522715\n",
      "episode: 883 - cum reward 17.0\n",
      "episode: 884 - cum reward 31.0\n",
      "episode: 885 - cum reward 9.0\n",
      "episode: 886 - cum reward 12.0\n",
      "episode: 887 - cum reward 32.0\n",
      "episode: 888 - cum reward 9.0\n",
      "episode: 889 - cum reward 25.0\n",
      "episode: 890 - cum reward 31.0\n",
      "episode: 891 - cum reward 15.0\n",
      "episode: 892 - cum reward 11.0\n",
      "episode: 893 - cum reward 17.0\n",
      "episode: 894 - cum reward 24.0\n",
      "episode: 895 - cum reward 14.0\n",
      "episode: 896 - cum reward 13.0\n",
      "episode: 897 - cum reward 19.0\n",
      "episode: 898 - cum reward 31.0\n",
      "episode: 899 - cum reward 32.0\n",
      "episode: 900 - cum reward 13.0\n",
      "episode: 901 - cum reward 12.0\n",
      "episode: 902 - cum reward 11.0\n",
      "episode: 903 - cum reward 13.0\n",
      "episode: 904 - cum reward 24.0\n",
      "episode: 905 - cum reward 14.0\n",
      "episode: 906 - cum reward 14.0\n",
      "episode: 907 - cum reward 23.0\n",
      "episode: 908 - cum reward 17.0\n",
      "0.99866384\n",
      "loss  -0.50279546\n",
      "episode: 909 - cum reward 40.0\n",
      "episode: 910 - cum reward 13.0\n",
      "episode: 911 - cum reward 12.0\n",
      "episode: 912 - cum reward 13.0\n",
      "episode: 913 - cum reward 22.0\n",
      "episode: 914 - cum reward 24.0\n",
      "episode: 915 - cum reward 10.0\n",
      "episode: 916 - cum reward 14.0\n",
      "episode: 917 - cum reward 16.0\n",
      "episode: 918 - cum reward 33.0\n",
      "episode: 919 - cum reward 11.0\n",
      "episode: 920 - cum reward 27.0\n",
      "episode: 921 - cum reward 12.0\n",
      "episode: 922 - cum reward 8.0\n",
      "episode: 923 - cum reward 14.0\n",
      "episode: 924 - cum reward 20.0\n",
      "episode: 925 - cum reward 9.0\n",
      "episode: 926 - cum reward 18.0\n",
      "episode: 927 - cum reward 11.0\n",
      "episode: 928 - cum reward 19.0\n",
      "episode: 929 - cum reward 23.0\n",
      "episode: 930 - cum reward 42.0\n",
      "episode: 931 - cum reward 11.0\n",
      "episode: 932 - cum reward 17.0\n",
      "episode: 933 - cum reward 10.0\n",
      "episode: 934 - cum reward 14.0\n",
      "0.9982803\n",
      "loss  -0.36424032\n",
      "episode: 935 - cum reward 10.0\n",
      "episode: 936 - cum reward 16.0\n",
      "episode: 937 - cum reward 13.0\n",
      "episode: 938 - cum reward 12.0\n",
      "episode: 939 - cum reward 14.0\n",
      "episode: 940 - cum reward 9.0\n",
      "episode: 941 - cum reward 12.0\n",
      "episode: 942 - cum reward 28.0\n",
      "episode: 943 - cum reward 17.0\n",
      "episode: 944 - cum reward 11.0\n",
      "episode: 945 - cum reward 17.0\n",
      "episode: 946 - cum reward 19.0\n",
      "episode: 947 - cum reward 25.0\n",
      "episode: 948 - cum reward 15.0\n",
      "episode: 949 - cum reward 25.0\n",
      "episode: 950 - cum reward 13.0\n",
      "episode: 951 - cum reward 11.0\n",
      "episode: 952 - cum reward 11.0\n",
      "episode: 953 - cum reward 10.0\n",
      "episode: 954 - cum reward 64.0\n",
      "episode: 955 - cum reward 13.0\n",
      "episode: 956 - cum reward 15.0\n",
      "episode: 957 - cum reward 12.0\n",
      "episode: 958 - cum reward 12.0\n",
      "episode: 959 - cum reward 10.0\n",
      "episode: 960 - cum reward 22.0\n",
      "0.9987072\n",
      "loss  -0.51592606\n",
      "episode: 961 - cum reward 19.0\n",
      "episode: 962 - cum reward 22.0\n",
      "episode: 963 - cum reward 18.0\n",
      "episode: 964 - cum reward 11.0\n",
      "episode: 965 - cum reward 15.0\n",
      "episode: 966 - cum reward 16.0\n",
      "episode: 967 - cum reward 17.0\n",
      "episode: 968 - cum reward 13.0\n",
      "episode: 969 - cum reward 14.0\n",
      "episode: 970 - cum reward 29.0\n",
      "episode: 971 - cum reward 16.0\n",
      "episode: 972 - cum reward 20.0\n",
      "episode: 973 - cum reward 16.0\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 974 - cum reward 26.0\n",
      "episode: 975 - cum reward 17.0\n",
      "episode: 976 - cum reward 17.0\n",
      "episode: 977 - cum reward 8.0\n",
      "episode: 978 - cum reward 19.0\n",
      "episode: 979 - cum reward 9.0\n",
      "episode: 980 - cum reward 19.0\n",
      "episode: 981 - cum reward 25.0\n",
      "episode: 982 - cum reward 20.0\n",
      "episode: 983 - cum reward 22.0\n",
      "episode: 984 - cum reward 19.0\n",
      "episode: 985 - cum reward 17.0\n",
      "episode: 986 - cum reward 10.0\n",
      "0.99801993\n",
      "loss  -0.38952497\n",
      "episode: 987 - cum reward 16.0\n",
      "episode: 988 - cum reward 18.0\n",
      "episode: 989 - cum reward 12.0\n",
      "episode: 990 - cum reward 20.0\n",
      "episode: 991 - cum reward 12.0\n",
      "episode: 992 - cum reward 12.0\n",
      "episode: 993 - cum reward 20.0\n",
      "episode: 994 - cum reward 10.0\n",
      "episode: 995 - cum reward 18.0\n",
      "episode: 996 - cum reward 11.0\n",
      "episode: 997 - cum reward 11.0\n",
      "episode: 998 - cum reward 29.0\n",
      "episode: 999 - cum reward 25.0\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Text(0.5, 1.0, 'cumulative reward per episode - rand_agent')"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAXcAAAEICAYAAACktLTqAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8li6FKAAAgAElEQVR4nO2dd5gW1fXHv+fdXXbpHaS6IChgQRABewErRo0tGntMMPmZaIomaDQaY0FjYjQao9EYS4yFEFFRLFQ7AoJ06Sx9gYVdyrLlvb8/ZuZ978w75U55657P8+yz77R7z7Qz55577rkkhADDMAxTWMSyLQDDMAwTPazcGYZhChBW7gzDMAUIK3eGYZgChJU7wzBMAcLKnWEYpgBh5Z5jENF1RPRJiOPfI6Jro5Qp1yEiQUT9si2HX4hoMRGdGnGZ/yKi+6IsM1OEffYZM8XZFoAJDhHdA6CfEOIqY50Q4pzsScT4QQhxeLZlYNQhorUAfiiE+CjbsqjAljvjGyLKilGQrXr1uomI+H3R4euR+/DNkSCiXkQ0kYgqiWgHET2hr7+HiF6W9ivXXQHF+vIMIrqPiD4joj1E9DYRdSSifxNRNRF9RUTldsdKx//QQabHiKhCL2cuEZ2krz8bwB0AvqfXuUAui4hKiWgXER0hldWZiPYTURd9+Twimq/v9xkRHeVybQQR3UREKwCs0NcNIKIPiWgnES0nosv09X30MmP68j+IaJtU1ktE9HP99/VEtJSIaohoNRHdKO13KhFtIKLfENEWAM/r628jos1EtImIfuBxT2cQ0YNENFu/hpOIqIO0faR+7ruIaIHsJtGPvZ+IPgWwD0Bfm/K7E9F/9WdmDRHdLG27h4gmENFr+vnNI6LB0va1RDRa/z2ciOboMm4loj9L+51Pmgtnly7TQGnbEL3cGiJ6DUCZRT7le+yF3fVQvH+/IqJt+j27XtrekYje0s95NoBDFOWwfSf0bc2J6AUiqtLl+jURbZC2e92v14noRf18FhPRMH3bSwB6A3ibtPft10GvY8YQQvCfloKhCMACAI8CaAntJTlR33YPgJelfcsBCADF+vIMACuhPZxtASwB8C2A0dBcXy8CeN7uWOn4H+q/rwPwibTtKgAd9XJ+BWALgDI7uWzK+ieA+6VtNwGYov8eAmAbgBH6uV8LYC2AUofrIwB8CKADgOb6NaoAcL0u2xAA2wEM0vdfD+AY/fdyAKsBDJS2DdF/j9GvGwE4BZrSGKpvOxVAA4CHAJTq9Z4NYCuAI3QZXtFl6+cg9wwAG6X9/2tcMwA9AOwAcC40Q+cMfbmzdOx6AIfr51hiKTsGYC6A3wFoBk35rwZwlnR/6gFcAqAEwK0A1hjl6Nd7tP77cwBX679bARip/z4UwF5dthIAv4b2rDXT/9YB+IW+7RK9vvuC3GOFdyTleijev3v1fc/Vt7fXt78K4HX9vhyh36dPFORweyfGA5gJoD2AngC+AbDBx/2q1eUsAvAggC+kehP3Kx/+si5ArvwBOA5AJSSlK227B97K/bfS9j8BeE9a/g6A+XbHSsfbKncbWaoADLaTy6as0QBWSds+BXCN/vspAH+wHLscwCkO9QoAp0vL3wPwsWWfpwHcrf9+CcAvARykl/swgB8D6ANgF4CYQz1vArhF/30qgDrjxdXX/RPAeGn5UHgrd3n/QXqZRQB+A+Aly/7vA7hWOvZel3sxAsB6y7rbkfyQ32NRDjEAmwGcpC8nlAWAWQB+D6CTpby7ALxuKWOjfm1OBrAJAEnbP0NSufu6xwrviOv1cLh/+2F+1rcBGKlf/3oAA6RtD0BBuXu8EwllrS//EEnlrnK/PrI8K/ul5cT9yoc/dssk6QVgnRCiIeDxW6Xf+22WWwUplIhu1ZuXu4loF7SWQSfFw6cDaEFEI0hzCx0N4H/6toMB/Epvru/Sy+4FoLtLeRXS74MBjLAcfyU0ZQ5o1tOp0BTQLGiK4RT972MhRFw/v3OI6AvSXDu7oFlN8vlVCiFqpeXuFjnWeV+GlP1L9DoOBnCp5RxOBNDN4VgrBwPobjn+DgBd7Y7Xz3kD7K/xDdA+VMtIc+Odp6/vLp+jXkYFtFZHdwAbha55pPOT5VO6x0R0h+5u2ENEf3c5Z9P1ULh/Oyzv1D5o70JnaJa333vp9U5Ynw/rM+t1v7ZYZC2jLPb1hCEvhU4TFQB6E1GxjYLfC6CFtHwQgrNX/98CQLVbebov8dcARgFYLISIE1EVtCYwoFmsjgghGonodQBXQPvYvCOEqNE3V0Bz2dzvQ3a5vgoAM4UQZzjsOxPAH6Eps5kAPgHwd2jN3pn6+ZVCc5NcA2CSEKKeiN6Uzs9aJ6BZvr2k5d4Kclv3r4fmQqqAZrn/yOVYt2tcAWCNEKK/St2k9UH0hGZtmysRYgWAK/R9LgIwgYg66vseKZVBepkbddl6EBFJCr43gFWSfEr3WAjxADTL2XNXSRaV++dEJTSXTS8AyyTZXVF4JzZDu8ZL9GX53qvcLzfyKoUuW+5JZkN7MMYTUUsiKiOiE/Rt8wGcTES9iagttKZcIIQQldBezKuIqIi0DkGnjqTW0F6ASgDFRPQ7AG2k7VsBlJN71MIr0FwoV+q/Df4B4Me6VU/6OY8hotaKp/IOgEOJ6GoiKtH/jiW9s09XVvuh+UdnCiGqdXkvhq7cofk9S/XzayCicwCc6VHv6wCuI6JBRNQCwN0Ksl4l7X8vgAlCiEYALwP4DhGdpd+LMr0TsKfiNZgNoIa0Dt/mehlHENGx0j7HENFFuvX3cwAHAHxhLYiIriKizrplvktfHdfPdwwRjSKiEmg+5gPQ3C+fQ3s+btav/0UAhkvFhr3HXgS5fwA0wwPARAD3EFELIhoErU/AC6934nUAtxNReyLqAeCn0jaV++XGVth0qucqrNx19IftOwD6Qes02gBNKUII8SGA16B1zsyFptjC8CMAt0HrvDsc2otqx/sApkDrnF0HzeqVm5lv6P93ENE8uwKEEF9Cay10B/CetH6OLscT0HyWK6H5+5XQWwBnArgcmnW5BcmOT4OZ0JrlFdIyAZgnlXEztBeyCsD3AbzlUe97AP4CYJou8zQFcV8C8C9dxjK9TuhyXQCtaV4J7dreBsX3Qn9mzoPm7loDrTXwLDQ3gcEkaM9RFYCrAVwkhKi3Ke5sAIuJaA+AxwBcLoTYL4RYDu0D+Ve9/O8A+I4Qok4IUQfNyr8OwE69nomSfKHusRdB7p+Fn0Jz0WyBdn+eVzjG6524F9q7uwbARwAmQPsYqt4vNx4EcKfu0rlV8ZisQWZ3HcMUFkQ0A1qn87NZqPseWAaZMZmFiH4C7UN5SrZlyTRsuTMMUzAQUTciOoGIYkR0GDQ31v+8jitEuEOVYZicQu80fc9umxDCK+qsGbSQXCPk9lUAf4tUwDyB3TIMwzAFCLtlGIZhChBPtwwR/RNaD/M2IcQR+roO0KJHyqGN2rpMCFGlx+A+huQw4+uEELZRHDKdOnUS5eXlAU+BYRimaTJ37tztQojOdttUfO7/ghZK9aK0bhyAqUKI8UQ0Tl/+DYBzAPTX/0ZAG/48wquC8vJyzJkzR0EUhmEYxoCIHEf1erplhBCzoMXQylwA4AX99wsALpTWvyg0vgDQjoi6gWEYhskoQX3uXYUQm/XfW5DMzdAD5gEFG/R1DMMwTAYJ3aGq57TwHXJDRGNJy189p7KyMqwYDMMwjERQ5b7VcLfo/42JGDbCnKinp74uBSHEM0KIYUKIYZ072/YHMAzDMAEJqtzfQjLJz7XQ8mcY66/RkxSNBLBbct8wDMMwGUIlFPI/0PJydyJtuqq7oc128joR3QAtec9l+u7vQguDXAktFPL6lAIZhmGYtOOp3IUQVzhsGmWzr4A2lRvDMAyTRXiEKsPkCEs2VWPuuqpsi8EUCJw4jGFyhHMf/xgAsHb8mCxLwhQCbLkzDMMUIKzcGYZhChBW7gzDMAUIK3eGYZgChJU7wzBMAcLKnWEYpgBh5c4wDFOAsHJnGIYpQFi5MwzDFCCs3BmGYQoQVu4MwzAFCCt3hmGYAoSVO8MwTAHCyp1hGKYAYeXOMAxTgLByZxiGKUBYuTMMwxQgrNwZhmEKEFbuDMMwBQgrd4ZhmAKElTvDMEwBwsqdYRimAGHlzjAMU4CwcmcYJoX9dY0oHzcZT89clW1RmICwcmcYJoWqfXUAgH99tja7gjCBYeXOMAxTgLByZxiGKUBYuTMMwxQgrNwZhmEKEFbuDMM4IkS2JWCCwsqdYRimAGHlzjAMU4CwcmcYhilAWLkzDMMUIKGUOxH9gogWE9EiIvoPEZURUR8i+pKIVhLRa0TULCphGYZhGDUCK3ci6gHgZgDDhBBHACgCcDmAhwA8KoToB6AKwA1RCMowDMOoE9YtUwygOREVA2gBYDOA0wFM0Le/AODCkHUwTJNizfa92RaBKQACK3chxEYAjwBYD02p7wYwF8AuIUSDvtsGAD3sjieisUQ0h4jmVFZWBhWDYQqO//v3vGyLwBQAYdwy7QFcAKAPgO4AWgI4W/V4IcQzQohhQohhnTt3DioGwzAMY0MYt8xoAGuEEJVCiHoAEwGcAKCd7qYBgJ4ANoaUkWEYhvFJGOW+HsBIImpBRARgFIAlAKYDuETf51oAk8KJyDBqbN69H1V767ItRiAqdu7LtghMgRHG5/4ltI7TeQAW6mU9A+A3AH5JRCsBdATwXARyMownxz04DSMenJptMQJx0sPTsy0CU2AUe+/ijBDibgB3W1avBjA8TLkME5S6hni2RWCYnIBHqDIMwxQgrNwZJscQOZBnlyjbEjBhYeXONBle/HwtbnqlcGLIb5/4DZ6cvjLbYjA5Cit3psnwu0mLMfmbzdkWIzL+M7sCf3x/ebbFYHIUVu4MwzAFCCt3hmGYAoSVO8MwTAHCyp1hGKYAYeXOMAxTgLByZ5gcIwfC3JkCgJU7wzAp8Acm/2HlzjAMU4CwcmcYJgU23PMfVu4MwzAFCCt3hskxRA7YzbmQvIwJByt3hskx1m7fh4+WbM1K3UIIvPzFOhzgvPh5Dyt3hskx6hrj+OGLc7JS95RFW3Dnm4vwpw84IVm+w8qdYZgENQcaAAA7beaiXb6lhme6yiNYuTMM48nGXftx1l9m4Q/vLMm2KIwirNwZhknB2p9apVvyc9dVZUEaJgis3JmC4/NVO7ItQt7jFCvDMTT5Ayt3puCYsXxbtkXwBYcdMumAlTvDZBnW7Wrs2HMAJz08DSu31WRblLyAlTtTeJD7ZraUFbBcIvK4ppngo6VbUbFzP56ZtTrbouQFrNyZwsNDd+eabs8xcQAAs9fuzLYITEhYuTNNjniOafd8aknkk6xNHVbuTMHhpX5yTT3lmjy5Cn9X/MHKnWly5J7l7rQ+t+QEAMoF5zujBCt3psmRazrTKQtkrskJ5OYHJxeoqa3HvrqGbIthgpU70+TINf3kJE82WhhOdjl5hSBlkFySxeDIez7AyAemZlsME6zcmSZHLuRLVyGeBTHz48rkJtW1bLkzEmNfnIOLn/os22I0KQylOevbSpSPm4xtNbVZkePQ376HJ6atcNyeLx8hJjdh5Z5lPliylZMxRYyXX9jY/sJnawEACyp2p1skW+oa43jkg29dOlQzKw9TWLByZyJn4rwNeP7TNdkWwxHDcs+VwA8nCz0Xonq49ZC/FGdbAKbw+OXrCwAA15/QJ8uSOJBj+ootdzX4cvgjlOVORO2IaAIRLSOipUR0HBF1IKIPiWiF/r99VMIyTZtt1bX439cbQpezcdd+07Lhpnl7waaUbZnASWnlguVukCutHEadsG6ZxwBMEUIMADAYwFIA4wBMFUL0BzBVX2aY0Fzzz9n4xWsLsGtf6hRwfrj070YHtllj/ew/X+Oiv30aquwgOPURZCNaJpfh74s/Ait3ImoL4GQAzwGAEKJOCLELwAUAXtB3ewHAhWGFZBgA2FqtRbV4KT0vg3dvXWPKXKCGgt1afQCANodoxc59wQSNClbuJvhy+COM5d4HQCWA54noayJ6lohaAugqhNis77MFQFe7g4loLBHNIaI5lZWVIcRgGP/c8/bixG+B1A/CcQ9OxUkPT8+ILPnglskl2EWkRhjlXgxgKICnhBBDAOyFxQUjNHPI9gkVQjwjhBgmhBjWuXPnEGIw+YwQAvEI/A+ya0Pl5Z+3rsq0n1WCAxbLPp3k0gjVfIAvixphlPsGABuEEF/qyxOgKfutRNQNAPT/+TXnGZNRnp61Gn3veBfVtfWe+xrvtJ3ull/4IC9/VnOmOEXLZFYKJVix5g+BlbsQYguACiI6TF81CsASAG8BuFZfdy2ASaEkZAqaV2evBwDs2OPdSWooFjvLPKzOyabOyuU4d4NccoXkkiy5TNg4958B+DcRNQOwGsD10D4YrxPRDQDWAbgsZB1MAROV+gpjeQuReYtUljeX49y3Vh/A1+ur0LxZEQAe1JRPhFLuQoj5AIbZbBoVplym6aFijLkpcOHwW7XOTFvJJjeSwj6Zwu4+3PnmIvzpssEZl4UJB6cfYDLCgYZGPPjuUuw5kJ7Meblg5UZNNtwydjWu35EMCc3FdLuMPazcmYzw+lcVeHrWajw+1T4Loooac9snnLtAZN4tI/92HMSUG1+sGumDzG6Z/IGVO5MR6hs1pWAdPOQLXa/Y6bwgetAcCpks4IPFW1L2Xbt9LxZvii57pMnn7rhPZNWFJhcs9ly6HvkAJw5jcgI/qiMd77isOMa+NDdl+6mPzAAArB0/Jpr6HOp2kolh/MKWexMjHhfYVp2dySmiws6NESbOXQgOhTTIvn3ORAUr9ybGUzNXYfgDU02dZPmGncoL4guWXQ2ZHsRkqi6HBjE5uohyyNfOce5qsHJvYsz8Vsvjs2l35lPb2uFHp7p2qEobg7z8mVZdsrLMh9wyOSQKowgr9zzkq7U7UT5ucjDrW39J//zhtzj0t+9FK5gLXgpX3v7I+8ttZTOsa9sOVdN+/mSzSxyWblTqy2pKBAdyUCTGAVbuecgbcyoAAJ+v3h64jNlrdqKuMXPJsfzwxPSVrrLZuQiCKELTByebqWVyqEPV6RvMSj3/YOXO5A0i5YfNNgTzD2fTp+zcoZphQVzIJZ87owYr9zwmkDWVY51RwRSxzbqQuiebbplcSvmbyyqcPzD+YOVuYcLcDXkfKpiPqAyScdV1Pt/7ZVtqsL++MVFuLnao5pIrZFXl3myLwPiElbtE1d463PrGAlzzz9nZFkWJQCFhWVYYobI3wq1D1X+5M5YnZwDLZudlrqcfAICb//N1tkXIiVGy+QQrd4kG3clZWXMgy5JkjlyMyPDCvkM1db8FFbtMszwd1rW1S5mZRcUtk0sdqga5ONiLsYeVuw1NaZBEphUIOVxcPy+uVygkAHy+agcuePJTPPvJas86RJYThznvw8qMCQ4rdxvy0JgNTK6GQ9rhdl+sk19s3KUN0lq6uQaNcWd3TuKYDCtSlRaT32gZIUTiXIOSrqsQj0czVy6jDit3iaZksRsMuGsKVlXuybYYah2qlv922xLLuvL839cbccgd7zoe51hABokqWubnr81PnGuu0feOd3H5P77IthhNClbuEk3JYpdZurk62yL4dMuo+dxVjjOOzXy0jPzbWS4/TJq/KbhAOum0b2av2RlRSU3QCgsAK3cmUBTCZ6u244Tx07C/rhGPfvgtfvJyappcO9wUrCoq0TIq1n3Q+qMgF+Lc567biZEPTEVNbb3yMenogK/YuQ9D//Chj3QaTdQK8wkrd4mm6JYBgFiA837g3aXYuGs/Vm7bg8emrsB7i1InuJCJZFh72Dj3HPK5y9VlK879j+8vx5bqWizcEN0kJEH477wN2Lm3DhPmbciqHIUGK3fGMYIlk4SNHpG3TFu2De98s1m5jrCJw7ZV1+KhKctMHYZ1DXE88O5S7N7vbRUHsYbdjlFV1oa48v3P5CfumVmrsGxLEJdg9p/XfICVu4TxvjS1Rl+gFLkRXyQ/Cs5rmr31O/dh1reVNvuofRz88qs3FuCpGaswd31VYt2b8zfimVmr8cj7yx3q867RVV6Xw7/zxCeeZcvlB2m5hUUIgQfeXYbzn/i0yfZ1pZu8V+7bqmvxxeodkZRlvHA799ahVh+aLrNia01OdD6GwU6p5IIdZJ8vxuJHT3x8bTpUVZSl27YQGmZfXeqz0qDPGdsQj2PG8m2otvi15eoWbrS3tKPSebv21dl+7AzLPSZp90w9C8acuvVSKG6Qur9auxObc2Ruglwj75X7mL9+gsufiSbESn7h7nlrccr2Mx6dhXMe+ziSunKJWADTPSpPjnvsuvp6tWgZp/XhBjEZseUxk3vDGO1ch+ue/wo/fcU8fF+u7pZX5/uS13q8Fz96cQ6u+efslA9MPIuWu6HUS2JJFRTkFlz6988x6k8zI5KqsMh75R5FqoCt1bVojIuUpn1TQVVRNzTGQydVi6Lz0G8kTHKf9LT/Dau/yEZLHmjQrPpV2/yPJXDtY/BxwVbqddc3mAes2fncvWWKBkO5FxeR8vPndMp2LSemAJR7WLZV12LEA1Pxx/eXN9nh3qov1z1vL8ZwPXTOr6XrrUDsXC1ee0jbFAQK0kpQodHGAhYeilNJOUdkuTuWIVJbHJnCGBldFKMAz1IaBCpAmrxy376nDgAwY/m20PNw5gN2Me2qltuHS7YCAPYe8G8peSkze1eL1ecuHMsK45YBwln1htvYTkkaa1LORaHcqEwN4/5ayzPcMkVZeNiNPomSoqQKUpWCO2DVaPLKXSYXn5kxj3+Max1SEP/mvwsx0WdssJ0Si9LnPvCuKe7H+ajDLXQxavwojFnfVqJ83GRs0vPXGIr7vL9+gjrd9XHnm4sAJK15AeCzldtRPm4yKnbuC/8x8iFv8gOjBQuUj5uMKYs2I65/lKLS7Q+8u1R5Xt6EWyagw39bdS3Kx00OdGxTgZW7RC4mNlq8qRozbSIdDP4+c1XoOgKlhXe4VPttooyUyvNRR/AOVeed/IwGffWr9QCAr9fvAgBTsq69BxpM+8qtotf0uW/nrquCCu5x/eryUuIDI7B4kxaZ89IX6xLnHJVyf2bWauVEdIkO1aKY7481kXOEEZOElbvOsi01uPedJYnlT1fuwAeL3UddZpqFG3bjlle/hvz+RNFE9Wu5/+BfX2GH7s5KZ9TMks3VuOmVeWjQT9jY5e63FqUeHzYUUkFGJxpdbsK0Zdu08i27qMW5B9uWSvIm1dRqH5/WpSXJ0NKIbRo3Y8SgrkGrtLioQP2fOQArdwnDp2ww9iW1fCmZ4saX5mDS/E3YUh1tXK/faIUlm6uxRY+aiUox2Cm7n74yD5O/2Yy1O/aa6vp0Zeq4hnChkOHOQ6WvJuX8wvWnBkMAe3Tl3qqsOGG5R5YCQsfJjSgju2WibDkySYqzLUA+IoTIypB943mWX4convFMnomXvHYDUkIGlkj7RGu7G+Wp5FC3fkCU5I1Ig5Hk9zdi3VuVSsrdx7mv3u5vLtWv11dhj8VVBSTdKsUx/24ZIDf7x3INVu4BqG8UaFacveak/F3xqwDsdg/zoVI91KsOQ65rnktafcYhKl0hKj7ztIVCqih36bdyS8ltmw95E526Ajigd/iWlRQlyvDb1XSgoRGlxUVK+373b5/Zrjc6nGW3TKFGqGWLgnHLbN69HxUZGniUrdmLVKaXC0KYlyqq5vGqyj2o2luHHXvrUutQOEuVznDn6BvnGtTi54X023N39f1ChG4uqNiVcH0YLT3rMUm3jCS/glhRopJYzUCWLRtz/36zYVciGiofKBjlftyD03DSw9MzUldO3eAInvFcMJh++srXGP3nmeYh/D7OrUHRNeJ3m4oMcoeqUwsiSIdqGC548lPcP3mpqwzGJTNdugzrzF7tWwSyEDKt2ldu24Pzn/gUD7y71HvnHKFglHsmaciA5V7fGE9JIWufMMsfdlZ6LqT8BYAde+tQZPNEKilYJd+C/9BCt1INi9gUvaRYt1o/gktpCscbYY9J95YwhUUmP0ThXXt+ObRrKwDAYQe1Vj4mm0/pjj1ampMlm/IncWBonzsRFQGYA2CjEOI8IuoD4FUAHQHMBXC1ECK1rZ3H1GcgHr7/b9/DmYO6JpYPvTM5OMSUfzvLYQNhvwtW+Ytjqdo9KuUeJFpGk8+rv8C/5a6Cn8Rha106OuVBTInOeOmc5UuXqRQcJneM4kNkdstEK4+fuvOFKCz3WwDIbZWHADwqhOgHoArADRHUoUwmBiJlwnIHgA8soZl2+D1blWH+YcsLs7+NbldSOG6x5slyXLY5KXfPUtUOsK5Wi5Zx22beaISL2h1vGAPyh0cAtqGQYUfOqmKkHwjy7NknU8gQudHIVSKUcieingDGAHhWXyYApwOYoO/yAoALw9ThF2OQhgpCCJz7uP8UvkYu6mxhl5s7DHZnE48LnPbIDLy1IPyky4C6hV+xUy2GXwiBrdW1GHbfh1i+pUbRcg/glrGLLrJ0UMrn5iaG3w5BP09Zm+YlnscLkVTor31Vgc27tbEKVqXvhbz/0s3VGPz7D7Ctxl+20GQYJnx/LdL59k2YuwFnPpqaQjgf4+rDWu5/AfBrAIYp2xHALiGEoWE3AOhhdyARjSWiOUQ0p7IyOmV1oFF9+HtQI78hnjsdqlE8dHZlHGiIY832vbjtjQXuxyq+alENEpLXvbtwM7bvqcO/v1ynpNyddnF1y9j2c1jXpeZxT61D+I6q8TNzVJlCaGJckkF2i/hufUm/n/14DXbvr8fM5f7eYbt7EWw4U7Tc+sYCfLs1NT1z4kOeaYFCEFi5E9F5ALYJIQIN4xRCPCOEGCaEGNa5c+egYtgU7EsG5X2370nmjW/IsuUuo6JcP1i8BZNt5hRVKcMUU293rM9LEVXfbVwIVO/XbIg2ZSWKyj1AVIbCISqWu7xatQO7MS7w4LtLbXPoq0bfbK2uxUY9wVlc2Mv3wLtLE8+3349O0LTBqqNjX59TgU9WbDetI0U5IyXh4spwvSEIY7mfAOB8IloLrQP1dACPAWhHREZHbU8AG0NJ6BM/99yP5S7PzFSfpRXRLoQAACAASURBVDh3O1Qe8rEvzcVNr8xzKcRulaJFrrRXOJzi+2v00Zaty4qVQiHd+mPCKAv5fXeqI0j5n67ajqdnrcYd/0vNpZOSzcCh/FullpcQwvYDt3Djbtyp1+E3R09iNiefmiSR18ajvl9P+AZXPfelv8IjwMnwy4XWhSqBlbsQ4nYhRE8hRDmAywFME0JcCWA6gEv03a4FMCm0lD7wY51VVKkPepJj250USWXNgRQrI9eYY5ORMIxFHrex4lTYvHs/Pl/lPvetV1y6MZS+TfMSpY50t05XPz53KyrWnNN8sG7sU8yb/8mK7ajcYz8j2QHpudU6Ue3LMAwWJctdsm0Sszn5VHpGSyvoRzXdRkVqy0gjnyz3dKQf+A2AV4noPgBfA3guDXU44udhCTr3opPl/r2nP8fq7XuxdvyYQOUa+FGS6fK5J9LBeoYB+pPF2OfMP89CzYEGdG9b5n2MzascFyIxvVqLZkVqlruLy8SPz916TeRlVeNCxUI2FHNpsU14qHS8qmUr+9ytyPlnvJDrDqr0TB2qAUi3W8auMzrfiGQQkxBihhDiPP33aiHEcCFEPyHEpUKI8JOc+pHFYf2mXfsVB7p44+RzN5IqOb1AG6r2mbZtcGg5hBHTqUwg6cZQQV2G1BfdDuvLX6Mnk1JSJgo7GRNnuOHsMnFLP2Be3mrj/zbn+nEox1O6VIz5V22Vu6Jv3Owych4P4Ed5yfsmc8L70+4Jt4wANijcu0zj9RHMBwpuhKrdC7x5934cP34aHvlgeSR1eEXL2L0/c9ftxIkPTccbc7WZk2Ys34YTH5pue7yfj5D8EH6xegdOfGg6/ve1/exMx97/kX0ZdhEhqhaozYseNXalynXt2lePu6U+ESec3DKa5e68TWbEA1NTJiSROxMdr4Gij1zGsNyb2Vru/okL4f3R9hmimexQ9S8LoIVSTpyn2C1nCtlMryltvU75OL9ywSl3O4yJJaKKD/eKc7d7wY3wqrlrNZ/3IpeZZIIqyRVbawAAc9baz/RTW2//UXIKNVTBNLoxg8+/XFe1QvIpLRTRaaOzsrRT+tbZlmRcXT+WZS9q9Y+InXIPShRuGXM/i/Y/aLTMGmlkrZ8i0u+Wse8jaRIdqrmK202PavCqVyhkmJhrbZu6oJt21yZGzBbpIQv//nK98vGAt2Xseqy0X5CPkjGQRrWOxDrpt8p9dbsnAs6K32611b9vVkrOrh+DDVX7cNojMxzlMUhY7jYJd1RbVuYwTftoGXO53mXaumWUpDGOT7YggmTUzAROsrBbJovYdoAl8lkHf3rkI73cMm7VyEmcnPDbN2AoG7ukWyrYXRdVETLxPjp9fOyG1TvhGikjnGqxv5duyl3lui2o2OW9E5IRWqUlwdwy1n2c4tz9YupQNSxaH1ovrBInysxzJ5ND3x1lCk65u418W7alBm8HHE4vK0C/bpm6hjhun7hQLwem/7bH+wyjnzRf81lam8YPT1nm6H+XsRPFOF+vd9Z3tAwEvl6vNkG0wa02o2RN9SqU4dbaEnCR3U65W6KlTDNjKbQAWpaqBaklLffU0acq19qYwNsgLoSn4aBkAEm7GPmP/PjcndId+HPLpNvnbnXL5J96LzjlbncT5AEWP/vP14HKld8Jr5hqq5X4zYZUS83tYVFJgiXzm/9qHw7rZMN/m7EKv3jNPX2AJkzqKrtTtPfNq7llZMluftXfPVhdudcmTlzY/nbCLVRSuPncbbZYQ2Gtrg+nOgzKStRmMUrMMxrRJNJCuF0r9XRcdpfSj889m/njVXGOc88fv0zhKXebdVF0gsgWj6ff0mJ521lLbt+HoCGbfju1+nZqCUBTYLX1jXhm1qqEVarqP1+ud+Jq5ajRrW1zX3ICSImoECL5wVCR1e2D/Pyna7Brn32nrF3R1sla5Kv+9MxVtpO5yB8J1cylbqcVJKeP7OsOUqdb3X5GqLolKlu/Yx/+O9e7tenEpPkbTZ20c9ftDBRIkXKGhvspsGSZp+DmUE1X86mV1JR2ekGItJfDqmzs9ndTSEGjZexyobsRk9rST05fib9OW4k2ZSW4fHhvWxnsvh2/f3tJ4reb3PKWXu1bYPaanb5k/ZXFNSOXp6Ir6118XSu27cHtE7+x3WZX9AGrcpcuzJvzN2FAtzap5UgFRTJNY4A+kbjNsxmkWLsi/BhQ1g9Osgzgu3/7FDv21uHiY3r6lgEAbnl1PpoVx/DtfecAAC5+6nMA8D2w0Ok65ZHhXoCWe7qaeQS0KdMUvOON1/9b3Sp+OyyDKne3AUx2yBMnG6mSZ6/Zidr6Rin0K4mXWCpiEwjdFEalemEXjufGHo9U0FWOlrs/twzgHiqpHa9qdat9LFXLUIpzVynTZp2q0ltducf0flg9NMYcunatG1XRo5gGUwigYuc+pcFxuUrhKXebdVGMTI3HBYr1cBTnWGH76A3zwyxM/+0IKu+D7y3ztb/hxpHFnfj1Rvzq9QXBPjCKh6h2vrkqN+EvCuqWV+erVWqtx2ZdiuWuWo5eWL2i8olCEVvHITi5hPxcS7syVF2Cp/9pppobLdsdmAI46eHpOH78NH1RkyePDPfCU+52D0UUD0pjXKA4Zijv1G11DXHHZEhWZX2goRG7HSxFwH+0TFCKYvadaHPXVaUoMBVUrrM2d6fvolPL8Rlfv9Bl0Ji1PPP61HVWy9zqDrNVAFI5qllFw86hCqQmdotSadZKI3V9RbrIc85K4phSJXi1EkP2xMqyN8ZFItVDsn5r61v7zx2qWcQ+oiN8uXEhUKJb7tYbf+nfPzPNcWpV5tb9r3luNl79qsKxLr/RMkFJKHdLfQ1xgTMfnQXAZ/yyyzbzixv+/OQS0pmB2T5pmXnZGs3idXaqPvcoOlRNUV5CQWmqdKjqLosBd03xdZyB3P/hdB52z4i3W1BNiPkVuzDgrimYtkwL4/zJy3Nx2J1TTPtYS8p2QyIITUK5B1WWsvsgLpIvsfUFmWcTT2xatrzLX3p0JkaV4MyLhFvGsj7oTFOqSjsay91ftEzwirx3KVbwM8lKTN1yd9mm2qFqaeFEEXAgIFLmbPVT6m6FdBH2yt1wjVAoI26envZ61rdaem67uYpTQm/1//ljtxeicrc8Zos27saFT35qWjfigY8wZZHzzEQGsh+xMS4cLd0UGaxuGZ8vVKYGTMgdqjKNAWeaUlY4qmF8Ltv2HKjHm/O1AWmR9Kk4FBFEwdopgPpGgckLNyd+q+BU/rz1VQlfsBdOk2JbocQ+Cq41ARRZWnR+PrBVeqepUZYdQdJFqMhQXVuPe9/RIrz+9dla2yyfWh325JFXpgCVu+WuPPfJmpR9tlYfwO8meWcRtGb7K4nZu2WspLhlfCqfTLtlALNFJw/48ZUzJEDsdVCWb0nOc5mp62XFeA5SMwi6Yx3h6ox9SX+bvkrxeHOrMS4EvL4rKpcyLoT07BgHKotkyqrpdJjtIDpPubzr/qbC3PcyY/k2h7KsPvf8s90LXrmHwjLy0MktY0Ulzt2J1ZV78Mcp0aQm9sIpwsGa0jaJvxaLE34/dvaSJMsI2tJQqkfFBw3r/XY/KKzl7keT+u1QVSlZACnK3U8Hpzw2wrEOF587gQInu7PKWeQ0NiQPfexWCm8QU4R3RX58zdEy7nVYdZefmOyrn5udmNA4DCquHTkUMoooAGuVQoiUcoVCp57futJpubs9T06TPHuJ43cO3jAfQ1mWeNy/S9Fpn5hVufsQceW2ZKvL6fLauWXkNW7K3w9O/SXWohJ2e/4Y7oVnuVufCad7ofIcmNwycUhx7l4y+LPkZJytZn+8u3CL5z6G9bVo027nl54cF1LYKflSAfN1Mn7OWlGJbTXeaX6tx6dsk35H0RJwYkHFLrMykmUQ5v8G1R4zXq1wKA+wzOqk/1+3c5/vEb3JMiw+d4fvij+lJVKUYtBbIMvnN7umFSXL3bKL9SPlVJb7mAv7bZ+v2uFqqNXU1mPKIu/3NCgFp9yDzCDkRMzqltFXeHXgpQxi8vGkRhUpc9Mr8zz3MR7sJ5V9uO6y3fjyHMe9jUtSsXM/3vnGuzPbUxLpGqfTcv/xy/Mw+s/2c+0a1Vrv98tf+MunLyN3VBrnOGHuBlz29Ocp9apgDoVUcct4Fx4XqS69qIMA7C1z84cqVS4Vt4wZR8s9pfWt/bfb26naK/7xBUb9aYajLLdPXIgfvzw3MclO1BSeco9wT1O0jEucuxWrdZQN5a6C36nRvKjY6WylBHn5Va1Jlcmx08H6nfsiv1+yLzulozbANfTtc1d0y6Ss8yuYR33WD3ZDY9xkBVuvxYaqfZ6T6Ngdl9IxbOxnWU7OFast1zXElVITOM1+BiQnqtmlEBoahMLzuTvEpwbB1EyUQiH9dqj6eSeDxpgHwRrOFjXavVBPJeuvbGs9meexqSsSyeKiwqzczQXvq2tEy9JiX9dSdllFOVmH334G57LMJJPvmdc/NGUZnv90bWLZuv3Eh6bjiuG9vOuzHGfXUgJSXX1yZy4A3PG/hZgwdwOW3HsWSovVUjhbKdEDNKLIhWNH4VnuEb5osj+uUQ//ipG3MnHPLeNORi1336a7v/3t3DJRYYqWyZLlDmiJ1qLsxC8y9fOYyzUG//j5mJn6JhQsd6UyhZ37Jli5TudiPfePV2xPkcGKMSjJD0VFzi0lkzwWy/2jpdrAJzfL3Itm+keBlbsiqo+YyjNuGjIf19w0MSLPF2RPbQPKx03GK/pcpipxzeXjJkMIkVEXgx/LvXzcZGzfc8BX+WH1iGuHqrQtnekHvCgpikViDRu4KRunnPNuyM+qm+VuWKRKicNEan4g6/J5f/1YSb4Uy12qQ2bZFrNf2u4ddGv1zq/YhfJxk7Fkc7VpvXUsS0Iuqfhpy7ZKuWXMcmoTrwd7AIy5cSNJAW1D4Sl3y3WOLFpGCMQIunJ3P26LPurt7zNX6ccqVAbNAs2kh0F2AThVG5XjJp1umcYMurKslBTFInULFVn6eWT21bmnErZDvjSN8XhkoZBersdFG81KVLW+ZGZV52PIofXspiPf00cHT19mHrTklIBObpm8/tWGlPONSXIGvfulxbpyZ8tdjaue/VIpTlzlhdyxty7RFDbSDxABby/YhLsnLXI8zi3O3Y0wUR8/fmmu72NUDPe9dY24+rkvA0iUfEEWbdyNP7zjPXDFX9lJ0jiGyZPS4hgEgN4dWkRSnuwqs7om9tZpYbJ+Ttc696/nHKrK5ZqXv1i9I2Ufv+M1TDH5Lhbxi5+vs1X+bi1Lo0XsFp9vrl9ab5PJNJEiWeGK/XXqCtv1zVi5+6OuMY4npq2MrDxjQu24EIjFNLfMhqr9eOHzdY7HGA+lceNVLfcwBuiUxf7jZZ0iBWQa4yLF36mK8bIYk4NHiamjMIs+95IiAoRa8jAVit06VD0mAbFDvjQNjdFFy1gV70tfpL4PTkrNsVzpd9yjFeu3tWR81Ky3Sf7YuXXSy0nLoP8yjrGNHpJW/unDb21lYrdMAMpKojut5Jyihs/dvP19G6W6eJPWJE0Ockm/5R4Ev3OuBuWggDMvbXFI6gSYr1WYDtV2LUrQr0urwMdrPncRoHPaHrdQyH113gPcTj60s2lZvoYN8bizz92H+KozOvl9vOJCSD5391aEV/2PvG9O4WF81KwGjdzH5TS3qykLpeWcXv5iHW6bkDoJvfVVXr6lBm/phqJBSXF6o2UKLhQSUJtdXlUdGDe/MS5QRKkK8UYbd8i/Plur1aFXouyWybAFminl3rFls8jLlF/KMNeNgMT4hSCU6G6ZqMYMuLWmDJ+72+Pk1oKob/Tu/FMdCKRmsPiMrlJ0yxjb3Xhi+kpcc9zBieWk5W5V7knFah0TkPiNZIsnYbfrP/7q4CWwynfWX7T5Ec4f3D2xrkyPlrHOBxAVhWm5K8SdqhrJsnKP6T53v1inOnPcL8PKPYROUyKdDRH5WoVt8TQP0dJbsbUG63bsi+xD6abc9ypY7m5yfLpyu+/R1XY0NMYDp0Nww9SPEo9mxi4DY4CTNdfRV9J5LJAyRhqtb4PECFX9eK+Pucoz2V43ei49xjs+PwgFarl7v6yqrhLDLSOE0NwyLh1eTnWoWpaZdsuYomXSUHVivtg0lN0YleVOhObNgg1CAYCv1moTP1hD9YLiFp66X0G5u31jpi2zT28roxKK+8f3l3tOOOMliy1CSHO5undWKrWGpfqN2Z+sSlnuO7tKChywzrmbYrl7tEpUxIs79ANERUFa7kUxSs6JGPLCNcQF9tU1aIOY9Dh3g30eSb6M+6v6IVF5eaMk3W6ZhrhAbX0jahuiP69GB1+pX4QQaK7gxssUbpZ7sqM+eoi050/lQ+k1H22iTJ8ymDpUhXuHqspHSA5QMNIsB05wZhHG69VxeiblMS+GLOl6DwvScr9v8lLcN3kp1o4f47iP6j1etrkGg373PgDo0TLJbUfc/b7rsUmfu1pdJz08XVGqaEi3cr/xxbn43CZELgrkVo5KThEnqvbVK/XRZAo35a7Ssgt6R99duAXvLpyCk/p38txX9Xr7fbxMYxc8lLtKDn/zM6Ip1VnfVvoTSsfvR8HpI1nXGE9kl7WOeo2agrTclVC8Wd9sSM6PWhTzO2G0YS1kL1TPjXQ1Bw3SpdiB6Cx3AGmx3Pt2ahnoODflnokuGRWrXDX/kZfrwooWLZMcKevmllEJH5Q/AH5z6FsxPAHGwCOvM3O6V3JkjNDdUFHMpWBHXiv3Kkv+8HRQJz0gdqGQbmytPoAH31uakZcyCHL/wScrg8WyZwtZuYf1d4fxuTsxpHf7QMe5K3ejDyN9D5SKW0b1efZtuVvqcDtNFeU+5vFkCoSPlnr3NzjKJZI5ZIyBR04KuXzcZJSPm4zdDqkiXvgs6eO3S50cJXmt3GevDd5jr/p61En+4pjF567C0zNXZy1roRdy592a7Xtd9sw9ogwbtVru3QLG5cuUBAxvcwtlzEQ0VZTXNUB/qkkOtxaZSmx4TYBBX04Ylrvx/nupgW827rJd/+hHyQFNRkqTdBFYuRNRLyKaTkRLiGgxEd2ir+9ARB8S0Qr9fzATJgL21TVg4ryNttv2KN54eb7Loph/5Q7krltGZYRqrhKlErL63M86/KDQZQaNXXYNlU3DwCErkSp3D2Gsm9du35uwyOPCfWC/nP43ExiWuyFVFAZ3XKTPJQOEs9wbAPxKCDEIwEgANxHRIADjAEwVQvQHMFVfTgteOtMYTKTK0b3apayTfXVFMYLTfLpuyCMLo0wPG5aoRlVmgyjDRq1uGcOvGobiIA8KgHoX5apiJKj4uTu3LsUlx/S03ZbJgXTWsE85hYbTsP5sUatHxkUpk8hVy10IsVkIMU//XQNgKYAeAC4A8IK+2wsALgwrZFD2HfAXgnf+4O4pMfJyyFVxjHx3EgGZtzJUSfdkHekknW6ZZpEod/O1PfWwzg57mql3cTfE4wKT5m/EgZDD1Uf27YiHLj7KdluUH00v48rNuKipbUjkdco2n67cnnBbGlfHSw8ccMnzPnddFV76Yh3W74xu8JsdkYRCElE5gCEAvgTQVQhhTJK5BUBXh2PGAhgLAL179w5Ys/uDeMBnfLWR0teJ4qIY1u/c56vMXCavLfeAyj1Gqe4Nq3IPk47AoNhSRjPFMt0iUaYt3+aasA5Qcxdoz7n9toymnHYR9s43F2L7nvQHTKiwt64Rn63SIr+M/jOvV8dpDEzzkiJc/NRnieVWpemLRg/9FBNRKwD/BfBzIYRpzK7QroTt4yKEeEYIMUwIMaxzZzWrxi/LtzrPMm9HLEauD1xJHitDO/L5dIIqdzt3SVmz9FvuJZYyncIv613it93mqPVDjCitvl5V3DqPc0WxWzE+fl7Xb78l975xqt3amTvr03kbQj3FRFQCTbH/WwgxUV+9lYi66du7AQgeg+SBl5Xhd8CC9tA7b7daY/lOPrtlgsYt23Uip8dyN9dTaimzg0MytT4B4+P9kCu3PR9bjgnl7rGfNYOnYYvsqbUq/RzsUCXt0/UcgKVCiD9Lm94CcK3++1oAk4KL507ULUhr7hgrQcPbgtI6jU02IP9erlOkdLZBlbudtWgty81yL++oNimH9QNhVfbtW5akHPPxr0/D4J6pnfp+cNMVZ+tRQLnyUc/HaC3VgAinVCLWKL2c7FAFcAKAqwGcTkTz9b9zAYwHcAYRrQAwWl9OC1H7B4tiHj73CO6EU2imHZ1al4auz41MpfwNg6xo5fjzoDmw7UIUrUqmQwvnFMWty1KVsh3WMlPqaJl6b7u3ax5p2GynVubzMPLq58p9z3QupShwyutuxSn3vnV9VYB5cVUJEy3ziRCChBBHCSGO1v/eFULsEEKMEkL0F0KMFkJEnxs0TZDHIKVMu2XSbdnkg+EkuzNkP7vbCMWJ/3e847YiG5/7mYO64onvD8HCe87Es9cMw+Hd2zger3rNrIaA1UfbrU3qQKkYASf0887tooq1TuN5Mi7BKz8aEVldQdgvdTq+87MTcd3x5dkTRhFF3e47mCMd5LUTOeqYca/0Apl2y6Rb+foJezusa+s0SuKM0yhDN8t9qMvQf7vWFxHhvKO6o3VZCUYP6ur+UVW0eq1uGasrxJiFxyrHwYpuHxWsp2Gcu6H0jz8kug9JWMo7tcRpA7qEKiOKkcVeTJi7AYB36+f1ORvSLosXea3co6Yo5m4tBx2YEpQgMfV+2OkjIiFds8X44aRDO+Osw7XIWiPW2++AI5XWkFuHqtPR1nfdWo+x2KJZER67/GhHv3eLCPPcWJ+fhOWe/VuZQozCR6O1ba7mMguLMbeDX1TDYaMir5W7X/fk2R7Dyr1yx+SCgouSiir1mP0oIkjCQgDuHDMIQNJyP+0wf9aeyj1028dJ/1hXW1t5hrU8emBXXHB0D1v3EAC0aBauE11W6FZZDeWeKx2qMgQK7YbMlHL/y0crAnkNMq0/sv/GhsBv55NXqtIYEe678AjH7SVFMdftUeP2DrYqLQ7dwXv9CX2U943SJXX5sb0w5qhuvo+LC5H4yBg+d6fGlNPwepVrVuLSQnOKb7aut7byDKPBqP6qkb3RqVUzUwQQEL6fRRbDyeeeTS4a0sN2fVGMQvdp/eikvqGOV+WxqSvwrc8xNEA0ARl+aGLK3X3/GJGr368xLhyVxqUO64Nil+dGZt5dZ+BPlw0OXH7rsmIMPMi549BKlC6p8RcfhbvPG+S6zwn9OqasiwuRsH4My91J2T5yqf21UVFwRS4fMqct1syfVivNqNZQ8n07t8KcO89A7w7R+dgBcwvLemkM5ZLNFNR/dLgvJUUUWvn16dwyrSM+g/D4FUMSvzPd+s1r5e431NlrVKM1dMzKvroGxwdwRN9UZRQGryT+YY0Avz7DsKM2e7Rr7qu87x2bmpIiHk9a1YZy93seKvu7KRmn462PlvVjaHwwrGMLohgNKyN/ZLpYQmkNV1A2k9c5fVyJwrtltGRjOZRtDEAz6SPPbhkfNCrOCGPgNT2Yl4Lec6DR9gG87azDcGSPtgD8dYj1aNccZwyyTb3j2ZXq9CJMuumExO8V95/jeHyMFCqRaFkarqPv4UvMiaq8lNr5g7unrGuULXfDLePzfVEZdu9qYSnWZ70/LUo0i9La2jRGx94yqn9i3aoHzlWrxAaj9D9echTaW+L1vSx3u2iTwT3bJn6XFBGevvqYQHKtuP8crHR5HrXyw6mjXFPsgPk5z3RARp4rd7/7h7v53duW2SqHZkWxRDZJqw/VjdKSmGOnsHcubEqxhgHzw+T2snilWrDSvCRcc9eq7IJEDghJuRtZ91Qt96N0JaXyMXDbR/VjYhXL+DhaswUa6YblTI9FMX8zfskYz1NJUczR5+70zNnlu5HvW1GMAkXzjOjTASVFsbSPE2n0yAGfDZoVJa9XxkOpM1pbxPi13Ot97m/l7CPso20EBA7u2BJv//RE3HuBeodrSSzm2G8QI28jcVh5B7x50wmmD4qq35LIX6Cln5f63gsOT1lnVe52L/o7PzvRtrwRfToA0CxOwy2zT0/MpPrC/PuHI/DhL05W+qC5fViNq3b5sb3w+o3HmbbNvO3UxG/5tn7869MSUTD7LdkCDYVaa1nvJ7HXlSOSLqzEIBuyiXMvMpS7/TNnZwzI900I9edAHkj23HXHKh1jpIHo0a45rj+hXOkYmXjc/sPl50N50VD7Dt+gyInCMj0IMs+Vu7/vdFjL3emFM4o9smdbX0qwpJgSMh3UxpotjtBZIf3A0b3amXyrqvli/E727ee8Tuqf2npREeuwg+wHSvXt3AqAdv+M86uuNZS72iPcuqwE/bu2RudW4VI6GPfkkM6tMFz/6ABaIrCDO8pJv5LPWq8OLRLXz6rEjfX7LFkE/TyrxxycHLRlKG6yaZkVJdwy9mV3ap3a52R9RtxacIZr0vpbtZPTqGpgt9a4aIj/AAWnD72ffo3jIu4769k+2brmaBkfyNEvj35vMH56Wj/HfZ+/7ti0xcHKL6KsbH5/fqoFK1NSlLTcrZ0tBOAv3zs6JfTy0e8Nxl+lHngAuFuqR/UBirlY7lbL+/zB3R3jr5+7dhi6W3y11o48o773f34y/nHNsMS6v105FNNvPRU3ntIXf7jwCEdFbZySncXp10/76PeOxgPfPdJzvye/PzSltXHfhUfgT5cNxu/OG4TrLJal3NehyWouz1DiKZZ7QrnbD1e/ckRv3OURWWS1rgHt+bGmD7bzub82diReHTsSd503CD8+5ZCUsuVLIODe9yIbFkFi6Qd1a4M/XHgEHrl0sCnEVY5Qe/aaYXjGxu/fpqwY/bu2tu20bOkxdmDsyckQSvMHOhyPXzEEpcWyW4Ytd2VkC+S7Q3ri1rMOc9z3tAFd0L1tqo86CuSsgrL1cK1HroySWCzxYbD6oImA9i2b4aqRB5vWd2xZ2SJNewAADyVJREFUiu9YOhtly0jVB+3mc7/mOLPcFx/TM2WGKoNRA7viUIvFbWflC2iWudyBfO6R3dCnU0vcfs5AXG05T6usgH1HoN8Ii3YtmuH7I7wnhxlzVLeUMMWrRh6MkqIYfnBin5QXtZe+rzFQzipqy1KjQ9W83nDLOCXRumhoD9xwovt4BLmjTp7js6bWnJQqeR2TQozo2xEj+3bEDSf2SZlLVj5GL9x1kJWsV4NkHCUiXD3yYLRr0cxUrxzWOnpQVwzslhrCa7wndv0GLT1aDr8YfWji98Bu0aXZsAYFZDpaJreCQn3SyWcT+7azD8Nrcyp8HXPpMT1xoCFu8rffdNohaFlajIenLAdgjsIhIlx3fDnOcfDPy5QUU+LY1Ljo5PLz1x+LCXM2oChGGNG3A+y4/7tHoKFReD5A5w/ujrcWbNJCLWF+gW59Y0Fi+akrh+In/56nyRkj12H+955/BB6asgyHdm2NPp1b2rp7VN0Mvz//8JQX9OZR/VFZcyDhDyWy963efs4AbN5dq1SPwa1nHuq47amrhuL5T9aiQ6tmOMYhX81z1w7DgorkTPeUaGWY9xvYrQ1O6t8JP7AMHDv50M44f3B33OZgmPTu4G1J7q9vxL0XHI6SolhiDgMCoXq/Q+5wh1th96G8YnhvlBTFMFMv18099+NTDsHYl+Z6ymvlwYtSW1JuH21rJ29jXCTWWefDBbxdivIxrUqLcdNph+DJ6as85ZaPsabytcNtcFw6yGvlftHQnvjl6wu8d9Tp1KoUh3RuiVWVe5WPsRt0cdtZA7ByW01CuVs7au/xcMcYFMdiqGuw9x3Lyv20w7p4DrO/coRmuWyrdlZuw8s74JdnHIq3FmxCjChh5ZWVxHDJMT1Nyv2cI7th2MHtMWddFYqLYomZhK4Y3gv/mV2hy6jt27tjCzx55VDbOkuKCPWNwjMM1cCutdO5dSn+LjXFu7Yuwxb9PGUlOrJvRwz2GPxl5aen93fcNuCgNnjoEvu5Rg1GDeyKUQOTrRHjvlnjNjq0bIaXbkjNwlhWUmQa6GLFa+wFAGys2o9bRmvnMXO5rtxtLHfjkXLyuRuulH5dWmHlNm0EZpvmJXjmmmNw2J1TICAcZ5ACgqeovmJ4akvKuI52bkZjXXFMG3T44ZKtCeVeprtBLji6OybN1+Zg9bLcZYgIt501wJdyb9u8REm5G4ZXl9al2FZzQLn8oOS1W8aNPp1a4kSb9KmyEpWta7kD6NCuWgee24vVTXLxqCouK9cdX56waC8aau5ACpr+w2rxXDasJ9q10PoafnBiObq00V7An53eT/LPJo+RLVlDCcQoaf3U1DYkXsY7zh3oKU9HPW95lBNad3S4L1FO7hwUI3LFLTOlCgO7tcGJ/TopdXqPGpj88F85Uqt/SO92iU5ng76dW6E4RjjVwVAwnh25xkHd2pjz1cS0kaRyR6HBIZ1aecoqc+6RB2GAQyd6Ig+OjXI3XD6lxTFMXboVANBRn9nKcB/KIaenuoQnG+/95cf2ck317MQlx/S0bS3Y6R7jgyXf0jFH+k/DoUpeW+5uvHnTCWjbvATl4yab1hvK/c2bTsDRvdoltr86dmRinw9+cYpn+S1Li3H3dwbh928vQUOAWYFuOLEPThvQBY9NXQFAi3pZO34MZizfhuue/8p3eQbWl+HhSwbjYcs+a8ePAZCcFcZ42Iz1BsmwOkp8zDbt2o+J/zfUtiltR8dWzbClujZ0GKqMk/UYz+a4ep3j+3VKuY5BePdm+7BQK9a6TurfOeX+Ghzdqx1W3H+O4wcjmTWSTOUao4GNb+fKEIOsZB6/fIhjeGCRi+VuGB1lJUXYq/dVXHZsr8Q6wJxPvbxTS6wdPyZFFwDA23r47fiL3VtoADD+oiMxbuLCxHKHls3wyKWDMXHeBpMH4eGLj0rII5PoD5E+lk9837nVFpaCsNztBvPIfjY5VMwIi7JOrebW3HTCeDDdctb06mDfiWvIbDyoxotldMydbBNOqIKfDkaj8/dcB+vBOK0YAf27aFaZapqFi/WWiNGp1MvG0guK3PEn31uv3EEychhjupBHd/qFHCaxHhFSbreWQHLyZ/N645kKkuzNDbdn1XBPG/vIAQfG/T/vqG4Y0ltzwxlRKUYr5iApgitIige7OW77W+Y0MD48Q/RW2hE9NMu/3GEeXCPw4azDuybmR0jnROV5b7kv+N2ZtjfPsNC/vusMU7PJUGj1lskegvTuG/mn3dwyH/z8FKzevgdjHv8kse79n5+Mfl2SsdtA0lI5pHMrfHnHKNtwQhX8KPfS4iLMuXO0Y4ioSLhlCB1blWL2b0e5TkEnM/7iI/Gbcw5D51aluHBID3S1mXkoKHLn7pijuuGRD1pizfa9KZb7/N+dgaPv/dC2jBd/MDzt07y9duNxkdfxwg+GY8BdUyIt0yAZaZM6stXtOQmKm2IznmPDgPr6d2ck3G5tykoSz2JDXJiu8Y9O6osLju6BSfOT01kGUe6f/OY01DcKDP79B4l1xxzcHjNvOxU1tQ0476+fJJR7n04tMfuOUejYqhSVNQdMHxaZ1mUl+Oq3o9G+RQka4iJlzEPU5L3l3rZFSUpvt0z7ls1Mlp6hVFtEkD1OxXJv3qwIh3dvm/DjA1pIYJH0YABalkaDrm3s0xyo4Dc0sFOrUsf427ik3AGgS+sy5VF2JUUxdGmtnUeUih0A+nU1+3ZH6q2JVmXme9pO/xDZtezKSorQ3sY6i5J01GEXruiE3wyJcoy8FbfnxC99O3tHABnPnPE8tywtRhtp/lrjWbReY+N569La+5lzS4HRolmx7cfs4I4tE4bXACkks0ubMhTFyFGxA9pz2Ll1aULudoqGUlDy3nK38uEvT8amXfsdt9913iCccmgXz5S6Khhfbq888QDw6tjjMPQPqVbkQxcfhYuP6enYlPNLlBMxGKeVa3M73HrmYWjbvCQRQXT3dwZh1IAuOKpn6j19bezIxAjXfGbar05Brd5J+PqNx6EhHk90VjvxwS9OxvHjpwEAPvrlyZ51GMo9qoi9T8edbhu99caNx2HFNvd86EaLNuiozguO7o6fvzYfAHBAt5Bn3XYa3lm4CUf3aoet1bUJo0AFeZBalzZleOEHwzG0t9/ILOdBlumg4JR7t7bNTZEsVkqLixwzMfrFsKJUrGU7Hx6gWSR+ZxNyI+GjjCCVrBF5kAuTPMiUFMXwf6cmX5SykiKMdrinUadizhbyB0q1v6C71GLp18V7cI7xEW8RMkmcQY92zW1bTR1blaKjxxgVo9UYND87EeHyY3vh1a8qEiN1e3dsYXpu/GANsfWTINAg0yNUC065++Vf1x+LmlrvGFU7Rg/sgp+cegjGKs4AM/H/jsfSzdWB6lKFiHDnmIG2+V388sT3h+K1ryocw9XSzT+vG5aSRZHxx0s3DEfVvnrvHQEc3r0Nbh7VH1cMT430UOWVH43AFp8Dyezo0a45fnnGofiuw8xNKtx+zkC0bV6Cszym13TjnZ+diHnrqwIfD2gRMdmYRIRyIQfysGHDxJw5c7ItBsMwTF5BRHOFEMPstuV9hyrDMAyTCit3hmGYAoSVO8MwTAHCyp1hGKYAYeXOMAxTgLByZxiGKUBYuTMMwxQgrNwZhmEKkJwYxERElQDWBTy8E4DtEYqTD/A5Nw34nJsGYc75YCGE7XD0nFDuYSCiOU4jtAoVPuemAZ9z0yBd58xuGYZhmAKElTvDMEwBUgjK/ZlsC5AF+JybBnzOTYO0nHPe+9wZhmGYVArBcmcYhmEssHJnGIYpQPJauRPR2US0nIhWEtG4bMsTFUTUi4imE9ESIlpMRLfo6zsQ0YdEtEL/315fT0T0uH4dviGiodk9g2AQURERfU1E7+jLfYjoS/28XiOiZvr6Un15pb69PJtyB4WI2hHRBCJaRkRLiei4JnCPf6E/04uI6D9EVFaI95mI/klE24hokbTO970lomv1/VcQ0bV+ZMhb5U5ERQCeBHAOgEEAriCiQdmVKjIaAPxKCDEIwEgAN+nnNg7AVCFEfwBT9WVAuwb99b+xAJ7KvMiRcAuApdLyQwAeFUL0A1AF4AZ9/Q0AqvT1j+r75SOPAZgihBgAYDC0cy/Ye0xEPQDcDGCYEOIIAEUALkdh3ud/ATjbss7XvSWiDgDuBjACwHAAdxsfBCWEEHn5B+A4AO9Ly7cDuD3bcqXpXCcBOAPAcgDd9HXdACzXfz8N4App/8R++fIHoKf+wJ8O4B0ABG3UXrH1fgN4H8Bx+u9ifT/K9jn4PN+2ANZY5S7we9wDQAWADvp9ewfAWYV6nwGUA1gU9N4CuALA09J6035ef3lruSP5oBhs0NcVFHpTdAiALwF0FUJs1jdtAdBV/10I1+IvAH4NwJgRuyOAXUIIY/Zy+ZwS56tv363vn0/0AVAJ4HndFfUsEbVEAd9jIcRGAI8AWA9gM7T7NheFfZ9l/N7bUPc8n5V7wUNErQD8F8DPhRDV8jahfcoLIo6ViM4DsE0IMTfbsmSQYgBDATwlhBgCYC+SzXQAhXWPAUB3KVwA7cPWHUBLpLoumgSZuLf5rNw3AuglLffU1xUERFQCTbH/WwgxUV+9lYi66du7Adimr8/3a3ECgPOJaC2AV6G5Zh4D0I6IivV95HNKnK++vS2AHZkUOAI2ANgghPhSX54ATdkX6j0GgNEA1gghKoUQ9QAmQrv3hXyfZfze21D3PJ+V+1cA+us97c2gdcy8lWWZIoGICMBzAJYKIf4sbXoLgNFjfi00X7yx/hq9130kgN1S8y/nEULcLoToKYQoh3YfpwkhrgQwHcAl+m7W8zWuwyX6/nll4QohtgCoIKLD9FWjACxBgd5jnfUARhJRC/0ZN865YO+zBb/39n0AZxJRe73Vc6a+To1sdzqE7LA4F8C3AFYB+G225YnwvE6E1mT7BsB8/e9caP7GqQBWAPgIQAd9f4IWObQKwEJo0QhZP4+A534qgHf0330BzAawEsAbAEr19WX68kp9e99syx3wXI8GMEe/z28CaF/o9xjA7wEsA7AIwEsASgvxPgP4D7R+hXporbQbgtxbAD/Qz38lgOv9yMDpBxiGYQqQfHbLMAzDMA6wcmcYhilAWLkzDMMUIKzcGYZhChBW7gzDMAUIK3eGYZgChJU7wzBMAfL/OdOXCavneKQAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "#env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 1000)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_111\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_342 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_343 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_344 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_345 (Dense)            (None, 4)                 132       \n",
      "=================================================================\n",
      "Total params: 2,532\n",
      "Trainable params: 2,532\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Model: \"model_113\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_350 (Dense)            (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_351 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_352 (Dense)            (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_353 (Dense)            (None, 1)                 33        \n",
      "=================================================================\n",
      "Total params: 2,433\n",
      "Trainable params: 2,433\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward -184.25776796935327\n",
      "episode: 1 - cum reward -189.7188397849216\n",
      "episode: 2 - cum reward -89.32067428250814\n",
      "episode: 3 - cum reward -201.13122889744562\n",
      "episode: 4 - cum reward -82.80972569626624\n",
      "episode: 5 - cum reward -223.33968360429787\n",
      "episode: 6 - cum reward -195.65721351916847\n",
      "episode: 7 - cum reward -124.6918889163855\n",
      "episode: 8 - cum reward -201.836140750835\n",
      "episode: 9 - cum reward -88.86257267247626\n",
      "-0.8462877135701886\n",
      "-95.6284728050232\n",
      "22.46339073404557\n",
      "7.2011014814001655\n",
      "loss  -0.0155251585\n",
      "episode: 10 - cum reward -205.76924930520224\n",
      "episode: 11 - cum reward -187.76524757600816\n",
      "episode: 12 - cum reward -149.83629706184533\n",
      "episode: 13 - cum reward -38.028492212305\n",
      "episode: 14 - cum reward -151.68694134437175\n",
      "episode: 15 - cum reward -141.49944064085912\n",
      "episode: 16 - cum reward -191.69290636673935\n",
      "episode: 17 - cum reward -30.449633268400817\n",
      "episode: 18 - cum reward -131.4836130309174\n",
      "episode: 19 - cum reward -112.20058430733198\n",
      "episode: 20 - cum reward -144.67998416187024\n",
      "-0.42879662099192223\n",
      "-85.49467945098877\n",
      "15.487158750589515\n",
      "4.889347188997014\n",
      "loss  -0.15041415\n",
      "episode: 21 - cum reward -178.70493696847183\n",
      "episode: 22 - cum reward -371.5884428467354\n",
      "episode: 23 - cum reward -136.71749914278507\n",
      "episode: 24 - cum reward -129.21931694127437\n",
      "episode: 25 - cum reward -291.52803181453567\n",
      "episode: 26 - cum reward -223.38714697032896\n",
      "episode: 27 - cum reward -282.42865184076595\n",
      "episode: 28 - cum reward -206.8275501179483\n",
      "episode: 29 - cum reward -251.9481461666055\n",
      "episode: 30 - cum reward -136.8496677995348\n",
      "episode: 31 - cum reward -301.1730480807282\n",
      "-0.27112397706557956\n",
      "-53.78745651245117\n",
      "20.065527146560992\n",
      "2.0978155130848095\n",
      "loss  -0.3256114\n",
      "episode: 32 - cum reward -142.86343571379925\n",
      "episode: 33 - cum reward -458.2955192350013\n",
      "episode: 34 - cum reward -297.29269098469916\n",
      "episode: 35 - cum reward -545.6750639176689\n",
      "episode: 36 - cum reward -792.5820765673528\n",
      "episode: 37 - cum reward -601.5944855848437\n",
      "episode: 38 - cum reward -912.3721605506588\n",
      "episode: 39 - cum reward -555.2918619742906\n",
      "episode: 40 - cum reward -619.4811334799715\n",
      "episode: 41 - cum reward -418.49613282376146\n",
      "episode: 42 - cum reward -510.8334771393393\n",
      "-0.8134059649294032\n",
      "-7.521640788624893\n",
      "447.4102028877959\n",
      "10.166423623708294\n",
      "loss  0.13952035\n",
      "episode: 43 - cum reward -640.9253647520052\n",
      "episode: 44 - cum reward -207.3919751884524\n",
      "episode: 45 - cum reward -159.84176705409084\n",
      "episode: 46 - cum reward -138.13934205082143\n",
      "episode: 47 - cum reward -138.00873360641225\n",
      "episode: 48 - cum reward -252.59293589544978\n",
      "episode: 49 - cum reward -165.4838206924821\n",
      "episode: 50 - cum reward -178.3722360470003\n",
      "episode: 51 - cum reward -140.01430163135038\n",
      "episode: 52 - cum reward -202.45476124344452\n",
      "episode: 53 - cum reward -113.98869476356427\n",
      "0.1102024538021292\n",
      "-60.35348120089111\n",
      "255.69387976019976\n",
      "7.410646056668727\n",
      "loss  -0.07719101\n",
      "episode: 54 - cum reward -204.57565570254926\n",
      "episode: 55 - cum reward -168.85406480979833\n",
      "episode: 56 - cum reward -187.36188365260057\n",
      "episode: 57 - cum reward -186.20209222907386\n",
      "episode: 58 - cum reward -159.46849718502457\n",
      "episode: 59 - cum reward -203.20531200595917\n",
      "episode: 60 - cum reward -190.21515629901356\n",
      "episode: 61 - cum reward -201.24144165437593\n",
      "episode: 62 - cum reward -162.5133207636229\n",
      "episode: 63 - cum reward -188.72146177678707\n",
      "episode: 64 - cum reward -180.0917855435179\n",
      "0.3105122404948001\n",
      "-7.27621847482726\n",
      "182.52706909179688\n",
      "7.190547575292405\n",
      "loss  0.0133434525\n",
      "episode: 65 - cum reward -169.51255298098158\n",
      "episode: 66 - cum reward 17.652505730875156\n",
      "episode: 67 - cum reward -136.12870561499687\n",
      "episode: 68 - cum reward 179.26747929260824\n",
      "episode: 69 - cum reward -172.28927473948414\n",
      "episode: 70 - cum reward -79.63149710898699\n",
      "episode: 71 - cum reward -61.80333490782785\n",
      "episode: 72 - cum reward 29.799314062958175\n",
      "episode: 73 - cum reward -98.98238839601267\n",
      "episode: 74 - cum reward 163.79055818542895\n",
      "episode: 75 - cum reward -35.76269107533085\n",
      "0.42597188768022093\n",
      "-41.301421655102416\n",
      "267.1455841064453\n",
      "9.110255618226926\n",
      "loss  -0.13035227\n",
      "episode: 76 - cum reward 190.3495937813521\n",
      "episode: 77 - cum reward -147.4898211094934\n",
      "episode: 78 - cum reward -167.39483677280526\n",
      "episode: 79 - cum reward -167.39883076954112\n",
      "episode: 80 - cum reward -128.477568192603\n",
      "episode: 81 - cum reward -126.49274938435642\n",
      "episode: 82 - cum reward -185.88721980482154\n",
      "episode: 83 - cum reward -106.86604923861634\n",
      "episode: 84 - cum reward -207.19706929944664\n",
      "episode: 85 - cum reward -203.65239822081287\n",
      "episode: 86 - cum reward -102.37714300774493\n",
      "0.1129354364027203\n",
      "-29.347335815429688\n",
      "138.25390313948262\n",
      "2.934264981547829\n",
      "loss  0.07055293\n",
      "episode: 87 - cum reward -218.37837398312007\n",
      "episode: 88 - cum reward 75.50523355121337\n",
      "episode: 89 - cum reward -145.63086854223093\n",
      "episode: 90 - cum reward -101.76389575288802\n",
      "episode: 91 - cum reward 1.4832099804435757\n",
      "episode: 92 - cum reward -6.771106465155567\n",
      "episode: 93 - cum reward 123.96447816212824\n",
      "episode: 94 - cum reward -97.70219278420777\n",
      "episode: 95 - cum reward 21.153211516046866\n",
      "episode: 96 - cum reward 60.44780267546307\n",
      "episode: 97 - cum reward 0.3908055112821338\n",
      "0.22672466752610726\n",
      "-45.50437545776367\n",
      "144.41716766357422\n",
      "4.63199717967435\n",
      "loss  -0.15179555\n",
      "episode: 98 - cum reward -135.1080049232205\n",
      "episode: 99 - cum reward -236.44870935486605\n",
      "episode: 100 - cum reward -150.83057442478002\n",
      "episode: 101 - cum reward -183.0830815551149\n",
      "episode: 102 - cum reward -171.85892260428477\n",
      "episode: 103 - cum reward -162.3340100527394\n",
      "episode: 104 - cum reward -110.16240974207625\n",
      "episode: 105 - cum reward -124.548124012323\n",
      "episode: 106 - cum reward -138.05347662397392\n",
      "episode: 107 - cum reward -136.79071002109947\n",
      "episode: 108 - cum reward -87.56224970219125\n",
      "-0.029259029024922164\n",
      "-7.913162869809199\n",
      "174.50435638646587\n",
      "4.731851934532375\n",
      "loss  0.040146966\n",
      "episode: 109 - cum reward -106.90607538196\n",
      "episode: 110 - cum reward 89.06003623970949\n",
      "episode: 111 - cum reward 50.80917306319995\n",
      "episode: 112 - cum reward -138.18460549898614\n",
      "episode: 113 - cum reward -114.87255095554245\n",
      "episode: 114 - cum reward -113.0694866585397\n",
      "episode: 115 - cum reward -165.21743859332452\n",
      "episode: 116 - cum reward -53.2163636853099\n",
      "episode: 117 - cum reward -89.85210779734197\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 118 - cum reward -30.939273351842203\n",
      "episode: 119 - cum reward 83.75108754642108\n",
      "0.0910284429801067\n",
      "-66.29582977294922\n",
      "33.60968203840678\n",
      "4.782722005987471\n",
      "loss  -0.15136275\n",
      "episode: 120 - cum reward -127.7296877413506\n",
      "episode: 121 - cum reward -47.37488191119409\n",
      "episode: 122 - cum reward -201.5246072866475\n",
      "episode: 123 - cum reward -69.22632130061918\n",
      "episode: 124 - cum reward -131.8582223879456\n",
      "episode: 125 - cum reward -132.27978076841302\n",
      "episode: 126 - cum reward -91.78998025289884\n",
      "episode: 127 - cum reward -133.66091062907685\n",
      "episode: 128 - cum reward -108.57776135791242\n",
      "episode: 129 - cum reward -34.24535062213563\n",
      "episode: 130 - cum reward -94.63917081117404\n",
      "-0.0346758702405022\n",
      "-20.359222412109375\n",
      "63.897746147145824\n",
      "2.5229538186706804\n",
      "loss  -0.08508712\n",
      "episode: 131 - cum reward -102.4606704593317\n",
      "episode: 132 - cum reward 12.83033047133247\n",
      "episode: 133 - cum reward -105.4859218398813\n",
      "episode: 134 - cum reward -185.28300519797534\n",
      "episode: 135 - cum reward 49.447321615503185\n",
      "episode: 136 - cum reward -80.06241797305438\n",
      "episode: 137 - cum reward -71.8672110003753\n",
      "episode: 138 - cum reward -72.57271686489378\n",
      "episode: 139 - cum reward -126.19750364759624\n",
      "episode: 140 - cum reward 40.92670722456728\n",
      "episode: 141 - cum reward -69.84274683273568\n",
      "0.0036138199830857776\n",
      "-86.92252349853516\n",
      "32.233455630339435\n",
      "4.724952431808968\n",
      "loss  -0.12469114\n",
      "episode: 142 - cum reward 91.6851272640511\n",
      "episode: 143 - cum reward -110.3268770884952\n",
      "episode: 144 - cum reward -87.28262333270183\n",
      "episode: 145 - cum reward -122.94655258154357\n",
      "episode: 146 - cum reward -113.43740285614324\n",
      "episode: 147 - cum reward -53.933144878667036\n",
      "episode: 148 - cum reward -127.40766380088239\n",
      "episode: 149 - cum reward -87.99503307954268\n",
      "episode: 150 - cum reward -94.61601515134785\n",
      "episode: 151 - cum reward -87.02261905376184\n",
      "episode: 152 - cum reward -251.33591757305365\n",
      "-0.07084554954988784\n",
      "-44.732791900634766\n",
      "45.15066178381085\n",
      "2.146447837560253\n",
      "loss  -0.17877002\n",
      "episode: 153 - cum reward -111.03197884116524\n",
      "episode: 154 - cum reward 56.82854946801981\n",
      "episode: 155 - cum reward 53.45078348147874\n",
      "episode: 156 - cum reward -3.1184323884562417\n",
      "episode: 157 - cum reward 49.30300410756516\n",
      "episode: 158 - cum reward 62.023111742975544\n",
      "episode: 159 - cum reward -5.565313042424492\n",
      "episode: 160 - cum reward 77.40893388315575\n",
      "episode: 161 - cum reward 72.59678368726583\n",
      "episode: 162 - cum reward -21.09297112928985\n",
      "episode: 163 - cum reward 48.07194946661882\n",
      "0.07467539969345913\n",
      "-93.81529378890991\n",
      "28.044469166797946\n",
      "4.227756654909067\n",
      "loss  -0.2247485\n",
      "episode: 164 - cum reward -43.04481298049792\n",
      "episode: 165 - cum reward -121.0021464780871\n",
      "episode: 166 - cum reward -138.94040292506398\n",
      "episode: 167 - cum reward -177.56598541852858\n",
      "episode: 168 - cum reward -106.11719000679558\n",
      "episode: 169 - cum reward -103.92266931865245\n",
      "episode: 170 - cum reward -133.575794734882\n",
      "episode: 171 - cum reward -123.78445446398234\n",
      "episode: 172 - cum reward -131.9475445405991\n",
      "episode: 173 - cum reward -149.9890713786126\n",
      "episode: 174 - cum reward -141.86835285545706\n",
      "-0.1080932644408254\n",
      "-6.336991508758462\n",
      "23.592517117875815\n",
      "1.849002341241483\n",
      "loss  -0.17118298\n",
      "episode: 175 - cum reward -87.719693183022\n",
      "episode: 176 - cum reward -186.57008465463957\n",
      "episode: 177 - cum reward -211.50074263636284\n",
      "episode: 178 - cum reward -128.8558305644369\n",
      "episode: 179 - cum reward -196.54835005315402\n",
      "episode: 180 - cum reward -183.89711895120877\n",
      "episode: 181 - cum reward -178.36440405745222\n",
      "episode: 182 - cum reward -181.1107934940884\n",
      "episode: 183 - cum reward -184.32842546598044\n",
      "episode: 184 - cum reward -216.92243812906088\n",
      "episode: 185 - cum reward -154.672198360586\n",
      "-0.2391852135315642\n",
      "-86.82855033874512\n",
      "10.815953670452018\n",
      "3.240852351437435\n",
      "loss  -0.14843668\n",
      "episode: 186 - cum reward -210.96785370632077\n",
      "episode: 187 - cum reward -149.76369649831705\n",
      "episode: 188 - cum reward -193.75216871283374\n",
      "episode: 189 - cum reward -164.8737867263636\n",
      "episode: 190 - cum reward -131.78440456747634\n",
      "episode: 191 - cum reward -135.10864751904646\n",
      "episode: 192 - cum reward -159.0915393679732\n",
      "episode: 193 - cum reward -150.5008698944955\n",
      "episode: 194 - cum reward -129.27758771854667\n",
      "episode: 195 - cum reward -183.02649199426446\n",
      "episode: 196 - cum reward -115.81969084265691\n",
      "-0.138187917768805\n",
      "-6.0525514760615415\n",
      "16.10293387982759\n",
      "1.9991869205616506\n",
      "loss  -0.06750456\n",
      "episode: 197 - cum reward -134.16216789194533\n",
      "episode: 198 - cum reward -89.29659816635603\n",
      "episode: 199 - cum reward -197.02679973550096\n",
      "episode: 200 - cum reward -110.44316176203301\n",
      "episode: 201 - cum reward -54.54315758936224\n",
      "episode: 202 - cum reward -89.41560656900239\n",
      "episode: 203 - cum reward -79.66908656942896\n",
      "episode: 204 - cum reward -41.26895235976191\n",
      "episode: 205 - cum reward -126.76012880683378\n",
      "episode: 206 - cum reward -165.0943078080285\n",
      "episode: 207 - cum reward -170.93090566260156\n",
      "-0.11374441344126683\n",
      "-94.66185760498047\n",
      "17.281744246286205\n",
      "3.066683491807916\n",
      "loss  -0.29399672\n",
      "episode: 208 - cum reward -9.041334318307001\n",
      "episode: 209 - cum reward -154.2487934187462\n",
      "episode: 210 - cum reward -163.0688458276065\n",
      "episode: 211 - cum reward -155.38911419728606\n",
      "episode: 212 - cum reward -96.42933779545804\n",
      "episode: 213 - cum reward -145.39221908894748\n",
      "episode: 214 - cum reward -139.91793817006777\n",
      "episode: 215 - cum reward -126.42056503378082\n",
      "episode: 216 - cum reward -130.12502119140956\n",
      "episode: 217 - cum reward -168.41062387448255\n",
      "episode: 218 - cum reward -114.90883019839576\n",
      "-0.1283934503040981\n",
      "-5.799625551247875\n",
      "13.649565969209364\n",
      "1.9312917713124584\n",
      "loss  -0.14712872\n",
      "episode: 219 - cum reward -177.39588830538014\n",
      "episode: 220 - cum reward -151.93902306300416\n",
      "episode: 221 - cum reward -275.3204896389268\n",
      "episode: 222 - cum reward -96.38481204579335\n",
      "episode: 223 - cum reward -202.04228653926032\n",
      "episode: 224 - cum reward 45.21256541591964\n",
      "episode: 225 - cum reward -181.14105899557325\n",
      "episode: 226 - cum reward -168.56316590984164\n",
      "episode: 227 - cum reward -105.87560229730741\n",
      "episode: 228 - cum reward -127.79942438394511\n",
      "episode: 229 - cum reward -227.94070680002136\n",
      "-0.16412372024428606\n",
      "-94.77457237243652\n",
      "14.108195324207346\n",
      "2.9042900372031593\n",
      "loss  -0.25920555\n",
      "episode: 230 - cum reward -76.88369393735525\n",
      "episode: 231 - cum reward -141.87036343492287\n",
      "episode: 232 - cum reward -153.47604921193724\n",
      "episode: 233 - cum reward -115.00913924941354\n",
      "episode: 234 - cum reward -124.49156919607725\n",
      "episode: 235 - cum reward -153.75708124613197\n",
      "episode: 236 - cum reward -131.93397679210705\n",
      "episode: 237 - cum reward -127.76703769754572\n",
      "episode: 238 - cum reward -139.49565548139358\n",
      "episode: 239 - cum reward -123.7502350103729\n",
      "episode: 240 - cum reward -181.04167502483833\n",
      "-0.12662590950392366\n",
      "-5.799605508126167\n",
      "12.991205348902792\n",
      "1.876505074159686\n",
      "loss  -0.18761316\n",
      "episode: 241 - cum reward -136.08644820074062\n",
      "episode: 242 - cum reward -147.2093148499187\n",
      "episode: 243 - cum reward 49.19632556638791\n",
      "episode: 244 - cum reward 148.80862839592203\n",
      "episode: 245 - cum reward -115.58159176400906\n",
      "episode: 246 - cum reward -188.44218628838652\n",
      "episode: 247 - cum reward -34.85909615355122\n",
      "episode: 248 - cum reward -124.69629576958738\n",
      "episode: 249 - cum reward -110.59082939317068\n",
      "episode: 250 - cum reward -221.2297465451378\n",
      "episode: 251 - cum reward 11.825519708377003\n",
      "-0.07462984772875986\n",
      "-93.76863384246826\n",
      "105.43467903137207\n",
      "3.259184451397438\n",
      "loss  -0.2798618\n",
      "episode: 252 - cum reward -84.94099195258126\n",
      "episode: 253 - cum reward -166.23525170178792\n",
      "episode: 254 - cum reward -180.90243812546558\n",
      "episode: 255 - cum reward -161.19556020879293\n",
      "episode: 256 - cum reward -116.00471863395012\n",
      "episode: 257 - cum reward -136.93740784902948\n",
      "episode: 258 - cum reward -117.43154949101994\n",
      "episode: 259 - cum reward -130.66276843253854\n",
      "episode: 260 - cum reward -190.05329401865103\n",
      "episode: 261 - cum reward -153.67308066501752\n",
      "episode: 262 - cum reward -151.227849802813\n",
      "-0.14282204509877292\n",
      "-5.761240586847453\n",
      "12.969063605103914\n",
      "1.9796738376860727\n",
      "loss  -0.09393469\n",
      "episode: 263 - cum reward -183.87846761993433\n",
      "episode: 264 - cum reward 28.19665569999146\n",
      "episode: 265 - cum reward -116.07392154256993\n",
      "episode: 266 - cum reward -39.657885769973895\n",
      "episode: 267 - cum reward -4.977978990913954\n",
      "episode: 268 - cum reward -87.18977787033229\n",
      "episode: 269 - cum reward -90.42414659499664\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "episode: 270 - cum reward -58.2075791028067\n",
      "episode: 271 - cum reward -114.36150103688873\n",
      "episode: 272 - cum reward -31.150895654663714\n",
      "episode: 273 - cum reward -23.243892552802464\n",
      "-0.04583874374263967\n",
      "-94.40737533569336\n",
      "17.210923539207332\n",
      "2.4301084118632463\n",
      "loss  -0.4204816\n",
      "episode: 274 - cum reward -72.89728689894827\n",
      "episode: 275 - cum reward -184.7719007078856\n",
      "episode: 276 - cum reward -145.8524113738241\n",
      "episode: 277 - cum reward -129.73092485475036\n",
      "episode: 278 - cum reward -179.92326352818966\n",
      "episode: 279 - cum reward -129.1591215854279\n",
      "episode: 280 - cum reward -108.038797460422\n",
      "episode: 281 - cum reward -130.1862740015889\n",
      "episode: 282 - cum reward -114.42713079612862\n",
      "episode: 283 - cum reward -153.33079076065547\n",
      "episode: 284 - cum reward -130.78570338568795\n",
      "-0.13302253232762848\n",
      "-6.186378481815968\n",
      "13.542862364550446\n",
      "1.993552718304925\n",
      "loss  -0.06656126\n",
      "episode: 285 - cum reward -153.15787054447796\n",
      "episode: 286 - cum reward -37.42036811570414\n",
      "episode: 287 - cum reward 14.936145523316977\n",
      "episode: 288 - cum reward -70.27956488241124\n",
      "episode: 289 - cum reward -86.28290220806437\n",
      "episode: 290 - cum reward 130.99254490826905\n",
      "episode: 291 - cum reward 9.810798770580604\n",
      "episode: 292 - cum reward -43.95700904627327\n",
      "episode: 293 - cum reward -25.876636704955747\n",
      "episode: 294 - cum reward -122.25958872776796\n",
      "episode: 295 - cum reward -46.36919329131228\n",
      "-0.021556381338846985\n",
      "-94.30782890319824\n",
      "22.145678696534112\n",
      "2.70688458114526\n",
      "loss  -0.3338249\n",
      "episode: 296 - cum reward 49.42881999142314\n",
      "episode: 297 - cum reward -130.4184069603458\n",
      "episode: 298 - cum reward -152.9218147425486\n",
      "episode: 299 - cum reward -139.05817097242752\n",
      "episode: 300 - cum reward -98.0414962962946\n",
      "episode: 301 - cum reward -159.49619988563782\n",
      "episode: 302 - cum reward -155.44564567544987\n",
      "episode: 303 - cum reward -157.93516446466901\n",
      "episode: 304 - cum reward -127.58448051966411\n",
      "episode: 305 - cum reward -100.08854011602091\n",
      "episode: 306 - cum reward -116.47718265262955\n",
      "-0.1252947778459801\n",
      "-5.8304709782592745\n",
      "10.416146045825828\n",
      "1.8842512784796914\n",
      "loss  -0.19804996\n",
      "episode: 307 - cum reward -122.81503946100341\n",
      "episode: 308 - cum reward -76.35247266650022\n",
      "episode: 309 - cum reward -11.965292497879405\n",
      "episode: 310 - cum reward 3.648260839692842\n",
      "episode: 311 - cum reward -15.587065419432987\n",
      "episode: 312 - cum reward 30.509537259788374\n",
      "episode: 313 - cum reward -58.49963393032662\n",
      "episode: 314 - cum reward -45.00560020323432\n",
      "episode: 315 - cum reward -65.47937580705153\n",
      "episode: 316 - cum reward 30.65435301803339\n",
      "episode: 317 - cum reward -31.280195663243724\n",
      "-0.017429251337328838\n",
      "-95.28112840652466\n",
      "26.255721985184323\n",
      "2.5330670592622826\n",
      "loss  -0.37397993\n",
      "episode: 318 - cum reward 101.63240624179632\n",
      "episode: 319 - cum reward -115.51541366195123\n",
      "episode: 320 - cum reward -149.3627974773625\n",
      "episode: 321 - cum reward -135.40998864347847\n",
      "episode: 322 - cum reward -123.07018579873879\n",
      "episode: 323 - cum reward -170.63381450663323\n",
      "episode: 324 - cum reward -107.38125617222354\n",
      "episode: 325 - cum reward 23.626413171279243\n",
      "episode: 326 - cum reward -154.82326013412373\n",
      "episode: 327 - cum reward -127.88381827651251\n",
      "episode: 328 - cum reward -99.17514871557026\n",
      "-0.10950301877453335\n",
      "-21.442153945844264\n",
      "19.742122904703237\n",
      "2.192732142119998\n",
      "loss  -0.09180434\n",
      "episode: 329 - cum reward -200.24193304149574\n",
      "episode: 330 - cum reward -72.69464799418853\n",
      "episode: 331 - cum reward -45.408522823519554\n",
      "episode: 332 - cum reward 16.222697819141956\n",
      "episode: 333 - cum reward -42.074225486252004\n",
      "episode: 334 - cum reward -33.45157029981638\n",
      "episode: 335 - cum reward -25.390664815510405\n",
      "episode: 336 - cum reward -47.59441209904276\n",
      "episode: 337 - cum reward 78.18031436573501\n",
      "episode: 338 - cum reward 75.21750937418769\n",
      "episode: 339 - cum reward -70.33783842764203\n",
      "-0.010921596012197785\n",
      "-94.50461912155151\n",
      "106.87475538253784\n",
      "2.915757861328792\n",
      "loss  -0.3326524\n",
      "episode: 340 - cum reward 49.74388846446675\n",
      "episode: 341 - cum reward -80.62450986301374\n",
      "episode: 342 - cum reward -198.86899983533382\n",
      "episode: 343 - cum reward -65.18647574535544\n",
      "episode: 344 - cum reward -79.55725330109107\n",
      "episode: 345 - cum reward -150.55045286359834\n",
      "episode: 346 - cum reward -91.10998531528125\n",
      "episode: 347 - cum reward -162.54241882139667\n",
      "episode: 348 - cum reward -161.54413885955353\n",
      "episode: 349 - cum reward -179.22034888229007\n",
      "episode: 350 - cum reward -181.0194188034117\n",
      "-0.1370644952502098\n",
      "-86.12279891967773\n",
      "11.1947968430911\n",
      "2.178209425479503\n",
      "loss  -0.11733987\n",
      "episode: 351 - cum reward 4.710232528564161\n",
      "episode: 352 - cum reward 35.85456481300196\n",
      "episode: 353 - cum reward -11.90065736718792\n",
      "episode: 354 - cum reward -53.64453667521555\n",
      "episode: 355 - cum reward -37.49180708257745\n",
      "episode: 356 - cum reward 24.686449090757517\n",
      "episode: 357 - cum reward -76.5029984709998\n",
      "episode: 358 - cum reward -33.23292147051455\n",
      "episode: 359 - cum reward -74.54607373757875\n",
      "episode: 360 - cum reward 17.49631286927717\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-120-d1ea5bdcc751>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLander-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mQActorCriticAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcompiled_model\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mq_agent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m800\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/notebooks/tools.py\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m     60\u001b[0m         \u001b[0mrews\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0;32mwhile\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m             \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-118-66ba180a87e4>\u001b[0m in \u001b[0;36mact\u001b[0;34m(self, state)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mact\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     53\u001b[0m         \u001b[0mstate\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 54\u001b[0;31m         \u001b[0mprob\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mflatten\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     55\u001b[0m         \u001b[0maction\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrandom\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mchoice\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_dim\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mp\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mprob\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     56\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLander-v2')\n",
    "q_agent = QActorCriticAgent(env, compiled_model=q_agent.model)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 800)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def policy_gradient_loss_continuous(returns):\n",
    "    def modified_mse(action_true, action_pred):\n",
    "        loss = -K.mean(returns * K.mean((action_true - action_pred)**2, axis=1))\n",
    "        return loss\n",
    "    return modified_mse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgentContinuous:                                                                                                                                                                                                \n",
    "    def __init__(self, env, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.shape[0]\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        pass\n",
    "    def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "        pass "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgentContinuous):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, gamma = .99, epsilon = .01, alpha = .01, memory_size = 6):\n",
    "        super().__init__(env, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        self.model_critic = self._build_model_critic()\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "    \n",
    "    def _build_model_critic(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        x = Dense(32, activation='relu')(input_state)\n",
    "        x = Dense(32, activation='relu')(x)\n",
    "        x = Dense(1, activation='linear')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        model.compile(loss='mse', optimizer=Adam(learning_rate=1e-2))\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.normal(prob)\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward + self.gamma * self.model_critic.predict(np.asarray(next_state).reshape(1,-1))]))\n",
    "        else:\n",
    "            self.episode.append(np.array([current_state, action, reward, reward]))\n",
    "        if done is True:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma).astype('float32')\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y_value = np.vstack(episode[:,3])\n",
    "            Y = np.vstack(episode[:,1])\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                Xs = np.vstack([ep[0] for ep in self.episodes])\n",
    "                Ys = np.vstack([ep[1] for ep in self.episodes])\n",
    "                Y_values = np.vstack([ep[3] for ep in self.episodes])\n",
    "                discounted_returns = np.hstack([ep[2] for ep in self.episodes])\n",
    "                early_stopping = self.model_critic.train_on_batch(Xs,Y_values)\n",
    "                baselines = self.model_critic.predict(Xs)\n",
    "                loss = policy_gradient_loss(discounted_returns)#baselines)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "                self.episodes = []\n",
    "            else:\n",
    "                self.episodes.append([X,Y,discounted_return, Y_value])\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_10\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 8)]               0         \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 32)                288       \n",
      "_________________________________________________________________\n",
      "dense_31 (Dense)             (None, 32)                1056      \n",
      "_________________________________________________________________\n",
      "dense_32 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 1,410\n",
      "Trainable params: 1,410\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward -558.0030592811014\n",
      "episode: 1 - cum reward -627.1235896057083\n",
      "episode: 2 - cum reward -478.171866270921\n",
      "episode: 3 - cum reward -26.19963633216399\n",
      "episode: 4 - cum reward -523.9392390330768\n",
      "episode: 5 - cum reward -200.39749380120867\n",
      "episode: 6 - cum reward -117.65392450433579\n",
      "episode: 7 - cum reward -530.3594154558305\n",
      "episode: 8 - cum reward -35.99259969526929\n",
      "episode: 9 - cum reward -454.2532442722762\n",
      "episode: 10 - cum reward -545.1567592785166\n",
      "episode: 11 - cum reward -790.4951432465905\n",
      "episode: 12 - cum reward -522.2525795182219\n",
      "episode: 13 - cum reward -562.0789230777125\n",
      "episode: 14 - cum reward -564.4644572156662\n",
      "episode: 15 - cum reward -387.68334994859384\n",
      "episode: 16 - cum reward -481.19986938235155\n",
      "episode: 17 - cum reward -674.4386511587968\n",
      "episode: 18 - cum reward -299.7152027123494\n",
      "episode: 19 - cum reward -634.5650814052633\n",
      "episode: 20 - cum reward -527.9133981916659\n",
      "episode: 21 - cum reward -640.4941111431478\n",
      "episode: 22 - cum reward -671.9517231445145\n",
      "episode: 23 - cum reward -283.39378504568833\n",
      "episode: 24 - cum reward -471.73165159305955\n",
      "episode: 25 - cum reward -748.8423517335302\n",
      "episode: 26 - cum reward -645.2584174897795\n",
      "episode: 27 - cum reward -861.3856585995883\n",
      "episode: 28 - cum reward -704.0829419241172\n",
      "episode: 29 - cum reward -1089.6799384224828\n",
      "episode: 30 - cum reward -723.2796756434669\n",
      "episode: 31 - cum reward -951.7728537452163\n",
      "episode: 32 - cum reward -1473.525821037492\n",
      "episode: 33 - cum reward -1319.2265038638561\n",
      "episode: 34 - cum reward -1947.6515368950836\n",
      "episode: 35 - cum reward -668.5290146651461\n",
      "episode: 36 - cum reward -585.4416104156077\n",
      "episode: 37 - cum reward -331.0386419259671\n",
      "episode: 38 - cum reward -254.9114672111579\n",
      "episode: 39 - cum reward -600.0098562920092\n",
      "episode: 40 - cum reward -677.3307131336645\n",
      "episode: 41 - cum reward -251.99593548539096\n",
      "episode: 42 - cum reward -806.5171826439905\n",
      "episode: 43 - cum reward -706.8040866501027\n",
      "episode: 44 - cum reward -501.29982607553643\n",
      "episode: 45 - cum reward -858.89978535774\n",
      "episode: 46 - cum reward -665.7348837671206\n",
      "episode: 47 - cum reward -634.3524300233089\n",
      "episode: 48 - cum reward -753.2247990202144\n",
      "episode: 49 - cum reward -880.4386708590309\n",
      "episode: 50 - cum reward -503.1230798161159\n",
      "episode: 51 - cum reward -852.1642326446994\n",
      "episode: 52 - cum reward -807.031224211955\n",
      "episode: 53 - cum reward -568.3311553058657\n",
      "episode: 54 - cum reward -868.016145399487\n",
      "episode: 55 - cum reward -596.0537499255609\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-9dcc911b1a05>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'LunarLanderContinuous-v2'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0mq_agent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mReinforceAgent\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mrewards\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_experiment_episode_train\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mq_agent\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m300\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrewards\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtitle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'cumulative reward per episode - rand_agent'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/notebooks/tools.py\u001b[0m in \u001b[0;36mrun_experiment_episode_train\u001b[0;34m(env, agent, nb_episode)\u001b[0m\n\u001b[1;32m     63\u001b[0m             \u001b[0mcurrent_state\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m             \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minfo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maction\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 65\u001b[0;31m             \u001b[0magent\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstate\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     66\u001b[0m             \u001b[0mrews\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mreward\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     67\u001b[0m         \u001b[0mrewards\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrews\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-17-52453a78595b>\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(self, current_state, action, reward, next_state, done)\u001b[0m\n\u001b[1;32m     53\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnext_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     54\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mdone\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 55\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgamma\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodel_critic\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext_state\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     56\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepisode\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcurrent_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maction\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mreward\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1011\u001b[0m         \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1012\u001b[0m         \u001b[0mworkers\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mworkers\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1013\u001b[0;31m         use_multiprocessing=use_multiprocessing)\n\u001b[0m\u001b[1;32m   1014\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1015\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0mreset_metrics\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(self, model, x, batch_size, verbose, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    496\u001b[0m         \u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mModeKeys\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mPREDICT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mverbose\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    497\u001b[0m         \u001b[0msteps\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msteps\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmax_queue_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_queue_size\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 498\u001b[0;31m         workers=workers, use_multiprocessing=use_multiprocessing, **kwargs)\n\u001b[0m\u001b[1;32m    499\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    500\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36m_model_iteration\u001b[0;34m(self, model, mode, x, y, batch_size, verbose, sample_weight, steps, callbacks, max_queue_size, workers, use_multiprocessing, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m               \u001b[0mmode\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m               \u001b[0mtraining_context\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtraining_context\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 475\u001b[0;31m               total_epochs=1)\n\u001b[0m\u001b[1;32m    476\u001b[0m           \u001b[0mcbks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake_logs\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mepoch_logs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2.py\u001b[0m in \u001b[0;36mrun_one_epoch\u001b[0;34m(model, iterator, execution_function, dataset_size, batch_size, strategy, steps_per_epoch, num_samples, mode, training_context, total_epochs)\u001b[0m\n\u001b[1;32m    126\u001b[0m         step=step, mode=mode, size=current_batch_size) as batch_logs:\n\u001b[1;32m    127\u001b[0m       \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 128\u001b[0;31m         \u001b[0mbatch_outs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    129\u001b[0m       \u001b[0;32mexcept\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mStopIteration\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOutOfRangeError\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    130\u001b[0m         \u001b[0;31m# TODO(kaftan): File bug about tf function and errors.OutOfRangeError?\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/keras/engine/training_v2_utils.py\u001b[0m in \u001b[0;36mexecution_function\u001b[0;34m(input_fn)\u001b[0m\n\u001b[1;32m     96\u001b[0m     \u001b[0;31m# `numpy` translates Tensors to values in Eager mode.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     97\u001b[0m     return nest.map_structure(_non_none_constant_value,\n\u001b[0;32m---> 98\u001b[0;31m                               distributed_function(input_fn))\n\u001b[0m\u001b[1;32m     99\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    100\u001b[0m   \u001b[0;32mreturn\u001b[0m \u001b[0mexecution_function\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    566\u001b[0m         \u001b[0mxla_context\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mExit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    567\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 568\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    569\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    570\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mtracing_count\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    604\u001b[0m       \u001b[0;31m# In this case we have not created variables on the first call. So we can\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    605\u001b[0m       \u001b[0;31m# run the first trace but we should fail if variables are created.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 606\u001b[0;31m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    607\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_created_variables\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    608\u001b[0m         raise ValueError(\"Creating variables on a non-first call to a function\"\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2361\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_lock\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2362\u001b[0m       \u001b[0mgraph_function\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_maybe_define_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2363\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mgraph_function\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_filtered_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=protected-access\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2364\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2365\u001b[0m   \u001b[0;34m@\u001b[0m\u001b[0mproperty\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_filtered_call\u001b[0;34m(self, args, kwargs)\u001b[0m\n\u001b[1;32m   1609\u001b[0m          if isinstance(t, (ops.Tensor,\n\u001b[1;32m   1610\u001b[0m                            resource_variable_ops.BaseResourceVariable))),\n\u001b[0;32m-> 1611\u001b[0;31m         self.captured_inputs)\n\u001b[0m\u001b[1;32m   1612\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1613\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_call_flat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcaptured_inputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcancellation_manager\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1690\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1691\u001b[0m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0;32m-> 1692\u001b[0;31m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[0m\u001b[1;32m   1693\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n\u001b[1;32m   1694\u001b[0m         \u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    543\u001b[0m               \u001b[0minputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    544\u001b[0m               \u001b[0mattrs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"executor_type\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mexecutor_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"config_proto\"\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 545\u001b[0;31m               ctx=ctx)\n\u001b[0m\u001b[1;32m    546\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    547\u001b[0m           outputs = execute.execute_with_cancellation(\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.6/site-packages/tensorflow_core/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     59\u001b[0m     tensors = pywrap_tensorflow.TFE_Py_Execute(ctx._handle, device_name,\n\u001b[1;32m     60\u001b[0m                                                \u001b[0mop_name\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mattrs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 61\u001b[0;31m                                                num_outputs)\n\u001b[0m\u001b[1;32m     62\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     63\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mname\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "q_agent = ReinforceAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
