{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we update arametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) V^\\pi(s) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a \\vert s) Q^\\pi(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a} = \\pi_\\theta(s_t)$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian policy is given by:\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n",
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = tf.reduce_sum(one_hot_action * tf.nn.log_softmax(action_probs), axis=1)\n",
    "        loss = -tf.reduce_mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, eps = 0.01, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .1, alpha = .01):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(32, activation='tanh')(input_state)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward]))\n",
    "        else:\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            loss = policy_gradient_loss(discounted_return)\n",
    "            self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "            self.model.train_on_batch(X,Y)\n",
    "            self.episode = []\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode_train(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            agent.train(current_state, action, reward, state, done)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_14\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_29 (Dense)             (None, 32)                160       \n",
      "_________________________________________________________________\n",
      "dense_30 (Dense)             (None, 2)                 66        \n",
      "=================================================================\n",
      "Total params: 226\n",
      "Trainable params: 226\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "q_agent = ReinforceAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 100)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
