{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Policy gradient\n",
    "In policy gradient, we update arametrize directly the policy $\\pi_\\theta$. It's especially welcome when the action space is continuous; in that case greedy policy based on Q-learning need to compute the $argmax_a Q(s,a)$. This could be pretty tedious. More generally, policy gradient algorithms are better to explore large state-action spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$J(\\theta) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) V^\\pi(s) \n",
    "= \\sum_{s \\in \\mathcal{S}} d^\\pi(s) \\sum_{a \\in \\mathcal{A}} \\pi_\\theta(a \\vert s) Q^\\pi(s, a)$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In discrete action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow [0,1]^{dim(A)}$ and $\\forall s$ $\\sum \\pi_\\theta(s) = 1$.\n",
    "\n",
    "In continous action space\n",
    "\n",
    "we parametrize $\\pi$ with $\\theta$, such as $\\pi_\\theta : S \\rightarrow \\mu^{dim(A)} \\times \\sigma^{dim(A)} =  \\mathbb{R}^{dim(A)} \\times \\mathbb{R}_{+,*}^{dim(A)}$\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is possible to show that the loss for discrete action ($1,...,N$) with softmax policy is weighted negative binary crossentropy:\n",
    "$-G\\sum_{j=1}^N[a^j\\log(\\hat{a}^j) + (1-a^j)\\log(1 - \\hat{a}^j)]$\n",
    "\n",
    "with:\n",
    "$a^j=1$ if $a_t = j$, $0$ otherwise.\n",
    "\n",
    "$\\hat{a} = \\pi_\\theta(s_t)$.\n",
    "\n",
    "$G$ is the discounted empirical return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$ from state $s_t$ and $a_t$\n",
    "\n",
    "\n",
    "It is possible to show that the loss for conitnous action ($1,...,N$) with multivariate Gaussian policy is given by:\n",
    "\n",
    "#TODO\n",
    "\n",
    "\n",
    "see https://aleksispi.github.io/assets/pg_autodiff.pdf for more explanation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforce"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import gym\n",
    "import copy\n",
    "import matplotlib.pyplot as plt\n",
    "from tools import discount_cumsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/raphael/rl_introduction/venv/lib/python3.6/site-packages/pandas/compat/__init__.py:85: UserWarning: Could not import the lzma module. Your installed Python is incomplete. Attempting to use lzma compression will result in a RuntimeError.\n",
      "  warnings.warn(msg)\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class DeepAgent:                                                                                                                                                                                                \n",
    "    def __init__(self, env, is_deterministic = False, gamma = .99, epsilon = .01):                                                                                                                          \n",
    "        self.env = env                                                                                                                                                                                      \n",
    "        self.is_deterministic = is_deterministic                                                                                                                                                            \n",
    "        self.gamma = gamma                                                                                                                                                                                  \n",
    "        self.epsilon = epsilon                                                                                                                                                                              \n",
    "        self.state_dim = env.observation_space.shape[0]\n",
    "        self.action_dim = env.action_space.n\n",
    "    def act(self, state):                                                                                                                                                                                   \n",
    "        if self.is_deterministic:                                                                                                                                                                           \n",
    "            action = np.argmax(self.policy[state])                                                                                                                                                          \n",
    "        else:                                                                                                                                                                                               \n",
    "            action = np.random.choice(np.arange(self.env.action_space.n),p=self.policy[state])                                                                                                              \n",
    "            return action                                                                                                                                                                                       \n",
    "        def train(current_state, action, reward, done):                                                                                                                                                         \n",
    "            pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "y = K.variable(np.array([1]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.], dtype=float32)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(K.log(y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "one_hot_action = K.variable(np.eye(3))\n",
    "action_probs = np.random.rand(9).reshape(3,3)\n",
    "action_probs = K.variable(action_probs / action_probs.sum(axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Variable 'Variable:0' shape=(3, 3) dtype=float32, numpy=\n",
       "array([[0.1963936 , 0.5228537 , 0.07761133],\n",
       "       [0.5495573 , 0.28435355, 0.19562896],\n",
       "       [0.21515797, 0.07879633, 0.82726526]], dtype=float32)>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "action_probs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<tf.Tensor: shape=(3, 3), dtype=float32, numpy=\n",
       "array([[-0.        , -0.73993206, -0.08078859],\n",
       "       [-0.79752445, -0.        , -0.2176946 ],\n",
       "       [-0.24227284, -0.08207413, -0.        ]], dtype=float32)>"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(1 - one_hot_action) * K.log(1 - action_probs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-1.6276345 , -1.2575369 , -0.18962988], dtype=float32)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "K.eval(K.sum(one_hot_action * K.log(action_probs),axis=1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow.keras.backend as K\n",
    "\n",
    "def policy_gradient_loss(returns):\n",
    "    def modified_crossentropy(one_hot_action, action_probs):\n",
    "        log_probs = K.sum(one_hot_action * K.log(action_probs) + (1 - one_hot_action) * K.log(1 - action_probs), axis=1)\n",
    "        loss = -K.mean(returns * log_probs)\n",
    "        return loss\n",
    "    return modified_crossentropy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward]))\n",
    "        else:\n",
    "            episode = np.asarray(self.episode)\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            loss = policy_gradient_loss(discounted_return)\n",
    "            self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "            self.model.train_on_batch(X,Y)\n",
    "            self.episode = []\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode_train(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset()\n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            current_state = state\n",
    "            state, reward, done, info = env.step(action)\n",
    "            agent.train(current_state, action, reward, state, done)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "        print('episode: {} - cum reward {}'.format(i, rewards[i]))\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "q_agent = ReinforceAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 1000)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.layers import Dense, multiply, Input\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.optimizers import RMSprop, Adam\n",
    "\n",
    "\n",
    "class ReinforceAgent(DeepAgent):\n",
    "    def __init__(self, env, compiled_model = None, load_model_path = None, is_deterministic = False, gamma = .99, epsilon = .01, alpha = .01, memory_size = 5):\n",
    "        super().__init__(env, is_deterministic, gamma, epsilon)\n",
    "        \n",
    "        if compiled_model is not None:\n",
    "            self.model = compiled_model\n",
    "        elif load_model_path is not None:\n",
    "            self.model = load_model(load_model_path)\n",
    "        else:\n",
    "            self.model = self._build_model()\n",
    "        \n",
    "        self.model.summary()\n",
    "        \n",
    "        self.episode = []\n",
    "        self.memory_size = memory_size\n",
    "        self.episodes = []\n",
    "        \n",
    "\n",
    "    def _build_model(self):\n",
    "        input_state = Input(name='input_state', shape=(self.state_dim,), dtype='float32')\n",
    "        input_discount_reward = Input(name='input_discount_reward', shape=(1,), dtype='float32')\n",
    "        x = Dense(8, activation='relu')(input_state)\n",
    "        x = Dense(4, activation='relu')(x)\n",
    "        x = Dense(self.action_dim, activation='softmax')(x)\n",
    "        model = Model(inputs=input_state, outputs=x)\n",
    "        return model\n",
    "\n",
    "    def act(self, state):\n",
    "        state = state.reshape(1, -1)\n",
    "        prob = self.model.predict(state, batch_size=1).flatten()\n",
    "        action = np.random.choice(self.action_dim, 1, p=prob)[0]\n",
    "        \n",
    "        return action\n",
    "\n",
    "    def train(self, current_state, action, reward, next_state, done):\n",
    "        if done is False:\n",
    "            self.episode.append(np.array([current_state, action, reward]))\n",
    "        else:\n",
    "            episode = np.asarray(self.episode)\n",
    "            self.episode = []\n",
    "            discounted_return = discount_cumsum(episode[:,2], self.gamma)\n",
    "            X = np.vstack(episode[:,0])\n",
    "            Y = np.zeros((len(episode), self.action_dim))\n",
    "            for i in range(len(episode)):\n",
    "                Y[i, episode[i,1]] = 1\n",
    "            if len(self.episodes) == self.memory_size:\n",
    "                episodes = np.asarray(self.episodes)\n",
    "                self.episodes = []\n",
    "                Xs = np.vstack(episodes[:,0])\n",
    "                Ys = np.vstack(episodes[:,1])\n",
    "                discounted_returns = np.hstack(episodes[:,2])\n",
    "                loss = policy_gradient_loss(discounted_returns)\n",
    "                self.model.compile(loss=loss, optimizer=Adam(learning_rate=1e-2))\n",
    "                self.model.train_on_batch(Xs,Ys)\n",
    "            else:\n",
    "                self.episodes.append((X,Y,discounted_return))\n",
    "\n",
    "    def save_model(self, path):\n",
    "        self.model.save(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_state (InputLayer)     [(None, 4)]               0         \n",
      "_________________________________________________________________\n",
      "dense_6 (Dense)              (None, 8)                 40        \n",
      "_________________________________________________________________\n",
      "dense_7 (Dense)              (None, 4)                 36        \n",
      "_________________________________________________________________\n",
      "dense_8 (Dense)              (None, 2)                 10        \n",
      "=================================================================\n",
      "Total params: 86\n",
      "Trainable params: 86\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "episode: 0 - cum reward 14.0\n",
      "episode: 1 - cum reward 14.0\n",
      "episode: 2 - cum reward 19.0\n",
      "episode: 3 - cum reward 12.0\n",
      "episode: 4 - cum reward 19.0\n",
      "episode: 5 - cum reward 25.0\n",
      "episode: 6 - cum reward 15.0\n",
      "episode: 7 - cum reward 15.0\n",
      "episode: 8 - cum reward 24.0\n",
      "episode: 9 - cum reward 56.0\n",
      "episode: 10 - cum reward 26.0\n",
      "episode: 11 - cum reward 25.0\n",
      "episode: 12 - cum reward 16.0\n",
      "episode: 13 - cum reward 48.0\n",
      "episode: 14 - cum reward 45.0\n",
      "episode: 15 - cum reward 37.0\n",
      "episode: 16 - cum reward 24.0\n",
      "episode: 17 - cum reward 24.0\n",
      "episode: 18 - cum reward 17.0\n",
      "episode: 19 - cum reward 31.0\n",
      "episode: 20 - cum reward 24.0\n",
      "episode: 21 - cum reward 64.0\n",
      "episode: 22 - cum reward 15.0\n",
      "episode: 23 - cum reward 26.0\n",
      "episode: 24 - cum reward 42.0\n",
      "episode: 25 - cum reward 28.0\n",
      "episode: 26 - cum reward 43.0\n",
      "episode: 27 - cum reward 19.0\n",
      "episode: 28 - cum reward 29.0\n",
      "episode: 29 - cum reward 27.0\n",
      "episode: 30 - cum reward 22.0\n",
      "episode: 31 - cum reward 80.0\n",
      "episode: 32 - cum reward 41.0\n",
      "episode: 33 - cum reward 25.0\n",
      "episode: 34 - cum reward 81.0\n",
      "episode: 35 - cum reward 32.0\n",
      "episode: 36 - cum reward 36.0\n",
      "episode: 37 - cum reward 11.0\n",
      "episode: 38 - cum reward 25.0\n",
      "episode: 39 - cum reward 15.0\n",
      "episode: 40 - cum reward 32.0\n",
      "episode: 41 - cum reward 74.0\n",
      "episode: 42 - cum reward 21.0\n",
      "episode: 43 - cum reward 10.0\n",
      "episode: 44 - cum reward 59.0\n",
      "episode: 45 - cum reward 51.0\n",
      "episode: 46 - cum reward 33.0\n",
      "episode: 47 - cum reward 28.0\n",
      "episode: 48 - cum reward 60.0\n",
      "episode: 49 - cum reward 53.0\n",
      "episode: 50 - cum reward 46.0\n",
      "episode: 51 - cum reward 27.0\n",
      "episode: 52 - cum reward 58.0\n",
      "episode: 53 - cum reward 23.0\n",
      "episode: 54 - cum reward 15.0\n",
      "episode: 55 - cum reward 31.0\n",
      "episode: 56 - cum reward 29.0\n",
      "episode: 57 - cum reward 40.0\n",
      "episode: 58 - cum reward 115.0\n",
      "episode: 59 - cum reward 18.0\n",
      "episode: 60 - cum reward 13.0\n",
      "episode: 61 - cum reward 43.0\n",
      "episode: 62 - cum reward 83.0\n",
      "episode: 63 - cum reward 25.0\n",
      "episode: 64 - cum reward 40.0\n",
      "episode: 65 - cum reward 40.0\n",
      "episode: 66 - cum reward 73.0\n",
      "episode: 67 - cum reward 32.0\n",
      "episode: 68 - cum reward 18.0\n",
      "episode: 69 - cum reward 38.0\n",
      "episode: 70 - cum reward 24.0\n",
      "episode: 71 - cum reward 17.0\n",
      "episode: 72 - cum reward 79.0\n",
      "episode: 73 - cum reward 68.0\n",
      "episode: 74 - cum reward 35.0\n",
      "episode: 75 - cum reward 40.0\n",
      "episode: 76 - cum reward 66.0\n",
      "episode: 77 - cum reward 59.0\n",
      "episode: 78 - cum reward 37.0\n",
      "episode: 79 - cum reward 71.0\n",
      "episode: 80 - cum reward 16.0\n",
      "episode: 81 - cum reward 32.0\n",
      "episode: 82 - cum reward 81.0\n",
      "episode: 83 - cum reward 23.0\n",
      "episode: 84 - cum reward 36.0\n",
      "episode: 85 - cum reward 49.0\n",
      "episode: 86 - cum reward 58.0\n",
      "episode: 87 - cum reward 78.0\n",
      "episode: 88 - cum reward 58.0\n",
      "episode: 89 - cum reward 63.0\n",
      "episode: 90 - cum reward 12.0\n",
      "episode: 91 - cum reward 22.0\n",
      "episode: 92 - cum reward 72.0\n",
      "episode: 93 - cum reward 138.0\n",
      "episode: 94 - cum reward 33.0\n",
      "episode: 95 - cum reward 32.0\n",
      "episode: 96 - cum reward 43.0\n",
      "episode: 97 - cum reward 69.0\n",
      "episode: 98 - cum reward 38.0\n",
      "episode: 99 - cum reward 48.0\n",
      "episode: 100 - cum reward 82.0\n",
      "episode: 101 - cum reward 20.0\n",
      "episode: 102 - cum reward 29.0\n",
      "episode: 103 - cum reward 70.0\n",
      "episode: 104 - cum reward 80.0\n",
      "episode: 105 - cum reward 45.0\n",
      "episode: 106 - cum reward 90.0\n",
      "episode: 107 - cum reward 86.0\n",
      "episode: 108 - cum reward 76.0\n",
      "episode: 109 - cum reward 138.0\n",
      "episode: 110 - cum reward 73.0\n",
      "episode: 111 - cum reward 69.0\n",
      "episode: 112 - cum reward 19.0\n",
      "episode: 113 - cum reward 99.0\n",
      "episode: 114 - cum reward 36.0\n",
      "episode: 115 - cum reward 99.0\n",
      "episode: 116 - cum reward 53.0\n",
      "episode: 117 - cum reward 126.0\n",
      "episode: 118 - cum reward 90.0\n",
      "episode: 119 - cum reward 32.0\n",
      "episode: 120 - cum reward 70.0\n",
      "episode: 121 - cum reward 27.0\n",
      "episode: 122 - cum reward 43.0\n",
      "episode: 123 - cum reward 81.0\n",
      "episode: 124 - cum reward 76.0\n",
      "episode: 125 - cum reward 31.0\n",
      "episode: 126 - cum reward 77.0\n",
      "episode: 127 - cum reward 59.0\n",
      "episode: 128 - cum reward 73.0\n",
      "episode: 129 - cum reward 77.0\n",
      "episode: 130 - cum reward 86.0\n",
      "episode: 131 - cum reward 34.0\n",
      "episode: 132 - cum reward 69.0\n",
      "episode: 133 - cum reward 103.0\n",
      "episode: 134 - cum reward 67.0\n",
      "episode: 135 - cum reward 101.0\n",
      "episode: 136 - cum reward 59.0\n",
      "episode: 137 - cum reward 109.0\n",
      "episode: 138 - cum reward 47.0\n",
      "episode: 139 - cum reward 60.0\n",
      "episode: 140 - cum reward 46.0\n",
      "episode: 141 - cum reward 36.0\n",
      "episode: 142 - cum reward 33.0\n",
      "episode: 143 - cum reward 61.0\n",
      "episode: 144 - cum reward 64.0\n",
      "episode: 145 - cum reward 75.0\n",
      "episode: 146 - cum reward 48.0\n",
      "episode: 147 - cum reward 62.0\n",
      "episode: 148 - cum reward 32.0\n",
      "episode: 149 - cum reward 79.0\n",
      "episode: 150 - cum reward 32.0\n",
      "episode: 151 - cum reward 77.0\n",
      "episode: 152 - cum reward 58.0\n",
      "episode: 153 - cum reward 125.0\n",
      "episode: 154 - cum reward 46.0\n",
      "episode: 155 - cum reward 79.0\n",
      "episode: 156 - cum reward 25.0\n",
      "episode: 157 - cum reward 31.0\n",
      "episode: 158 - cum reward 80.0\n",
      "episode: 159 - cum reward 134.0\n",
      "episode: 160 - cum reward 88.0\n",
      "episode: 161 - cum reward 82.0\n",
      "episode: 162 - cum reward 15.0\n",
      "episode: 163 - cum reward 69.0\n",
      "episode: 164 - cum reward 36.0\n",
      "episode: 165 - cum reward 121.0\n",
      "episode: 166 - cum reward 170.0\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "q_agent = ReinforceAgent(env)\n",
    "rewards = run_experiment_episode_train(env, q_agent, 300)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
