{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xycHKOBvw9Jb"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb02GRl9w9Jf"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/1_Environment_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1CMtVDmw9Jg",
    "outputId": "3215f76e-7ec8-4fba-edc9-76167d59841b"
   },
   "outputs": [],
   "source": [
    "# Run and restart runtime\n",
    "!pip install gymnasium[box2d,atari,accept-rom-license]\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryEDr1wCw9Jh"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import gymnasium\n",
    "from time import time,sleep\n",
    "# from rl_introduction.render_colab import gym_render\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPktUkoZw9Jj"
   },
   "source": [
    "# 1_Environment_and_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5MzQYXUw9Jh"
   },
   "source": [
    "### Introduction to Reinforcement Learning (RL)\n",
    "\n",
    "In RL, we study the interaction between an **agent** and an **environment**. The agent takes actions to achieve a goal, guided by rewards from the environment. Our aim is to develop agents that can learn optimal behaviors through these interactions.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hEQfAzCw9Jj"
   },
   "source": [
    "### Creating an Environment\n",
    "\n",
    "An environment in RL defines the space in which the agent operates. It returns a new state and a reward for each action taken by the agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, action):\n",
    "        if (action % 2 == self.state):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.state = np.random.randint(2)\n",
    "        return self.state, reward, self.done, {}\n",
    "        \n",
    "    def reset(self):\n",
    "        self.state = np.random.randint(2)\n",
    "        self.done = False\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKJx1oBQw9Jj"
   },
   "source": [
    "### Building an Agent\n",
    "Agents in RL decide which actions to take in an environment. A simple agent might act randomly or follow a predetermined policy.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V5bOAdaw9Jk"
   },
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        return np.random.randint(2) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running an Experiment\n",
    "\n",
    "To evaluate our agent's performance, we generate trajectories of state-action-reward sequences and compute the total reward.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_steps):\n",
    "    state = env.reset()\n",
    "    res = [state]\n",
    "    for _ in range(nb_steps):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        res += [action, reward, state]\n",
    "        \n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Understanding the Environment and Agent\n",
    "\n",
    "**Question 1:** What is the **state space** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 2:** What is the **action space** in the provided `Env`/`Agent` class?\n",
    "\n",
    "\n",
    "**Question 3:** What is the **Transition model** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 4:** What is the **Policy** in the provided `Agent` class?\n",
    "\n",
    "\n",
    "**Question 5:** What is the **Reward Function** in the provided `Env` class?\n",
    "\n",
    "\n",
    "**Question 6:** What object **run_experiment** is returning?\n",
    "\n",
    "\n",
    "**Exercise 1:** Instantiating the class `Agent` and `Env` to `run_experiment` on **100 steps**.\n",
    "\n",
    "\n",
    "\n",
    "**Exercise 2:** Compute the **cumulative reward** and **discouted cumultative reward**, also known as the return value. You can return more information from `run_experiment` to help.\n",
    "\n",
    "\n",
    "**Question 7:** In this `MDP`, what is the **Expected Return** when following the random policy of the `Agent`?\n",
    "\n",
    "\n",
    "**Question 8:** what would be the **best policy** function for the `Env` environment? \n",
    "\n",
    "\n",
    "**Exercise 3:** Implement the best policy function and use it to run the best agent. Compare its performance to the random agent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiQDaiUew9Jl"
   },
   "source": [
    "### Corrections\n",
    "\n",
    "**Question 1:** What is the **state space** in the provided `Env` class?\n",
    "\n",
    "**States:** $S = \\{0,1\\}$\n",
    "\n",
    "**Question 2:** What is the **action space** in the provided `Env`/`Agent` class?\n",
    "\n",
    "**Actions:** $A = \\{0,1\\}$ or $\\mathbb{N^+}$\n",
    "\n",
    "**Question 3:** What is the **Transition model** in the provided `Env` class?\n",
    "\n",
    "**Transition model:** $P_{ss'}^a = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]$\n",
    "\n",
    "For all $a \\in A$, and for all $s, s' \\in S : P_{ss'}^a = 0.5$\n",
    "\n",
    "**Question 4:** What is the **Policy** in the provided `Agent` class?\n",
    "\n",
    "Policy $\\pi$ is defined as follows:\n",
    "$\\pi(0) = 0$ with probability $0.5$, $\\pi(0) = 1$ with probability $0.5$ \n",
    "$\\pi(1) = 0$ with probability $0.5$, $\\pi(1) = 1$ with probability $0.5$ \n",
    "\n",
    "**Question 5:** What is the **Reward Function** in the provided `Env` class?\n",
    "\n",
    "Reward Function $R(s, a)$ is deterministic in this case:\n",
    "\n",
    "$R(0, a) = 1$ for all $a \\in 2\\mathbb{N}$\n",
    "$R(0, a) = -1$ for all $a \\in 2\\mathbb{N} + 1$\n",
    "$R(1, a) = 1$ for all $a \\in 2\\mathbb{N} + 1$\n",
    "$R(1, a) = -1$ for all $a \\in 2\\mathbb{N}$\n",
    "\n",
    "**Question 6:** What object does **run_experiment** return?\n",
    "\n",
    "It returns a trajectory $(s0, a0, r0, s1, a1, r1, ...)$.\n",
    "\n",
    "**Exercise 1:** Instantiate the class `Agent` and `Env` to `run_experiment` on **100 steps**.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "env = Env()\n",
    "agent = Agent(env)\n",
    "run_experiment(env, agent, nb_steps=100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**Exercise 2:** Compute the **cumulative reward** and **discouted cumultative reward** also known as the return value for each step of the trajectory. Provide the **cumulative reward** and **discouted (0.8) cumultative reward** at step 42.  You can return more information from `run_experiment` to help."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_steps):\n",
    "    state = env.reset()\n",
    "    res = [state]\n",
    "    rewards = []\n",
    "    for _ in range(nb_steps):\n",
    "        action = agent.act(state)\n",
    "        state, reward, done, info = env.step(action)\n",
    "        res += [action, reward, state]\n",
    "        rewards.append(reward)\n",
    "        \n",
    "    return res, rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_cumulative_reward(rewards, discout_factor=1):\n",
    "    trajectory_steps_length = len(rewards)\n",
    "    cumulative_rewards = []\n",
    "    for step in range(trajectory_steps_length):\n",
    "        rewards_from_step = rewards[step:]\n",
    "        cumulative_reward = 0\n",
    "        step = 0\n",
    "        for reward in rewards_from_step:\n",
    "            cumulative_reward += reward * discout_factor ** step\n",
    "            step += 1\n",
    "        cumulative_rewards.append(cumulative_reward)\n",
    "    return cumulative_rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_, rewards = run_experiment(env, agent, nb_steps=100)\n",
    "print(compute_cumulative_reward(rewards)[42], compute_cumulative_reward(rewards, 0.8)[42])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "\n",
    "**Question 7:** In this `MDP`, what is the **Expected Return** when following the random policy of the `Agent`?\n",
    "\n",
    "\n",
    "The expected return is 0.\n",
    "\n",
    "\n",
    "**Question 8:** what would be the **best policy** function for the `Env` environment? \n",
    "\n",
    "$\\pi(0) = 0 with probablitly 1, 1 with probability 0$ \n",
    "$\\pi(1) = 0 with probablitly 0, 1 with probability 1$ \n",
    "\n",
    "**Exercise 3:** Implement the best policy function and use it to run the best agent. Compare its performance to the random agent.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def best_policy(state):\n",
    "    if state == 0:\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "class Best_Agent:\n",
    "    def __init__(self, env):\n",
    "        pass\n",
    "    \n",
    "    def act(self, state):\n",
    "        return best_policy(state)\n",
    "# Instantiation\n",
    "env = Env()\n",
    "my_random_agent = Agent(env)\n",
    "my_best_agent = Best_Agent(env)\n",
    "\n",
    "nb_experiment = 100\n",
    "sum_random_agent_rewards = []\n",
    "sum_best_agent_rewards = []\n",
    "for exp in range(nb_experiment):\n",
    "    _, random_agent_rewards = run_experiment(env, my_random_agent, nb_steps=100)\n",
    "    _, best_agent_rewards = run_experiment(env, my_best_agent, nb_steps=100)\n",
    "    sum_random_agent_rewards.append(sum(random_agent_rewards))\n",
    "    sum_best_agent_rewards. append(sum(best_agent_rewards))\n",
    "\n",
    "plt.plot(sum_random_agent_rewards, 'o')\n",
    "plt.plot(sum_best_agent_rewards,'o')\n",
    "plt.title('Best agent vs Random agent / sum reward')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQugMTYw9Jt"
   },
   "source": [
    "# ENVIRONMENT FROM GYMNASIUM\n",
    "https://gymnasium.farama.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu7KaXVaw9Jt"
   },
   "source": [
    "## Discrete state action environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJy3wEiFw9Ju"
   },
   "source": [
    "### FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_XhcNEYw9Ju"
   },
   "source": [
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/FrozenLake.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGlopK7lw9Ju",
    "outputId": "ca2c7aa7-c4d2-442d-b0a3-bc9ff42cb79a"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "env.reset()\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "5ie1mWFk9I9H",
    "outputId": "2ddcc87d-c274-4a45-96c9-2fa46f0518f2"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='FrozenLake-v1', directory='./video', agent = 'random', slow_coeff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJmZDRLxw9Jv"
   },
   "source": [
    "## Discrete action continuous space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRrWI8Pw9Jv"
   },
   "source": [
    "### CartPole\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/CartPole-v1.png?raw=1\">\n",
    "observations: position of cart, velocity of cart, angle of pole, rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtx8uTlGw9Jv",
    "outputId": "7a9b6adf-16d5-464b-d491-8d3e45a2030b"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKNb_Geww9Jv"
   },
   "source": [
    "Most of the environments are provide with a render:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "tWY4g3NJw9Jv",
    "outputId": "d97ae2c3-f91a-41ef-a6a1-444f48fff951"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='CartPole-v0', directory='./video', agent = 'random', slow_coeff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpqvwIVIw9Jv"
   },
   "source": [
    "## Continuous action-space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tmqzr4lw9Jw"
   },
   "source": [
    "### MountainCarContinuous\n",
    "An underpowered car must climb a one-dimensional hill to reach a target. Unlike MountainCar v0, the action (engine force applied) is allowed to be a continuous value.\n",
    "\n",
    "The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates.\n",
    "\n",
    "On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty (it might in a more challenging version).\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/MountainCarContinuous-v0.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_ujQi2nw9Jw",
    "outputId": "35fab4ec-03ef-40e8-b243-a17261bc9e49"
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "-FZIxRFfw9Jw",
    "outputId": "d97f092b-f200-4b63-9954-183e5bc3a3a5"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='MountainCarContinuous-v0', directory='./video', agent = 'random', slow_coeff=1, max_step=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlJrp8Irw9Jw"
   },
   "source": [
    "### LunarLanderContinuous\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from -1 to +1. First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power. Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off.\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/LunarLanderContinuous-v2.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onRtZX-Yw9Jw",
    "outputId": "10f191d6-7d08-42c1-b55f-e10ef1e7ef39"
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "KvkMCFRzw9Jw",
    "outputId": "8602bf63-4b04-4854-d71e-eb347fd94ac1"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='LunarLanderContinuous-v2', directory='./video', agent = 'random', slow_coeff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROOf4sCfw9Jx"
   },
   "source": [
    "## High space dimension environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ1XKRCtw9Jx"
   },
   "source": [
    "### Atari games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0nsqIiHw9Jx",
    "outputId": "459a8b57-6b8a-4ea1-dd58-e34c49ff9376"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "oshCqJarw9Jx",
    "outputId": "7299f5d6-822a-48ed-831d-7c7792fbd92f"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='Pong-v0', directory='./video', agent = 'random', slow_coeff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bZwconVw9Jx"
   },
   "source": [
    "# Evaluate Random agent in open ai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypXHfhcGw9Jx"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zECidwdxw9Jx"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def act(self, state):\n",
    "            return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsdjKv-Rw9Jy",
    "outputId": "f2c6a827-e84d-4cac-da8b-6e56004d2b13"
   },
   "outputs": [],
   "source": [
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, 20)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DMG7lVjw9Jz"
   },
   "source": [
    "### Build your CartPole euristic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPyKEMBHw9Jz"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0SWH_Tpw9Jz"
   },
   "outputs": [],
   "source": [
    "#TODO: Create a cartPole agent that is better than Random:\n",
    "class MyCartPoleAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def act(self, state):\n",
    "        #Complete\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QxBtZOk8KMrz"
   },
   "outputs": [],
   "source": [
    "#Done: Create a cartPole agent that is better than Random:\n",
    "class MyCartPoleAgent(Agent):\n",
    "    def __init__(self, env, params=[-0.9,-0.1,0,0,-0.9, 0, 0.9]):\n",
    "        super().__init__(env)\n",
    "        self.params = params\n",
    "    def act(self, state):\n",
    "        if state[0] > self.params[0] and state[2] > self.params[1] and state[3] <= self.params[2]:\n",
    "            action = 0\n",
    "        elif state[3] > self.params[4] and state[1] >= self.params[5] and state[1] <= self.params[6]:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 1\n",
    "        return action\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wp07bd2Jw9Jz",
    "outputId": "ef8b6820-c656-4921-fbab-76c67b3e9655",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_experience = 100\n",
    "plt.figure(figsize=(20,20))\n",
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "print(f'total reward random agent: {sum(rewards)}')\n",
    "plt.plot(rewards, label=f'random agent')\n",
    "rand_agent = MyCartPoleAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "print(f'total reward manual agent: {sum(rewards)}')\n",
    "plt.plot(rewards, label=f'manual agent')\n",
    "plt.title('cumulative reward per episode - my agent')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLJb4F23MxHY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
