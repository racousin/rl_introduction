{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "xycHKOBvw9Jb"
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "sb02GRl9w9Jf"
   },
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/1_Environment_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "I1CMtVDmw9Jg",
    "outputId": "3215f76e-7ec8-4fba-edc9-76167d59841b"
   },
   "outputs": [],
   "source": [
    "# Run and restart runtime\n",
    "!apt-get install swig build-essential python-dev python3-dev > /dev/null 2>&1\n",
    "!pip install pygame==2.1.0 > /dev/null 2>&1\n",
    "!pip install box2d-py==2.3.5  > /dev/null 2>&1\n",
    "!pip install gym[Box2d,atari,accept-rom-license]==0.23.1 > /dev/null 2>&1\n",
    "!pip install pyvirtualdisplay > /dev/null 2>&1\n",
    "!apt-get install -y xvfb python-opengl ffmpeg > /dev/null 2>&1\n",
    "!pip install colabgymrender==1.0.2 > /dev/null 2>&1\n",
    "!pip install imageio==2.4.1 > /dev/null 2>&1\n",
    "!git clone https://github.com/racousin/rl_introduction.git > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "O5MzQYXUw9Jh"
   },
   "source": [
    "### Objective\n",
    "Here we present the type of problem that RL is addressing. And the main interactions between the environment and the agents.\n",
    "\n",
    "**Complete the TODO steps! Good luck!**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ryEDr1wCw9Jh"
   },
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import gym\n",
    "from time import time,sleep\n",
    "from rl_introduction.rl_introduction.render_colab import gym_render\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qPktUkoZw9Jj"
   },
   "source": [
    "# 1_Environment_and_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1hEQfAzCw9Jj"
   },
   "source": [
    "The environment encapsules the `transition model` ($\\mathbb{P}(S_{t+1},R_{t+1}|S_t,A_t)$).\n",
    "\n",
    "Science of environments is complex, particulary for real problems simulation (e.g for autonomous vehicules).\n",
    "\n",
    "It is easiest when then environment is already completly defined (e.g Videos Games)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QKJx1oBQw9Jj"
   },
   "source": [
    "### Minimal environment class\n",
    "The main part of a virtual environment is the `step function`. It takes the <b>action</b> and it returns a new <b>state</b>, the associated <b>reward</b>, a boolean (<b>done</b>) to indicate if it is a final step, and eventually some <b>info</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6V5bOAdaw9Jk"
   },
   "outputs": [],
   "source": [
    "# Environment class\n",
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, action):\n",
    "        ...\n",
    "        return state, reward, done, info\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AiQDaiUew9Jl"
   },
   "source": [
    "### Exemple of environment\n",
    "\n",
    "This environment generates a random state number. If the action is the same parity, reward is $1$, otherwise reward is $-1$\n",
    "\n",
    "**States:** $S = \\{0,1\\}$\n",
    "\n",
    "**Actions:** $A = \\mathbb{N^+}$\n",
    "\n",
    "**Transition model:** $P_{ss'}^a = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]$\n",
    "\n",
    "$\\forall a \\in A, \\forall s,s' \\in S : P_{ss'}^a = 0.5$\n",
    "\n",
    "**Reward function:**\n",
    "$R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a]$, $\\textit{deterministic, in this case}$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} : R(0,a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1: R(1,a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} : R(1,a) = -1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1 : R(0,a) = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0II5_tM4xrrc"
   },
   "outputs": [],
   "source": [
    "#TODO: Complete the Env_odd_even step\n",
    "# Exemple of environment\n",
    "class Env_odd_even(Env):\n",
    "    def step(self, action):\n",
    "        #complete here\n",
    "        return self.state, reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "LTmuulZow9Jm"
   },
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "env = Env_odd_even()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "UX1oI24nw9Jm",
    "outputId": "0ce66c53-806b-405a-ef76-74cc91bf979d"
   },
   "outputs": [],
   "source": [
    "# Try to interact with the environment\n",
    "action = 17\n",
    "state, reward, _, _ = env.step(action)\n",
    "print(f\"action: {action}, reward: {reward}, state: {state}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zYlxqsglw9Jm"
   },
   "source": [
    "### Minimal agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-SgYRogdw9Jm"
   },
   "source": [
    "The `act function` return an <b>action</b> for a <b>state</b> according to the agent policy.\n",
    "A smart agent may learn and update its policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dTjaSDx7w9Jn"
   },
   "outputs": [],
   "source": [
    "# Agent class\n",
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, state):\n",
    "        return action\n",
    "    def train():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eHOLJrWCw9Jn"
   },
   "source": [
    "### Random agent\n",
    "It acts randomly. It is useful as lower bound for benchmark.\n",
    "\n",
    "**Policy:** $a_t \\sim \\pi(\\cdot | s_t) \\to \\mathbb{P}(a_t|s_t)$\n",
    "\n",
    "$\\forall s \\in S : \\mathbb{P}(a=0|s) = \\mathbb{P}(a=1|s) = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "N2BJ915byJAk"
   },
   "outputs": [],
   "source": [
    "# Random agent\n",
    "#TODO: Complete the Random Agent\n",
    "class MyRandomAgent(Agent):\n",
    "    def act(self, state):\n",
    "        #complete here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "vus9RZkIw9Jn",
    "outputId": "3016886a-0416-412c-a0a6-7fb44e889605"
   },
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "agent = MyRandomAgent(env)\n",
    "# Try to interact with the agent\n",
    "state = 1\n",
    "action = agent.act(state)\n",
    "print(f\"action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "HcFO-Vygw9Jo"
   },
   "source": [
    "### Super agent\n",
    "When we know the best policy. It serves as higher bound for the benchmark and it allows to compute the regret.\n",
    "\n",
    "\n",
    "**Policy:**\n",
    "\n",
    "$\\mathbb{P}(a=0|s=0) = \\mathbb{P}(a=1|s=1) = 1$\n",
    "\n",
    "$\\mathbb{P}(a=1|s=0) = \\mathbb{P}(a=0|s=1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "wUtVCmXnw9Jo"
   },
   "outputs": [],
   "source": [
    "# Optimal agent\n",
    "#TODO: Complete the Super Agent\n",
    "class MySuperAgent(Agent):\n",
    "    def act(self, state):\n",
    "      #complete here\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "c7po-1Pow9Jo",
    "outputId": "53d2cf53-8ff1-42c5-a47e-4bd4f18e5d5e"
   },
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "agent = MySuperAgent(env)\n",
    "# Try to interact with the agent\n",
    "state = 1\n",
    "action = agent.act(state)\n",
    "print(f\"action: {action}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Aw60QYrxw9Jp"
   },
   "source": [
    "### Minimal agent and environement example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jNA6MA3Qw9Jp"
   },
   "outputs": [],
   "source": [
    "# Instantiation\n",
    "env = Env_odd_even()\n",
    "my_random_agent = MyRandomAgent(env)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H-NwBxvbw9Jp"
   },
   "outputs": [],
   "source": [
    "previous_state = env.reset() #initial state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4eZA1uT9w9Jp",
    "outputId": "d85bfbdc-4660-4209-d524-50679fc77358"
   },
   "outputs": [],
   "source": [
    "# Try some \n",
    "action = my_random_agent.act(previous_state)\n",
    "state, reward, _, _ = env.step(action)\n",
    "print(f\"previous_state: {previous_state}, action: {action}, reward: {reward}, state: {state}\")\n",
    "previous_state = state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8VwiMYJ-w9Jq"
   },
   "source": [
    "### Compute rewards in agent-environment interaction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KG_fKH0Cw9Jq"
   },
   "source": [
    "Now, we want to compare the performance of our agents. The empirical way to do it is to observe the cumulative reward (following the agent policy).\n",
    "\n",
    "**Generate trajectory:** $\\tau$ following the policy $\\pi$ $S_1,A_1,R_2,â€¦,S_T$ to compute an estimation of Return $G_t = \\sum_{k=0}^{T-t-1} R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "hk1cmcsZw9Jq",
    "outputId": "d13be594-819c-4085-cc48-fa7ebd53bf63"
   },
   "outputs": [],
   "source": [
    "#TODO: Compute an estimation of Return (ie cumulative reward)\n",
    "def run_experiment(env, agent, nb_step):\n",
    "    state = env.reset()\n",
    "    for i in range(nb_step):\n",
    "      #complete here\n",
    "    return cumulative_reward, []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0gsXiuiFw9Jq"
   },
   "outputs": [],
   "source": [
    "nb_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "G4cLIa9Pw9Jr",
    "outputId": "cc323264-2500-435b-d335-db6872b90054"
   },
   "outputs": [],
   "source": [
    "env = Env_odd_even()\n",
    "my_random_agent = MyRandomAgent(env)\n",
    "cumulative_reward, rewards = run_experiment(env, my_random_agent, nb_step)\n",
    "print(f\"cumulative_reward: {cumulative_reward}\")\n",
    "plt.plot(rewards.cumsum(), 'o')\n",
    "plt.title('cumulative reward - random_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r-oMQiTrzzEZ"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 316
    },
    "id": "irsyGHx6w9Jr",
    "outputId": "8e8dcb06-bd4c-470b-9680-e1af5540fb14"
   },
   "outputs": [],
   "source": [
    "env = Env_odd_even()\n",
    "my_super_agent = MySuperAgent(env)\n",
    "cumulative_reward, rewards = run_experiment(env, my_super_agent, nb_step)\n",
    "print(f\"cumulative_reward: {cumulative_reward}\")\n",
    "plt.plot(rewards.cumsum(),'o')\n",
    "plt.title('cumulative reward - super_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "JLR1Q3r8w9Jr"
   },
   "source": [
    "Remark: We can calculate **the expected return:** $ J(\\pi) = E_{\\tau\\sim \\pi}[{G(\\tau)}] = \\int_{\\tau} \\mathbb{P}(\\tau|\\pi) G(\\tau)$ using the transition model  $ \\mathbb{P}(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t)$\n",
    "\n",
    "For the random policy $J(\\pi) = 0$ and regret is T\n",
    "\n",
    "For the optimal policy $J(\\pi^*) = T$ and regret is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b8_S_Rhkw9Jr"
   },
   "source": [
    "### Exemple of environment with episode end\n",
    "We will consider a \"smarter\" environment that avoid cheaters. **Same as Env_odd_even** env, it will generate a random state number. If the action is the same parity, reward is $1$, otherwise reward is $-1$. **But, now, if the agent wins 3 consectives times, the episode ends**. **And in all cases, after 500 iterations, episode ends.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 130
    },
    "id": "BUA1ONMLw9Jr",
    "outputId": "654046db-6a8b-4344-b435-d1c46a3bd883"
   },
   "outputs": [],
   "source": [
    "#TODO update Env_odd_even to build the new env logic \n",
    "class Env_odd_even2(Env):\n",
    "    def __init__(self):\n",
    "        #complete here\n",
    "    def step(self, action):\n",
    "        #complete here\n",
    "        return self.state, reward, self.done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CtjElTiYw9Js"
   },
   "source": [
    "Now, we observe the cumulative reward through trajectories (following the agent policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nCcTzcGPw9Js"
   },
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset() \n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "tpCznyQkw9Js",
    "outputId": "b3eb0b42-f8cd-475e-d580-f06c89ac9ffd"
   },
   "outputs": [],
   "source": [
    "env = Env_odd_even2()\n",
    "rewards = run_experiment_episode(env, my_random_agent, 20)\n",
    "plt.plot(rewards, 'o')\n",
    "plt.title('cumulative reward per episode - random_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 298
    },
    "id": "ygWmeFtlw9Jt",
    "outputId": "f183f04f-075d-49cf-9e9a-a1a8cd4fd4a9"
   },
   "outputs": [],
   "source": [
    "env = Env_odd_even2()\n",
    "rewards = run_experiment_episode(env, my_super_agent, 20)\n",
    "plt.plot(rewards,'o')\n",
    "plt.title('cumulative reward per episode - super_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pDRscbeWw9Jt"
   },
   "source": [
    "The super agent is far to be the best for the new environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xLQugMTYw9Jt"
   },
   "source": [
    "# ENVIRONMENT FROM OPENAI\n",
    "https://gymnasium.farama.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pu7KaXVaw9Jt"
   },
   "source": [
    "## Discrete state action environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qJy3wEiFw9Ju"
   },
   "source": [
    "### FrozenLake"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c_XhcNEYw9Ju"
   },
   "source": [
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/FrozenLake.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "QGlopK7lw9Ju",
    "outputId": "ca2c7aa7-c4d2-442d-b0a3-bc9ff42cb79a"
   },
   "outputs": [],
   "source": [
    "env = gym.make('FrozenLake-v1')\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "env.reset()\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 349
    },
    "id": "5ie1mWFk9I9H",
    "outputId": "2ddcc87d-c274-4a45-96c9-2fa46f0518f2"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='FrozenLake-v1', directory='./video', agent = 'random', slow_coeff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dJmZDRLxw9Jv"
   },
   "source": [
    "## Discrete action continuous space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_gRrWI8Pw9Jv"
   },
   "source": [
    "### CartPole\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/CartPole-v1.png?raw=1\">\n",
    "observations: position of cart, velocity of cart, angle of pole, rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Wtx8uTlGw9Jv",
    "outputId": "7a9b6adf-16d5-464b-d491-8d3e45a2030b"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "lKNb_Geww9Jv"
   },
   "source": [
    "Most of the environments are provide with a render:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "tWY4g3NJw9Jv",
    "outputId": "d97ae2c3-f91a-41ef-a6a1-444f48fff951"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='CartPole-v0', directory='./video', agent = 'random', slow_coeff=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ZpqvwIVIw9Jv"
   },
   "source": [
    "## Continuous action-space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0Tmqzr4lw9Jw"
   },
   "source": [
    "### MountainCarContinuous\n",
    "An underpowered car must climb a one-dimensional hill to reach a target. Unlike MountainCar v0, the action (engine force applied) is allowed to be a continuous value.\n",
    "\n",
    "The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates.\n",
    "\n",
    "On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty (it might in a more challenging version).\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/MountainCarContinuous-v0.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6_ujQi2nw9Jw",
    "outputId": "35fab4ec-03ef-40e8-b243-a17261bc9e49"
   },
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 493
    },
    "id": "-FZIxRFfw9Jw",
    "outputId": "d97f092b-f200-4b63-9954-183e5bc3a3a5"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='MountainCarContinuous-v0', directory='./video', agent = 'random', slow_coeff=1, max_step=50)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlJrp8Irw9Jw"
   },
   "source": [
    "### LunarLanderContinuous\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from -1 to +1. First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power. Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off.\n",
    "<img src=\"https://github.com/racousin/rl_introduction/blob/master/notebooks/images/LunarLanderContinuous-v2.png?raw=1\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "onRtZX-Yw9Jw",
    "outputId": "10f191d6-7d08-42c1-b55f-e10ef1e7ef39"
   },
   "outputs": [],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 421
    },
    "id": "KvkMCFRzw9Jw",
    "outputId": "8602bf63-4b04-4854-d71e-eb347fd94ac1"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='LunarLanderContinuous-v2', directory='./video', agent = 'random', slow_coeff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "ROOf4sCfw9Jx"
   },
   "source": [
    "## High space dimension environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VQ1XKRCtw9Jx"
   },
   "source": [
    "### Atari games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "r0nsqIiHw9Jx",
    "outputId": "459a8b57-6b8a-4ea1-dd58-e34c49ff9376"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 231
    },
    "id": "oshCqJarw9Jx",
    "outputId": "7299f5d6-822a-48ed-831d-7c7792fbd92f"
   },
   "outputs": [],
   "source": [
    "gym_render(env_name='Pong-v0', directory='./video', agent = 'random', slow_coeff=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1bZwconVw9Jx"
   },
   "source": [
    "# Evaluate Random agent in open ai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ypXHfhcGw9Jx"
   },
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "zECidwdxw9Jx"
   },
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def act(self, state):\n",
    "            return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "tsdjKv-Rw9Jy",
    "outputId": "f2c6a827-e84d-4cac-da8b-6e56004d2b13"
   },
   "outputs": [],
   "source": [
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, 20)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3DMG7lVjw9Jz"
   },
   "source": [
    "### Build your CartPole euristic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BPyKEMBHw9Jz"
   },
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "I0SWH_Tpw9Jz"
   },
   "outputs": [],
   "source": [
    "#TODO: Create a cartPole agent that is better than Random:\n",
    "class MyCartPoleAgent(Agent):\n",
    "    def __init__(self, env):\n",
    "        super().__init__(env)\n",
    "    def act(self, state):\n",
    "        #Complete\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "wp07bd2Jw9Jz",
    "outputId": "ef8b6820-c656-4921-fbab-76c67b3e9655",
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "nb_experience = 100\n",
    "plt.figure(figsize=(20,20))\n",
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "print(f'total reward random agent: {sum(rewards)}')\n",
    "plt.plot(rewards, label=f'random agent')\n",
    "rand_agent = MyCartPoleAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "print(f'total reward manual agent: {sum(rewards)}')\n",
    "plt.plot(rewards, label=f'manual agent')\n",
    "plt.title('cumulative reward per episode - my agent')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hLJb4F23MxHY"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
