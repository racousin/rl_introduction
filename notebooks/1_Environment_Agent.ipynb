{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Run in collab\n",
    "<a href=\"https://colab.research.google.com/github/racousin/rl_introduction/blob/master/notebooks/1_Environment_Agent.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Objective\n",
    "Here we present the type of problem that RL is addressing. And the main interactions between the environment and the agents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-03-06 12:04:38.049549: W tensorflow/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2022-03-06 12:04:38.049576: I tensorflow/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "import numpy as np\n",
    "from time import sleep\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "import gym\n",
    "from time import time,sleep\n",
    "sns.set_style(\"darkgrid\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1_Environment_Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The environment encapsules the rules/transition model ($\\mathbb{P}(S_{t+1},R_{t+1}|S_t,A_t)$) that gives feedback to the agent.\n",
    "\n",
    "Science of environments is complex, particulary for real problems simulation (e.g for autonomous vehicules).\n",
    "\n",
    "It is easiest when then environment is already completly defined (e.g Videos Games)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal environment class\n",
    "The main part of a virtual environment is the <b>step</b> function. It takes the <b>action</b> and it returns a new <b>state</b>, the associated <b>reward</b>, a boolean (<b>done</b>) to inticate if it is a final step, and eventually some <b>info</b>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env:\n",
    "    def __init__(self):\n",
    "        self.state = 0\n",
    "        self.done = False\n",
    "        \n",
    "    def step(self, action):\n",
    "        ...\n",
    "        return state, reward, done, info\n",
    "        \n",
    "        \n",
    "    def reset(self):\n",
    "        self.__init__()\n",
    "        return self.state"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple of environment\n",
    "\n",
    "This environment generates a random state number. If the action is the same parity, reward is $1$, otherwise reward is $-1$\n",
    "\n",
    "**States:** $S = \\{0,1\\}$\n",
    "\n",
    "**Actions:** $A = \\mathbb{N^+}$\n",
    "\n",
    "**Transition model:** $P_{ss'}^a = \\mathbb{P} [S_{t+1} = s' \\vert S_t = s, A_t = a]$\n",
    "\n",
    "$\\forall a \\in A, \\forall s,s' \\in S : P_{ss'}^a = 0.5$\n",
    "\n",
    "**Reward function:**\n",
    "$R(s, a) = \\mathbb{E} [R_{t+1} \\vert S_t = s, A_t = a]$, $\\textit{deterministic, in this case}$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} : R(0,a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1: R(1,a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} : R(1,a) = -1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1 : R(0,a) = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_odd_even(Env):\n",
    "    def step(self, action):\n",
    "        if (action % 2 == 0 and self.state % 2 == 0) or (action % 2 == 1 and self.state % 2 == 1):\n",
    "            reward = 1\n",
    "        else:\n",
    "            reward = -1\n",
    "        self.state = np.random.randint(2)\n",
    "        return self.state, reward, self.done, {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env_odd_even()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env.step(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Minimal agent class"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The <b>act</b> function return an <b>action</b> for a <b>state</b> according to the agent policy.\n",
    "A smart agent may learn and update its policy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Agent:\n",
    "    def __init__(self, env):\n",
    "        self.env = env\n",
    "    def act(self, state):\n",
    "        return action\n",
    "    def train():\n",
    "        pass"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Random agent\n",
    "It acts randomly. It is useful as lower bound for benchmark.\n",
    "\n",
    "**Policy:** $a_t \\sim \\pi(\\cdot | s_t) \\to \\mathbb{P}(a_t|s_t)$\n",
    "\n",
    "$\\forall s \\in S : \\mathbb{P}(a=0|s) = \\mathbb{P}(a=1|s) = 0.5$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyRandomAgent(Agent):\n",
    "    def act(self, state):\n",
    "        action = np.random.randint(2)\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = MyRandomAgent(env)\n",
    "agent.act(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Super agent\n",
    "When we know the best policy. It serves as higher bound for the benchmark and it allows to compute the regret.\n",
    "\n",
    "\n",
    "**Policy:**\n",
    "\n",
    "$\\mathbb{P}(a=0|s=0) = \\mathbb{P}(a=1|s=1) = 1$\n",
    "\n",
    "$\\mathbb{P}(a=1|s=0) = \\mathbb{P}(a=0|s=1) = 0$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MySuperAgent(Agent):\n",
    "    def act(self, state):\n",
    "        if state % 2 == 0:\n",
    "            action = 0\n",
    "        else:\n",
    "            action = 1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Compute rewards in agent-environment interaction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we want to compare the performance of our agents. The empirical way to do it is to observe the cumulative reward (following the agent policy).\n",
    "\n",
    "**Generate trajectory:** $\\tau$ following the policy $\\pi$ $S_1,A_1,R_2,â€¦,S_T$ to compute an estimation of Return $G_t = \\sum_{k=0}^{T-t-1} \\gamma^k R_{t+k+1}$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(env, agent, nb_step):\n",
    "    rewards = np.zeros(nb_step)\n",
    "    state = env.reset()\n",
    "    for i in range(nb_step):\n",
    "        action = agent.act(state) #agent chooses action\n",
    "        state, reward, done, info = env.step(action) # feedback of environment\n",
    "        rewards[i] = reward\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_step = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env_odd_even()\n",
    "my_random_agent = MyRandomAgent(env)\n",
    "rewards = run_experiment(env, my_random_agent, nb_step)\n",
    "plt.plot(rewards.cumsum(), 'o')\n",
    "plt.title('cumulative reward - random_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env_odd_even()\n",
    "my_super_agent = MySuperAgent(env)\n",
    "rewards = run_experiment(env, my_super_agent, nb_step)\n",
    "plt.plot(rewards.cumsum(),'o')\n",
    "plt.title('cumulative reward - super_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can calculate **the expected return:** $ J(\\pi) = E_{\\tau\\sim \\pi}[{G(\\tau)}] = \\int_{\\tau} \\mathbb{P}(\\tau|\\pi) G(\\tau)$ using the transition model  $ \\mathbb{P}(\\tau|\\pi) = \\rho_0 (s_0) \\prod_{t=0}^{T-1} P(s_{t+1} | s_t, a_t) \\pi(a_t | s_t)$\n",
    "\n",
    "For the random policy $J(\\pi) = 0$ and regret is T\n",
    "\n",
    "For the optimal policy $J(\\pi^*) = T$ and regret is 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exemple of environment with episode end\n",
    "Environment generate a random state number. If the action is the same parity, reward is $1$, otherwise reward is $-1$. If the agent wins 3 consectives times, the episode ends. After 500 iterations, episode ends."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**States:** $S = \\{S_1, S_2, S_3\\} = \\{\\{0,1\\}, \\{0,1,2\\}, \\{1,...,500\\}\\} $\n",
    "\n",
    "**Actions:** $A = \\mathbb{N^+}$\n",
    "\n",
    "**Transition model:** Also a marckov chain model\n",
    "\n",
    "First dimension of the state is following the same law as before\n",
    "$\\forall a \\in A, \\forall s,s' \\in S_1 : P_{ss'}^a = 0.5$\n",
    "\n",
    "**Reward function:** it depends only on first state dimension\n",
    "\n",
    "\n",
    "$\\forall a \\in 2, \\forall s_2,s_3 \\in S_2, S_3 \\mathbb{N} : R((0,s_2,s_3),a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1, \\forall s_2,s_3 \\in S_2, S_3: R((1,s_2,s_3),a) = 1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N}, \\forall s_2,s_3 \\in S_2, S_3 : R((1,s_2,s_3),a) = -1$\n",
    "\n",
    "$\\forall a \\in 2 \\mathbb{N} + 1, \\forall s_2,s_3 \\in S_2, S_3 : R((0,s_2,s_3),a) = -1$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Env_odd_even2(Env):\n",
    "    def __init__(self):\n",
    "        self.state1 = 0\n",
    "        self.nb_win = 0\n",
    "        self.limit_game = 500\n",
    "        self.state = (self.state1, self.nb_win, self.limit_game)\n",
    "        self.done = False\n",
    "    def step(self, action):\n",
    "        if (action % 2 == 0 and self.state1 % 2 == 0) or (action % 2 == 1 and self.state1 % 2 == 1):\n",
    "            self.nb_win += 1\n",
    "            reward = 1\n",
    "        else:\n",
    "            self.nb_win = 0\n",
    "            reward = -1\n",
    "        self.state1 = np.random.randint(2)\n",
    "        if self.nb_win == 3 or self.limit_game == 0:\n",
    "            self.done = True\n",
    "        self.limit_game -= 1\n",
    "        self.state = (self.state1, self.nb_win, self.limit_game)\n",
    "        return self.state, reward, self.done, {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we observe the cumulative reward through trajectories (following the agent policy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment_episode(env, agent, nb_episode, is_using_first_state_only=False):\n",
    "    rewards = np.zeros(nb_episode)\n",
    "    for i in range(nb_episode):\n",
    "        state = env.reset() \n",
    "        done = False\n",
    "        rews = []\n",
    "        while done is False:\n",
    "            if is_using_first_state_only:\n",
    "                state = state[0] # for agent using only the first dimension state\n",
    "            action = agent.act(state)\n",
    "            state, reward, done, info = env.step(action)\n",
    "            rews.append(reward)\n",
    "        rewards[i] = sum(rews)\n",
    "    return rewards"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env_odd_even2()\n",
    "rewards = run_experiment_episode(env, my_random_agent, 20)\n",
    "plt.plot(rewards, 'o')\n",
    "plt.title('cumulative reward per episode - random_agent')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = Env_odd_even2()\n",
    "rewards = run_experiment_episode(env, my_super_agent, 20, is_using_first_state_only=True)\n",
    "plt.plot(rewards,'o')\n",
    "plt.title('cumulative reward per episode - super_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The super agent is far to be the best for the new environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ENVIRONMENT FROM OPENAI\n",
    "https://gym.openai.com/envs/#atari"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete state action environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gym import envs\n",
    "all_envs = envs.registry.all()\n",
    "env_ids = [env_spec.id for env_spec in all_envs]\n",
    "print(sorted(env_ids))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameNotFound",
     "evalue": "Environment `NChain` doesn't exist.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameNotFound\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_119524/3393524756.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0menv\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgym\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'NChain-v0'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'description action space:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0maction_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'description observation space:'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mobservation_space\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'run some random iteration:'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(id, **kwargs)\u001b[0m\n\u001b[1;32m    674\u001b[0m \u001b[0;31m# fmt: on\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    675\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;34m\"Env\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 676\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mregistry\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    677\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    678\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mmake\u001b[0;34m(self, path, **kwargs)\u001b[0m\n\u001b[1;32m    488\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    489\u001b[0m         \u001b[0;31m# Get all versions of this spec.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 490\u001b[0;31m         \u001b[0mversions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv_specs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mversions\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    491\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;31m# We check what the latest version of the environment is and display\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36mversions\u001b[0;34m(self, namespace, name)\u001b[0m\n\u001b[1;32m    218\u001b[0m         \u001b[0mSee\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m`\u001b[0m\u001b[0mgym\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0menvs\u001b[0m\u001b[0;34m/\u001b[0m\u001b[0m__relocated__\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpy\u001b[0m\u001b[0;31m`\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mmore\u001b[0m \u001b[0minfo\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    219\u001b[0m         \"\"\"\n\u001b[0;32m--> 220\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_assert_name_exists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    221\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    222\u001b[0m         \u001b[0mversions\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtree\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mnamespace\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/rl_introduction/venv/lib/python3.7/site-packages/gym/envs/registration.py\u001b[0m in \u001b[0;36m_assert_name_exists\u001b[0;34m(self, namespace, name)\u001b[0m\n\u001b[1;32m    295\u001b[0m                 \u001b[0mmessage\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34mf\" Did you mean: `{suggestions[0]}`?\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    296\u001b[0m         \u001b[0;31m# Throw the error\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 297\u001b[0;31m         \u001b[0;32mraise\u001b[0m \u001b[0merror\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mNameNotFound\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    298\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    299\u001b[0m     def _assert_version_exists(\n",
      "\u001b[0;31mNameNotFound\u001b[0m: Environment `NChain` doesn't exist."
     ]
    }
   ],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "env.reset()\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### NChain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"images/NChain-illustration.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('NChain-v0')\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "env.reset()\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete action continuous space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### CartPole\n",
    "A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. The episode ends when the pole is more than 15 degrees from vertical, or the cart moves more than 2.4 units from the center.\n",
    "<img src=\"images/CartPole-v1.png\">\n",
    "observations: position of cart, velocity of cart, angle of pole, rotation rate of pole"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Most of the environments are provide with a render:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 150\n",
    "state = env.reset()\n",
    "for _ in range(time_steps):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        print('reset')\n",
    "    env.render()\n",
    "    sleep(0.1)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Continuous action-space environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### MountainCarContinuous\n",
    "An underpowered car must climb a one-dimensional hill to reach a target. Unlike MountainCar v0, the action (engine force applied) is allowed to be a continuous value.\n",
    "\n",
    "The target is on top of a hill on the right-hand side of the car. If the car reaches it or goes beyond, the episode terminates.\n",
    "\n",
    "On the left-hand side, there is another hill. Climbing this hill can be used to gain potential energy and accelerate towards the target. On top of this second hill, the car cannot go further than a position equal to -1, as if there was a wall. Hitting this limit does not generate a penalty (it might in a more challenging version).\n",
    "<img src=\"images/MountainCarContinuous-v0.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('MountainCarContinuous-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "for _ in range(3):\n",
    "    print()\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 500\n",
    "state = env.reset()\n",
    "for _ in range(time_steps):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        print('reset')\n",
    "    env.render()\n",
    "    sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LunarLanderContinuous\n",
    "Landing pad is always at coordinates (0,0). Coordinates are the first two numbers in state vector. Reward for moving from the top of the screen to landing pad and zero speed is about 100..140 points. If lander moves away from landing pad it loses reward back. Episode finishes if the lander crashes or comes to rest, receiving additional -100 or +100 points. Each leg ground contact is +10. Firing main engine is -0.3 points each frame. Solved is 200 points. Landing outside landing pad is possible. Fuel is infinite, so an agent can learn to fly and then land on its first attempt. Action is two real values vector from -1 to +1. First controls main engine, -1..0 off, 0..+1 throttle from 50% to 100% power. Engine can't work with less than 50% power. Second value -1.0..-0.5 fire left engine, +0.5..+1.0 fire right engine, -0.5..0.5 off.\n",
    "<img src=\"images/LunarLanderContinuous-v2.png\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "description action space: Box(-1.0, 1.0, (2,), float32)\n",
      "description observation space: Box(-inf, inf, (8,), float32)\n",
      "run some random iteration:\n",
      "\n",
      "action: \n",
      "[0.10522658 0.09982915]\n",
      "state: \n",
      "[ 0.00490608  1.3904307   0.24788669 -0.44975185 -0.00565469 -0.05632671\n",
      "  0.          0.        ]\n",
      "reward: \n",
      "1.581574368012906\n",
      "action: \n",
      "[ 0.0109742  -0.56965506]\n",
      "state: \n",
      "[ 0.00734243  1.38042     0.24529831 -0.44493514 -0.00729829 -0.03287514\n",
      "  0.          0.        ]\n",
      "reward: \n",
      "1.2136705168513866\n",
      "action: \n",
      "[-0.7439694   0.66857177]\n",
      "state: \n",
      "[ 0.0098258   1.3698078   0.2511902  -0.47166124 -0.01012301 -0.05649971\n",
      "  0.          0.        ]\n",
      "reward: \n",
      "-1.8734469747294862\n"
     ]
    }
   ],
   "source": [
    "env = gym.make('LunarLanderContinuous-v2')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 500\n",
    "state = env.reset()\n",
    "for _ in range(time_steps):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        print('reset')\n",
    "    env.render()\n",
    "    sleep(0.01)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## High space dimension environment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Atari games\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')\n",
    "env.reset()\n",
    "print('description action space:', env.action_space)\n",
    "print('description observation space:', env.observation_space)\n",
    "print('run some random iteration:')\n",
    "print()\n",
    "for _ in range(3):\n",
    "    action = env.action_space.sample()\n",
    "    print('action: ')\n",
    "    print(action)\n",
    "    state, reward, done, info = env.step(action)\n",
    "    print('state: ')\n",
    "    print(state)\n",
    "    print('reward: ')\n",
    "    print(reward)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "time_steps = 1000\n",
    "state = env.reset()\n",
    "for _ in range(time_steps):\n",
    "    action = env.action_space.sample()\n",
    "    _, _, done, _ = env.step(action)\n",
    "    if done is True:\n",
    "        state = env.reset()\n",
    "        print('reset')\n",
    "    env.render()\n",
    "    sleep(0.004)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluate Random agent in open ai gym"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('Pong-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RandomAgent(Agent):\n",
    "    def act(self, state):\n",
    "            return self.env.action_space.sample()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, 20)\n",
    "plt.plot(rewards)\n",
    "plt.title('cumulative reward per episode - rand_agent')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collab Render"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!apt-get install -y xvfb python-opengl > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install gym pyvirtualdisplay > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from IPython import display as ipythondisplay"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gym\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyvirtualdisplay import Display\n",
    "display = Display(visible=0, size=(400, 300))\n",
    "display.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v0\")\n",
    "#env = gym.make('Pong-v0')\n",
    "#env = gym.make('LunarLanderContinuous-v2')\n",
    "#env = gym.make('BipedalWalker-v2')\n",
    "env.reset()\n",
    "prev_screen = env.render(mode='rgb_array')\n",
    "plt.imshow(prev_screen)\n",
    "\n",
    "for i in range(50):\n",
    "    action = env.action_space.sample()\n",
    "    obs, reward, done, info = env.step(action)\n",
    "    screen = env.render(mode='rgb_array')\n",
    "    \n",
    "    plt.imshow(screen)\n",
    "    ipythondisplay.clear_output(wait=True)\n",
    "    ipythondisplay.display(plt.gcf())\n",
    "    sleep(0.004)\n",
    "    \n",
    "    if done:\n",
    "        break\n",
    "\n",
    "ipythondisplay.clear_output(wait=True)\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build your CartPole euristic agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "env = gym.make('CartPole-v0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyCartPoleAgent(Agent):\n",
    "    def __init__(self, env, params=[0.27,-0.1,-0.5,-0.2,0.2, 0, 1]):\n",
    "        super().__init__(env)\n",
    "        self.params = params\n",
    "    def act(self, state):\n",
    "        if state[0] > self.params[0] and state[2] > self.params[1] and state[3] <= self.params[2]:\n",
    "            action = 0\n",
    "        elif state[3] > self.params[4] and state[1] >= self.params[5] and state[1] <= self.params[6]:\n",
    "            action = 1\n",
    "        else:\n",
    "            action = 1\n",
    "        return action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "params0 = [0.27,-0.1,-0.5,-0.2,0.2, 0, 0]\n",
    "params1 = [-0.27,0.1,0.5,0.2,-0.2, -0.1, 0.1]\n",
    "params2 = [-0.9,-0.1,0,0,-0.9, 0, 0.9]\n",
    "\n",
    "nb_experience = 500\n",
    "\n",
    "plt.figure(figsize=(20,20))\n",
    "rand_agent = RandomAgent(env)\n",
    "rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "print(f'total reward random: {sum(rewards)}')\n",
    "plt.plot(rewards, label=f'random ')\n",
    "for i, param in enumerate([params0, params1, params2]):\n",
    "    rand_agent = MyCartPoleAgent(env,params=param)\n",
    "    rewards = run_experiment_episode(env, rand_agent, nb_experience)\n",
    "    print(f'total reward param {i}: {sum(rewards)}')\n",
    "    plt.plot(rewards, label=f'param {i}')\n",
    "plt.title('cumulative reward per episode - my agent')\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
